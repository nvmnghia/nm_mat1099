\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={\(f(0) = -1\) and \(f(1) \approx \num {0.459697694}\) have the opposite signs, so there's a root in \(\interval {0}{1}\). \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.1] S[table-format=1.2] S[table-format=1.3] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 0 & 1 & 0.5 & -0.170475781 \\ 2 & 0.5 & 1 & 0.75 & 0.134336535 \\ 3 & 0.5 & 0.75 & 0.625 & -0.020393704 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {0.625}\).}||exercise-2=={\begin {enumerate}[label = (\alph *)] \item \(f(-2) = \num {-22.5}\) and \(f(\num {1.5}) = \num {3.75}\) have the opposite signs, so there's a root in \(\interval {-2}{\num {1.5}}\). \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.3] S[table-format=-1.2] S[table-format=-1.4] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -2 & 1.5 & -0.25 & 2.109375 \\ 2 & -2 & -0.25 & -1.125 & -1.294921875 \\ 3 & -1.125 & -0.25 & -0.6875 & 1.878662109 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {-0.6875}\). \par \item \(f(\num {-1.25}) = \num {-2.953125}\) and \(f(\num {2.5}) = \num {31.5}\) have the opposite signs, so there's a root in \(\interval {\num {-1.25}}{\num {2.5}}\). \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.1] S[table-format=1.1] S[table-format=1.1] S[table-format=1]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -1.5 & 2.5 & 0.5 & 0 \\ \bottomrule \end {tabular} \end {table} \par The solution is found in the first iteration so \(p_3\) doesn't exist. \end {enumerate}}||exercise-3=={\begin {enumerate}[label = (\alph *)] \item \(f(0) = -6\) and \(f(1) = 2\) have the opposite signs, so there's a root in \(\interval {0}{1}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {1 - 0}{2^n} < 10^{-2} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.6] S[table-format=1.5] S[table-format=1.7] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 0 & 1 & 0.5 & -0.625 \\ 2 & 0.5 & 1 & 0.75 & 0.984375 \\ 3 & 0.5 & 0.75 & 0.625 & 0.259766 \\ 4 & 0.5 & 0.625 & 0.5625 & -0.161865 \\ 5 & 0.5625 & 0.625 & 0.59375 & 0.054047 \\ 6 & 0.5625 & 0.59375 & 0.578125 & -0.052624 \\ 7 & 0.578125 & 0.59375 & 0.5859375 & 0.001031 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {0.5859}\). \par \item \(f(1) = 2\) and \(f(\num {3.2}) = \num {-0.112}\) have the opposite signs, so there's a root in \(\interval {1}{\num {3.2}}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {\num {3.2} - 1}{2^n} < 10^{-2} \iff n \geq 8\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.5] S[table-format=1.6] S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 1 & 3.2 & 2.1 & 1.791 \\ 2 & 2.1 & 3.2 & 2.65 & 0.552125 \\ 3 & 2.65 & 3.2 & 2.925 & 0.085828 \\ 4 & 2.925 & 3.2 & 3.0625 & -0.054443 \\ 5 & 2.925 & 3.0625 & 2.99375 & 0.006328 \\ 6 & 2.99375 & 3.0625 & 3.028125 & -0.026521 \\ 7 & 2.99375 & 3.02813 & 3.010938 & -0.010697 \\ 8 & 2.99375 & 3.010938 & 3.002344 & -0.002333 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {3.0023}\). \par \item \(f(\num {3.2}) = \num {-0.112}\) and \(f(4) = 2\) have the opposite signs, so there's a root in \(\interval {\num {3.2}}{4}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {4 - \num {3.2}}{2^n} < 10^{-2} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.5] S[table-format=1.6] S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 3.2 & 4 & 3.6 & 0.336 \\ 2 & 3.2 & 3.6 & 3.4 & -0.016 \\ 3 & 3.4 & 3.6 & 3.5 & 0.125 \\ 4 & 3.4 & 3.5 & 3.45 & 0.046125 \\ 5 & 3.4 & 3.45 & 3.425 & 0.013016 \\ 6 & 3.4 & 3.425 & 3.4125 & -0.001998 \\ 7 & 3.4125 & 3.425 & 3.41875 & 0.005382 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {3.4188}\). \end {enumerate}}||exercise-4=={\begin {enumerate}[label = (\alph *)] \item \(f(-2) = 12\) and \(f(-1) = -1\) have the opposite signs, so there's a root in \(\interval {-2}{-1}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {-1 - (-2)}{2^n} < 10^{-2} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.6] S[table-format=-1.5] S[table-format=-1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -2 & -1 & -1.5 & 0.8125 \\ 2 & -1.5 & -1 & -1.25 & -0.902344 \\ 3 & -1.5 & -1.25 & -1.375 & -0.288818 \\ 4 & -1.5 & -1.375 & -1.4375 & 0.195328 \\ 5 & -1.4375 & -1.375 & -1.40625 & -0.062667 \\ 6 & -1.4375 & -1.40625 & -1.421875 & 0.062263 \\ 7 & -1.421875 & -1.40625 & -1.414063 & -0.001208 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {-1.4141}\). \par \item \(f(0) = 4\) and \(f(2) = -4\) have the opposite signs, so there's a root in \(\interval {0}{2}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {2 - 0}{2^n} < 10^{-2} \iff n \geq 8\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.5] S[table-format=1.6] S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 0 & 2 & 1 & 3 \\ 2 & 1 & 2 & 1.5 & -0.6875 \\ 3 & 1 & 1.5 & 1.25 & 1.285156 \\ 4 & 1.25 & 1.5 & 1.375 & 0.312744 \\ 5 & 1.375 & 1.5 & 1.4375 & -0.186508 \\ 6 & 1.375 & 1.4375 & 1.40625 & 0.063676 \\ 7 & 1.40625 & 1.4375 & 1.421875 & -0.061318 \\ 8 & 1.40625 & 1.421875 & 1.414063 & 0.001208 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {1.4141}\). \par \item \(f(2) = -4\) and \(f(3) = 7\) have the opposite signs, so there's a root in \(\interval {2}{3}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {3 - 2}{2^n} < 10^{-2} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.5] S[table-format=1.6] S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 2 & 3 & 2.5 & -3.1875 \\ 2 & 2.5 & 3 & 2.75 & 0.347656 \\ 3 & 2.5 & 2.75 & 2.625 & -1.757568 \\ 4 & 2.625 & 2.75 & 2.6875 & -0.795639 \\ 5 & 2.6875 & 2.75 & 2.71875 & -0.247466 \\ 6 & 2.71875 & 2.75 & 2.734375 & 0.044125 \\ 7 & 2.71875 & 2.734375 & 2.726563 & -0.103151 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {2.7266}\). \par \item \(f(-1) = -1\) and \(f(0) = 4\) have the opposite signs, so there's a root in \(\interval {-1}{0}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-2}\) is: \par \[\abs {p_n - p} \leq \frac {0 - (-1)}{2^n} < 10^{-2} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.6] S[table-format=-1.5] S[table-format=-1.6] S[table-format=-1.6]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -1 & 0 & -0.5 & 1.3125 \\ 2 & -1 & -0.5 & -0.75 & -0.089844 \\ 3 & -0.75 & -0.5 & -0.625 & 0.578369 \\ 4 & -0.75 & -0.625 & -0.6875 & 0.232681 \\ 5 & -0.75 & -0.6875 & -0.71875 & 0.068086 \\ 6 & -0.75 & -0.71875 & -0.734375 & -0.011768 \\ 7 & -0.734375 & -0.71875 & -0.726563 & 0.027943 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {-0.7266}\). \end {enumerate}}||exercise-5=={\begin {enumerate}[label = (\alph *)] \item \(f(0) = -1\) and \(f(1) = \num {0.5}\) have the opposite signs, so there's a root in \(\interval {0}{1}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {1 - 0}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0 & 1 & 0.5 & -0.207106781 \\ 2 & 0.5 & 1 & 0.75 & 0.155396442 \\ 3 & 0.5 & 0.75 & 0.625 & -0.023419777 \\ 4 & 0.625 & 0.75 & 0.6875 & 0.066571094 \\ 5 & 0.625 & 0.6875 & 0.65625 & 0.021724521 \\ 6 & 0.625 & 0.65625 & 0.640625 & -0.000810008 \\ 7 & 0.640625 & 0.65625 & 0.6484375 & 0.010466611 \\ 8 & 0.640625 & 0.6484375 & 0.64453125 & 0.004830646 \\ 9 & 0.640625 & 0.64453125 & 0.642578125 & 0.002010906 \\ 10 & 0.640625 & 0.642578125 & 0.641601562 & 0.000600596 \\ 11 & 0.640625 & 0.641601562 & 0.641113281 & -0.000104669 \\ 12 & 0.641113281 & 0.641601562 & 0.641357422 & 0.000247972 \\ 13 & 0.641113281 & 0.641357422 & 0.641235352 & 0.000071654 \\ 14 & 0.641113281 & 0.641235352 & 0.641174316 & -0.000016507 \\ 15 & 0.641174316 & 0.641235352 & 0.641204834 & 0.000027573 \\ 16 & 0.641174316 & 0.641204834 & 0.641189575 & 0.000005533 \\ 17 & 0.641174316 & 0.641189575 & 0.641181946 & -0.000005487 \\ \bottomrule \end {longtable} \par So \(p \approx \num {-0.641182}\). \par \item \(f(0) = -1\) and \(f(1) = e\) have the opposite signs, so there's a root in \(\interval {0}{1}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {1 - 0}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0 & 1 & 0.5 & 0.898721271 \\ 2 & 0 & 0.5 & 0.25 & -0.028474583 \\ 3 & 0.25 & 0.5 & 0.375 & 0.439366415 \\ 4 & 0.25 & 0.375 & 0.3125 & 0.206681691 \\ 5 & 0.25 & 0.3125 & 0.28125 & 0.089433196 \\ 6 & 0.25 & 0.28125 & 0.265625 & 0.030564234 \\ 7 & 0.25 & 0.265625 & 0.2578125 & 0.001066368 \\ 8 & 0.25 & 0.2578125 & 0.25390625 & -0.013698684 \\ 9 & 0.25390625 & 0.2578125 & 0.255859375 & -0.006314807 \\ 10 & 0.255859375 & 0.2578125 & 0.256835938 & -0.002623882 \\ 11 & 0.256835938 & 0.2578125 & 0.257324219 & -0.000778673 \\ 12 & 0.257324219 & 0.2578125 & 0.257568359 & 0.000143868 \\ 13 & 0.257324219 & 0.257568359 & 0.257446289 & -0.000317397 \\ 14 & 0.257446289 & 0.257568359 & 0.257507324 & -0.000086763 \\ 15 & 0.257507324 & 0.257568359 & 0.257537842 & 0.000028553 \\ 16 & 0.257507324 & 0.257537842 & 0.257522583 & -0.000029105 \\ 17 & 0.257522583 & 0.257537842 & 0.257530212 & -0.000000276 \\ \bottomrule \end {longtable} \par So \(p \approx \num {0.25753}\). \par \item \(f(-3) \approx \num {-9.76102172}\) and \(f(-2) \approx \num {1.614574483}\) have the opposite signs, so there's a root in \(\interval {-3}{-2}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {-2 - (-3)}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=-1.8] S[table-format=-1.8] S[table-format=-1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & -3 & -2 & -2.5 & -3.66831093 \\ 2 & -2.5 & -2 & -2.25 & -0.613918903 \\ 3 & -2.25 & -2 & -2.125 & 0.630246832 \\ 4 & -2.25 & -2.125 & -2.1875 & 0.038075532 \\ 5 & -2.25 & -2.1875 & -2.21875 & -0.280836176 \\ 6 & -2.21875 & -2.1875 & -2.203125 & -0.119556815 \\ 7 & -2.203125 & -2.1875 & -2.1953125 & -0.040278514 \\ 8 & -2.1953125 & -2.1875 & -2.19140625 & -0.000985195 \\ 9 & -2.19140625 & -2.1875 & -2.18945312 & 0.018574337 \\ 10 & -2.19140625 & -2.18945312 & -2.19042969 & 0.008801851 \\ 11 & -2.19140625 & -2.19042969 & -2.19091797 & 0.003910147 \\ 12 & -2.19140625 & -2.19091797 & -2.19116211 & 0.00146293 \\ 13 & -2.19140625 & -2.19116211 & -2.19128418 & 0.000238981 \\ 14 & -2.19140625 & -2.19128418 & -2.19134521 & -0.000373078 \\ 15 & -2.19134521 & -2.19128418 & -2.1913147 & -0.000067041 \\ 16 & -2.1913147 & -2.19128418 & -2.19129944 & 0.000085972 \\ \bottomrule \end {longtable} \par So \(p \approx \num {-2.191299}\). \par \item \(f(\num {0.2}) \approx \num {-0.283986684}\) and \(f(\num {0.3}) \approx \num {0.006600946}\) have the opposite signs, so there's a root in \(\interval {\num {0.2}}{\num {0.3}}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {\num {0.3} - \num {0.2}}{2^n} < 10^{-5} \iff n \geq 14\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0.2 & 0.3 & 0.25 & -0.132771895 \\ 2 & 0.25 & 0.3 & 0.275 & -0.061583071 \\ 3 & 0.275 & 0.3 & 0.2875 & -0.027112719 \\ 4 & 0.2875 & 0.3 & 0.29375 & -0.010160959 \\ 5 & 0.29375 & 0.3 & 0.296875 & -0.001756232 \\ 6 & 0.296875 & 0.3 & 0.2984375 & 0.002428306 \\ 7 & 0.296875 & 0.2984375 & 0.29765625 & 0.000337524 \\ 8 & 0.296875 & 0.29765625 & 0.297265625 & -0.000708983 \\ 9 & 0.297265625 & 0.29765625 & 0.297460938 & -0.000185637 \\ 10 & 0.297460938 & 0.29765625 & 0.297558594 & 0.000075967 \\ 11 & 0.297460938 & 0.297558594 & 0.297509766 & -0.000054829 \\ 12 & 0.297509766 & 0.297558594 & 0.29753418 & 0.00001057 \\ 13 & 0.297509766 & 0.29753418 & 0.297521973 & -0.000022129 \\ 14 & 0.297521973 & 0.29753418 & 0.297528076 & -0.000005779 \\ \bottomrule \end {longtable} \par So \(p \approx \num {0.297528}\). \end {enumerate}}||exercise-6=={\begin {enumerate}[label = (\alph *)] \item \(f(1) \approx \num {0.281718172}\) and \(f(2) \approx \num {-1.389056099}\) have the opposite signs, so there's a root in \(\interval {1}{2}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {2 - 1}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 1 & 2 & 1.5 & 0.01831093 \\ 2 & 1.5 & 2 & 1.75 & -0.504602676 \\ 3 & 1.5 & 1.75 & 1.625 & -0.203419037 \\ 4 & 1.5 & 1.625 & 1.5625 & -0.083233182 \\ 5 & 1.5 & 1.5625 & 1.53125 & -0.030203153 \\ 6 & 1.5 & 1.53125 & 1.515625 & -0.005390404 \\ 7 & 1.5 & 1.515625 & 1.5078125 & 0.006598107 \\ 8 & 1.5078125 & 1.515625 & 1.51171875 & 0.000638447 \\ 9 & 1.51171875 & 1.515625 & 1.51367188 & -0.002367313 \\ 10 & 1.51171875 & 1.51367188 & 1.51269531 & -0.000862268 \\ 11 & 1.51171875 & 1.51269531 & 1.51220703 & -0.00011137 \\ 12 & 1.51171875 & 1.51220703 & 1.51196289 & 0.000263674 \\ 13 & 1.51196289 & 1.51220703 & 1.51208496 & 0.000076186 \\ 14 & 1.51208496 & 1.51220703 & 1.512146 & -0.000017584 \\ 15 & 1.51208496 & 1.512146 & 1.51211548 & 0.000029303 \\ 16 & 1.51211548 & 1.512146 & 1.51213074 & 0.00000586 \\ 17 & 1.51213074 & 1.512146 & 1.51213837 & -0.000005861 \\ \bottomrule \end {longtable} \par So \(p \approx \num {1.512138}\). \par \item \(f(0) = 2\) and \(f(1) \approx \num {0.902625089}\) have the same sign, so there's no root in \(\interval {0}{1}\). \par \item \(f(1) = 1\) and \(f(2) = \num {-0.693147181}\) have the opposite signs, so there's a root in \(\interval {1}{2}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {2 - 1}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & 1 & 2 & 1.5 & -0.155465108 \\ 2 & 1 & 1.5 & 1.25 & 0.339356449 \\ 3 & 1.25 & 1.5 & 1.375 & 0.072171269 \\ 4 & 1.375 & 1.5 & 1.4375 & -0.046499244 \\ 5 & 1.375 & 1.4375 & 1.40625 & 0.011612476 \\ 6 & 1.40625 & 1.4375 & 1.421875 & -0.017747908 \\ 7 & 1.40625 & 1.421875 & 1.4140625 & -0.003144013 \\ 8 & 1.40625 & 1.4140625 & 1.41015625 & 0.004215136 \\ 9 & 1.41015625 & 1.4140625 & 1.41210938 & 0.00053079 \\ 10 & 1.41210938 & 1.4140625 & 1.41308594 & -0.001307804 \\ 11 & 1.41210938 & 1.41308594 & 1.41259766 & -0.000388805 \\ 12 & 1.41210938 & 1.41259766 & 1.41235352 & 0.000070918 \\ 13 & 1.41235352 & 1.41259766 & 1.41247559 & -0.000158962 \\ 14 & 1.41235352 & 1.41247559 & 1.41241455 & -0.000044027 \\ 15 & 1.41235352 & 1.41241455 & 1.41238403 & 0.000013444 \\ 16 & 1.41238403 & 1.41241455 & 1.41239929 & -0.000015292 \\ 17 & 1.41238403 & 1.41239929 & 1.41239166 & -0.000000924 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {1.412392}\). \par \item \(f(0) = 1\) and \(f(1) = \num {-0.5}\) have the opposite signs, so there's a root in \(\interval {0}{\num {0.5}}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq \frac {\num {0.5} - 0}{2^n} < 10^{-5} \iff n \geq 16\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0 & 0.5 & 0.25 & -0.164213562 \\ 2 & 0 & 0.25 & 0.125 & 0.359633135 \\ 3 & 0.125 & 0.25 & 0.1875 & 0.076359534 \\ 4 & 0.1875 & 0.25 & 0.21875 & -0.050036568 \\ 5 & 0.1875 & 0.21875 & 0.203125 & 0.011726391 \\ 6 & 0.203125 & 0.21875 & 0.2109375 & -0.019525681 \\ 7 & 0.203125 & 0.2109375 & 0.20703125 & -0.003990833 \\ 8 & 0.203125 & 0.20703125 & 0.205078125 & 0.003845166 \\ 9 & 0.205078125 & 0.20703125 & 0.206054688 & -0.00007851 \\ 10 & 0.205078125 & 0.206054688 & 0.205566406 & 0.001881912 \\ 11 & 0.205566406 & 0.206054688 & 0.205810547 & 0.000901347 \\ 12 & 0.205810547 & 0.206054688 & 0.205932617 & 0.00041133 \\ 13 & 0.205932617 & 0.206054688 & 0.205993652 & 0.000166388 \\ 14 & 0.205993652 & 0.206054688 & 0.20602417 & 0.000043934 \\ 15 & 0.20602417 & 0.206054688 & 0.206039429 & -0.000017289 \\ 16 & 0.20602417 & 0.206039429 & 0.206031799 & 0.000013322 \\ \bottomrule \end {longtable} \par So \(p \approx \num {0.206032}\). \end {enumerate}}||exercise-7=={\begin {enumerate}[label = (\alph *)] \item Graph of \(y = x\) and \(y = 2 \sin {x}\) is as follow: \par \begin {center} \subfile {graphics/exercise_7_graph/exercise_7_graph.tex} \end {center} \par \item According to the graph, the first positive root \(p\) of \(f = x - 2 \sin {x}\) is in \(\interval {\dfrac {\pi }{2}}{\pi }\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) in that interval is: \par \[\abs {p_n - p} \leq \frac {\pi - \dfrac {\pi }{2}}{2^n} < 10^{-5} \iff n \geq 18\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 1.57079633 & 3.14159265 & 2.35619449 & 0.941980928 \\ 2 & 1.57079633 & 2.35619449 & 1.96349541 & 0.115736343 \\ 3 & 1.57079633 & 1.96349541 & 1.76714587 & -0.194424693 \\ 4 & 1.76714587 & 1.96349541 & 1.86532064 & -0.048560033 \\ 5 & 1.86532064 & 1.96349541 & 1.91440802 & 0.031319893 \\ 6 & 1.86532064 & 1.91440802 & 1.88986433 & -0.009192031 \\ 7 & 1.88986433 & 1.91440802 & 1.90213618 & 0.010921526 \\ 8 & 1.88986433 & 1.90213618 & 1.89600025 & 0.000829072 \\ 9 & 1.88986433 & 1.89600025 & 1.89293229 & -0.004190408 \\ 10 & 1.89293229 & 1.89600025 & 1.89446627 & -0.001682899 \\ 11 & 1.89446627 & 1.89600025 & 1.89523326 & -0.000427471 \\ 12 & 1.89523326 & 1.89600025 & 1.89561676 & 0.000200661 \\ 13 & 1.89523326 & 1.89561676 & 1.89542501 & -0.00011344 \\ 14 & 1.89542501 & 1.89561676 & 1.89552088 & 0.000043602 \\ 15 & 1.89542501 & 1.89552088 & 1.89547295 & -0.000034921 \\ 16 & 1.89547295 & 1.89552088 & 1.89549692 & 0.00000434 \\ 17 & 1.89547295 & 1.89549692 & 1.89548493 & -0.000015291 \\ 18 & 1.89548493 & 1.89549692 & 1.89549092 & -0.000005476 \\ \bottomrule \end {longtable} \par So \(p \approx \num {1.895491}\). \end {enumerate}}||exercise-8=={\begin {enumerate}[label = (\alph *)] \item Graph of \(y = x\) and \(y = \tan {x}\) is as follow: \par \begin {center} \subfile {graphics/exercise_8_graph/exercise_8_graph.tex} \end {center} \par \item According to the graph, the first positive root \(p\) of \(f = x - \tan {x}\) is in \(\interval {\pi }{\dfrac {3 \pi }{2}}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) in that interval is: \par \[\abs {p_n - p} \leq \frac {\frac {3 \pi }{2} - \pi }{2^n} < 10^{-5} \iff n \geq 18\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 3.14159265 & 4.71238898 & 3.92699082 & 2.92699082 \\ 2 & 3.92699082 & 4.71238898 & 4.3196899 & 1.90547634 \\ 3 & 4.3196899 & 4.71238898 & 4.51603944 & -0.511300053 \\ 4 & 4.3196899 & 4.51603944 & 4.41786467 & 1.12130646 \\ 5 & 4.41786467 & 4.51603944 & 4.46695205 & 0.474728271 \\ 6 & 4.46695205 & 4.51603944 & 4.49149575 & 0.038293523 \\ 7 & 4.49149575 & 4.51603944 & 4.50376759 & -0.219861735 \\ 8 & 4.49149575 & 4.50376759 & 4.49763167 & -0.086980389 \\ 9 & 4.49149575 & 4.49763167 & 4.49456371 & -0.023432692 \\ 10 & 4.49149575 & 4.49456371 & 4.49302973 & 0.007653323 \\ 11 & 4.49302973 & 4.49456371 & 4.49379672 & -0.007833371 \\ 12 & 4.49302973 & 4.49379672 & 4.49341322 & -0.00007602 \\ 13 & 4.49302973 & 4.49341322 & 4.49322148 & 0.003792144 \\ 14 & 4.49322148 & 4.49341322 & 4.49331735 & 0.001858936 \\ 15 & 4.49331735 & 4.49341322 & 4.49336529 & 0.000891677 \\ 16 & 4.49336529 & 4.49341322 & 4.49338925 & 0.000407883 \\ 17 & 4.49338925 & 4.49341322 & 4.49340124 & 0.000165946 \\ 18 & 4.49340124 & 4.49341322 & 4.49340723 & 0.000044966 \\ \bottomrule \end {longtable} \par So \(p \approx \num {4.493407}\). \end {enumerate}}||exercise-9=={\begin {enumerate}[label = (\alph *)] \item The graphs of the 2 functions are as follow: \par \begin {center} \subfile {graphics/exercise_9_graph/exercise_9_graph.tex} \end {center} \par \item Let \(f = e^x - 2 - \cos {e^x - 2}\). \(f(\num {0.5}) \approx \num {-1.290212} \) and \(f(\num {1.5}) \approx \num {3.27174}\) have the opposite signs, so there's a root \(p\) of \(f\) in \(\interval {\num {0.5}}{\num {1.5}}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-5}\) in that interval is: \par \[\abs {p_n - p} \leq \frac {\num {1.5} - \num {0.5}}{2^n} < 10^{-5} \iff n \geq 17\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0.5 & 1.5 & 1 & -0.034655726 \\ 2 & 1 & 1.5 & 1.25 & 1.40997635 \\ 3 & 1 & 1.25 & 1.125 & 0.609079747 \\ 4 & 1 & 1.125 & 1.0625 & 0.266982288 \\ 5 & 1 & 1.0625 & 1.03125 & 0.111147764 \\ 6 & 1 & 1.03125 & 1.015625 & 0.037002875 \\ 7 & 1 & 1.015625 & 1.0078125 & 0.000864425 \\ 8 & 1 & 1.0078125 & 1.00390625 & -0.016972716 \\ 9 & 1.00390625 & 1.0078125 & 1.00585938 & -0.00807344 \\ 10 & 1.00585938 & 1.0078125 & 1.00683594 & -0.003609335 \\ 11 & 1.00683594 & 1.0078125 & 1.00732422 & -0.001373662 \\ 12 & 1.00732422 & 1.0078125 & 1.00756836 & -0.00025492 \\ 13 & 1.00756836 & 1.0078125 & 1.00769043 & 0.000304677 \\ 14 & 1.00756836 & 1.00769043 & 1.00762939 & 0.000024859 \\ 15 & 1.00756836 & 1.00762939 & 1.00759888 & -0.000115035 \\ 16 & 1.00759888 & 1.00762939 & 1.00761414 & -0.000045089 \\ \bottomrule \end {longtable} \par So \(p \approx \num {1.007614}\). \end {enumerate}}||exercise-10=={\(f\) has 5 zeros: \(\pm 2\), \(\pm 1\), \(0\). \par \begin {enumerate}[label = (\alph *)] \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.1] S[table-format=1.1] S[table-format=-1.1] S[table-format=-1.8]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -1.5 & 2.5 & 0.5 & 0.52734375 \\ 2 & -1.5 & 0.5 & -0.5 & -1.58203125 \\ 3 & -0.5 & 0.5 & 0 & 0 \\ \bottomrule \end {tabular} \end {table} \par So when applied on \(\interval {\num {-1.5}}{\num {2.5}}\), the Bisection method gives \(0\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.1] S[table-format=1.2] S[table-format=1.3] S[table-format=1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -0.5 & 2.4 & 0.95 & 0.001398666 \\ 2 & -0.5 & 0.95 & 0.225 & 0.62070919 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 2\), the interval shrinks to \(\interval {\num {-0.5}}{\num {0.95}}\). So when applied on \(\interval {\num {-0.5}}{\num {2.4}}\), the Bisection method gives \(0\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.2] S[table-format=1] S[table-format=1.3] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -0.5 & 3 & 1.25 & -0.241012573 \\ 2 & 1.25 & 3 & 2.125 & 15.2352825 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 2\), the interval shrinks to \(\interval {\num {1.25}}{3}\). So when applied on \(\interval {\num {-0.5}}{3}\), the Bisection method gives \(2\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1] S[table-format=-1.2] S[table-format=-1.3] S[table-format=-2.7]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -3 & -0.5 & -1.75 & -19.1924286 \\ 2 & -3 & -1.75 & -2.375 & 283.204185 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 2\), the interval shrinks to \(\interval {\-3}{\num {-1.75}}\). So when applied on \(\interval {-3}{\num {-0.5}}\), the Bisection method gives \(-2\). \end {enumerate}}||exercise-11=={\(f\) has 5 zeros: \(\pm 2\), \(\pm 1\), \(0\). \par \begin {enumerate}[label = (\alph *)] \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.3] S[table-format=1.1] S[table-format=-1.4] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -3 & 2.5 & -0.25 & -1.44195557 \\ 2 & -0.25 & 2.5 & 1.125 & -0.012767315 \\ 3 & 1.125 & 2.5 & 1.8125 & -1.95457248 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 3\), the interval shrinks to \(\interval {\num {1.125}}{\num {2.5}}\). So when applied on \(\interval {-3}{\num {2.5}}\), the Bisection method gives \(2\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.1] S[table-format=-1.3] S[table-format=-1.4] S[table-format=2.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -2.5 & 3 & 0.25 & 0.519104004 \\ 2 & -2.5 & 0.25 & -1.125 & 3.68975401 \\ 3 & -2.5 & -1.125 & -1.8125 & 23.4201732 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 3\), the interval shrinks to \(\interval {\num {-2.5}}{\num {-1.125}}\). So when applied on \(\interval {\num {-2.5}}{3}\), the Bisection method gives \(-2\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.2] S[table-format=-1.3] S[table-format=-1.4] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -1.75 & 1.5 & -0.125 & -0.620491505 \\ 2 & -1.75 & -0.125 & -0.9375 & -1.33009678 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 2\), the interval shrinks to \(\interval {\num {-1.75}}{\num {-0.125}}\). So when applied on \(\interval {\num {-1.75}}{\num {1.5}}\), the Bisection method gives \(-1\). \par \item Applying Bisection method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.3] S[table-format=1.2] S[table-format=1.4] S[table-format=1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 1 & -1.5 & 1.75 & 0.125 & 0.375359058 \\ 2 & 0.125 & 1.75 & 0.9375 & 0.001384076 \\ \bottomrule \end {tabular} \end {table} \par At \(n = 2\), the interval shrinks to \(\interval {\num {0.125}}{\num {1.75}}\). So when applied on \(\interval {\num {-1.5}}{\num {1.75}}\), the Bisection method gives \(1\). \end {enumerate}}||exercise-12=={Let \(f(x) = x^2 - 3\). The positive zero of \(f\) is \(\sqrt {3}\), so by approximating that positive zero, we get an approximation of \(\sqrt {3}\). \par The positive zero of \(f\) clearly is inside \(\interval {1}{2}\). Using Bisection, the number of iteration \(n\) needed to approximate \(\sqrt {3}\) to within \(10^{-4}\) in that interval is: \par \[\frac {2 - 1}{2^n} < 10^{-4} \iff n \geq 14\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 1 & 2 & 1.5 & -0.75 \\ 2 & 1.5 & 2 & 1.75 & 0.0625 \\ 3 & 1.5 & 1.75 & 1.625 & -0.359375 \\ 4 & 1.625 & 1.75 & 1.6875 & -0.15234375 \\ 5 & 1.6875 & 1.75 & 1.71875 & -0.045898438 \\ 6 & 1.71875 & 1.75 & 1.734375 & 0.008056641 \\ 7 & 1.71875 & 1.734375 & 1.7265625 & -0.018981934 \\ 8 & 1.7265625 & 1.734375 & 1.73046875 & -0.005477905 \\ 9 & 1.73046875 & 1.734375 & 1.73242188 & 0.001285553 \\ 10 & 1.73046875 & 1.73242188 & 1.73144531 & -0.00209713 \\ 11 & 1.73144531 & 1.73242188 & 1.73193359 & -0.000406027 \\ 12 & 1.73193359 & 1.73242188 & 1.73217773 & 0.000439703 \\ 13 & 1.73193359 & 1.73217773 & 1.73205566 & 0.000016823 \\ 14 & 1.73193359 & 1.73205566 & 1.73199463 & -0.000194605 \\ \bottomrule \end {longtable} \par So \(\sqrt {3} \approx \num {1.73199}\).}||exercise-13=={Let \(f(x) = x^3 - 25\). The zero of \(f\) is \(\sqrt [3]{25}\), so by approximating that positive zero, we get an approximation of \(\sqrt [3]{25}\). \par The positive zero of \(f\) clearly is inside \(\interval {2}{3}\). Using Bisection, the number of iteration \(n\) needed to approximate \(\sqrt [3]{25}\) to within \(10^{-4}\) in that interval is: \par \[\frac {3 - 2}{2^n} < 10^{-4} \iff n \geq 14\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 2 & 3 & 2.5 & -9.375 \\ 2 & 2.5 & 3 & 2.75 & -4.203125 \\ 3 & 2.75 & 3 & 2.875 & -1.23632812 \\ 4 & 2.875 & 3 & 2.9375 & 0.347412109 \\ 5 & 2.875 & 2.9375 & 2.90625 & -0.452972412 \\ 6 & 2.90625 & 2.9375 & 2.921875 & -0.054920197 \\ 7 & 2.921875 & 2.9375 & 2.9296875 & 0.145709515 \\ 8 & 2.921875 & 2.9296875 & 2.92578125 & 0.045260727 \\ 9 & 2.921875 & 2.92578125 & 2.92382812 & -0.004863195 \\ 10 & 2.92382812 & 2.92578125 & 2.92480469 & 0.020190398 \\ 11 & 2.92382812 & 2.92480469 & 2.92431641 & 0.00766151 \\ 12 & 2.92382812 & 2.92431641 & 2.92407227 & 0.001398635 \\ 13 & 2.92382812 & 2.92407227 & 2.9239502 & -0.001732411 \\ 14 & 2.9239502 & 2.92407227 & 2.92401123 & -0.000166921 \\ \bottomrule \end {longtable} \par So \(\sqrt [3]{25} \approx \num {2.92401}\).}||exercise-14=={Let \(f(x) = x^3 + x − 4\). \(f(1) = -2\) and \(f(4) = 64\) have the opposite signs, so there's a root \(p\) of \(f\) in \(\interval {1}{4}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-3}\) in that interval is: \par \[\abs {p_n - p} \leq \frac {4 - 1}{2^n} < 10^{-3} \iff n \geq 12\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 1 & 4 & 2.5 & 14.125 \\ 2 & 1 & 2.5 & 1.75 & 3.109375 \\ 3 & 1 & 1.75 & 1.375 & -0.025390625 \\ 4 & 1.375 & 1.75 & 1.5625 & 1.37719727 \\ 5 & 1.375 & 1.5625 & 1.46875 & 0.637176514 \\ 6 & 1.375 & 1.46875 & 1.421875 & 0.296520233 \\ 7 & 1.375 & 1.421875 & 1.3984375 & 0.13326025 \\ 8 & 1.375 & 1.3984375 & 1.38671875 & 0.053363502 \\ 9 & 1.375 & 1.38671875 & 1.38085938 & 0.013844214 \\ 10 & 1.375 & 1.38085938 & 1.37792969 & -0.005808686 \\ 11 & 1.37792969 & 1.38085938 & 1.37939453 & 0.004008885 \\ 12 & 1.37792969 & 1.37939453 & 1.37866211 & -0.000902119 \\ \bottomrule \end {longtable} \par So \(p \approx \num {1.3787}\).}||exercise-15=={Let \(f(x) = x^3 - x − 1\). \(f(1) = -2\) and \(f(4) = 64\) have the opposite signs, so there's a root \(p\) of \(f\) in \(\interval {1}{2}\). \par The number of iteration \(n\) needed to approximate \(p\) to within \(10^{-4}\) in that interval is: \par \[\abs {p_n - p} \leq \frac {2 - 1}{2^n} < 10^{-4} \iff n \geq 14\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.8] S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 1 & 2 & 1.5 & 0.875 \\ 2 & 1 & 1.5 & 1.25 & -0.296875 \\ 3 & 1.25 & 1.5 & 1.375 & 0.224609375 \\ 4 & 1.25 & 1.375 & 1.3125 & -0.051513672 \\ 5 & 1.3125 & 1.375 & 1.34375 & 0.082611084 \\ 6 & 1.3125 & 1.34375 & 1.328125 & 0.014575958 \\ 7 & 1.3125 & 1.328125 & 1.3203125 & -0.018710613 \\ 8 & 1.3203125 & 1.328125 & 1.32421875 & -0.002127945 \\ 9 & 1.32421875 & 1.328125 & 1.32617188 & 0.00620883 \\ 10 & 1.32421875 & 1.32617188 & 1.32519531 & 0.002036651 \\ 11 & 1.32421875 & 1.32519531 & 1.32470703 & -0.000046595 \\ 12 & 1.32470703 & 1.32519531 & 1.32495117 & 0.000994791 \\ 13 & 1.32470703 & 1.32495117 & 1.3248291 & 0.000474039 \\ 14 & 1.32470703 & 1.3248291 & 1.32476807 & 0.000213707 \\ \bottomrule \end {longtable} \par So \(p \approx \num {1.32477}\).}||exercise-16=={For \(f(p_n) < 10^{-3}\), it is required that \(n > 1\) as: \par \begin {align*} &\qquad & f(p_n) & < 10^{-3} \\ \iff && (p_n - 1)^{10} & < 10^{-3} \\ \iff && \frac {1}{n^{10}} & < 10^{-3} \\ \iff && n & > 1 \end {align*} \par For \(\abs {p - p_n} < 10^{-3}\), it is required that \(n > 1000\) as: \par \begin {align*} &\qquad & \abs {p - p_n} & < 10^{-3} \\ \iff && \frac {1}{n} & < 10^{-3} \\ \iff && n & > 1000 \end {align*} \par \qed }||exercise-17=={It's clear that the difference of 2 consecutive terms goes to zero: \par \[\lim _{n \to \infty } (p_n - p_{n - 1}) = \lim _{n \to \infty } \frac {1}{n} = 0\] \par However, the sequence diverges as: \par \begin {align*} p_n & = \sum _{k = 1}^{n} \frac {1}{k} \\ & = 1 + \frac {1}{2} + \frac {1}{3} + \frac {1}{4} + \ldots \\ & > 1 + (\frac {1}{2}) + (\frac {1}{4} + \frac {1}{4}) + \ldots \\ & = 1 + \frac {1}{2} + \frac {1}{2} + \ldots \\ & = \infty \end {align*}}||exercise-18=={Let \(p\) be the zero converged by Bisection. \par With \(-1 < a < 0\) and \(2 < b < 3\): \par \begin {gather*} \sin {\pi a} < 0 \\ \sin {\pi b} > 0 \\ 1 < a + b < 3 \end {gather*} \par \begin {enumerate}[label = (\alph *)] \item If \(a + b < 2\), then \(\num {0.5} < p_1 = \frac {a + b}{2} < 1\). Then \(\sin {p_1} > 0\), and the interval shrinks to \(\interval {a}{p_1}\). \(0\) is the only zero in that interval, so \(p = 0\). \par \item If \(a + b > 2\), then \(1 < p_1 = \frac {a + b}{2} < \num {1.5}\). Then \(\sin {p_1} < 0\), and the interval shrinks to \(\interval {p_1}{b}\). \(2\) is the only zero in that interval, so \(p = 0\). \par \item If \(a + b = 2\), then \(p_1 = \frac {a + b}{2} = 1\). Then \(\sin {p_1} = 0\), and a zero \(p = 1\) is found. \end {enumerate}}||exercise-19=={Let \(d\) be the depth of the water, so \(d = r - h\). Let \par \[f(h) = 10 (\num {0.5} \pi - \arcsin (h) - h \sqrt {1 - h^2}) - \num {12.4}\] \par Instead of finding \(d\) directly, we find \(h\), also to within \(\SI {0.01}{ft}\). The number of iteration \(n\) needed to approximate \(h\) to within \(\num {0.01}\) in \(\interval {0}{r}\) is: \par \[\abs {h - h_n} < \frac {1 - 0}{2^n} < \num {0.01} \iff n \geq 7\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=1.5] S[table-format=1.6] S[table-format=1.7] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0 & 1 & 0.5 & -6.25815151 \\ 2 & 0 & 0.5 & 0.25 & -1.63945387 \\ 3 & 0 & 0.25 & 0.125 & 0.814489029 \\ 4 & 0.125 & 0.25 & 0.1875 & -0.419946724 \\ 5 & 0.125 & 0.1875 & 0.15625 & 0.195725903 \\ 6 & 0.15625 & 0.1875 & 0.171875 & -0.112536394 \\ 7 & 0.15625 & 0.171875 & 0.1640625 & 0.041493241 \\ \bottomrule \end {longtable} \par So \(h \approx \num {0.1641}\), hence \(d = r - h \approx \num {0.8359}\).}||exercise-20=={As \(\omega < 0\), the plane rotates clockwise. After \SI {1}{\s }, the particle still sticks to the plane, so: \par \[\theta (1) < \frac {\pi }{2} \iff - \frac {\pi }{2} < \omega < 0\] \par After \SI {1}{\s }, the particle has moved \SI {1.7}{ft}, so that: \par \[x(1) = \num {1.7} = - \frac {\num {32.17}}{2 \omega ^2} \left (\frac {e^{\omega t} - e^{- \omega t}}{2} - \sin \omega t\right )\] \par Let \par \[f(\omega ) = \num {3.4} \omega ^2 + \num {32.17} \left (\frac {e^{\omega t} - e^{- \omega t}}{2} - \sin \omega t\right )\] \par The root of the above function in \(\interval [open]{- \frac {\pi }{2}}{0}\) will be the solution of the problem. \par Applying Bisection on \(f\) on \(\interval {- \frac {\pi }{2}}{0}\) fails as \(f(0) = 0\). We need to expand (arbitrarily even) the searching interval a bit for the method to work, and check the solution later on. Hence, we use the interval \(\interval {- \frac {\pi }{2}}{1}\). \par The number of iteration \(n\) needed to approximate \(\omega \) to within \(10^{-5}\) is: \par \[\abs {\omega - \omega _n} < \frac {1 - (- \num {0.5} \pi )}{2^n} < 10^{-5} \iff n \geq 18\] \par Applying Bisection method generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-1.9] S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & -1.57079633 & 1 & -0.285398163 & 0.027657569 \\ 2 & -1.57079633 & -0.285398163 & -0.928097245 & -5.65148786 \\ 3 & -0.928097245 & -0.285398163 & -0.606747704 & -1.14396969 \\ 4 & -0.606747704 & -0.285398163 & -0.446072934 & -0.275313029 \\ 5 & -0.446072934 & -0.285398163 & -0.365735549 & -0.06982238 \\ 6 & -0.365735549 & -0.285398163 & -0.325566856 & -0.009667545 \\ 7 & -0.325566856 & -0.285398163 & -0.30548251 & 0.011587981 \\ 8 & -0.325566856 & -0.30548251 & -0.315524683 & 0.001641051 \\ 9 & -0.325566856 & -0.315524683 & -0.320545769 & -0.003838965 \\ 10 & -0.320545769 & -0.315524683 & -0.318035226 & -0.001055895 \\ 11 & -0.318035226 & -0.315524683 & -0.316779954 & 0.00030328 \\ 12 & -0.318035226 & -0.316779954 & -0.31740759 & -0.000373625 \\ 13 & -0.31740759 & -0.316779954 & -0.317093772 & -0.000034503 \\ 14 & -0.317093772 & -0.316779954 & -0.316936863 & 0.000134556 \\ 15 & -0.317093772 & -0.316936863 & -0.317015318 & 0.000050068 \\ 16 & -0.317093772 & -0.317015318 & -0.317054545 & 0.000007793 \\ 17 & -0.317093772 & -0.317054545 & -0.317074159 & -0.000013352 \\ 18 & -0.317074159 & -0.317054545 & -0.317064352 & -0.000002779 \\ \bottomrule \end {longtable} \par As \(\num {-0.317064} \in \interval [open]{- \frac {\pi }{2}}{0}\), it is a valid approximation of \(\omega \). We conclude that \(\omega \approx \num {-0.317064}\).}||exercise-21=={\begin {enumerate}[label = \alph *)] \item For \(x = p\): \par \[g_1(p) = (3 + p - 2p^2)^{\frac {1}{4}} = (p^4 - f(p))^{\sfrac {1}{4}} = \abs {p}\] \par So \(p\) is a fixed-point of \(g_1\). \par \item For \(x = p\): \par \begin {align*} g_2(p) &= \left (\dfrac {p + 3 - p^4}{2}\right )^{\sfrac {1}{2}} \\ &= \left (\dfrac {2p^2}{2}\right )^{\frac {1}{2}} \\ &= \abs {p} \end {align*} \par So \(p\) is a fixed-point of \(g_2\). \par \item For \(x = p\): \par \begin {align*} g_3(p) &= \left (\dfrac {p + 3}{p^2 + 2}\right )^{\sfrac {1}{2}} \\ &= \left (\dfrac {p^4 + 2p^2}{p^2 + 2}\right )^{\sfrac {1}{2}} \\ &= \abs {p} \end {align*} \par So \(p\) is a fixed-point of \(g_3\). \par \item For \(x = p\): \par \begin {align*} g_4(p) &= \dfrac {3p^4 + 2p^2 + 3}{4p^3 + 4p - 1} \\ &= \dfrac {4p^4 - (3 + p - 2p^2) + 2p^2 + 3}{4p^3 + 4p - 1} \\ &= \dfrac {4p^4 + 4p^2 -p}{4p^3 + 4p - 1} \\ &= p \end {align*} \par So \(p\) is a fixed-point of \(g_4\). \end {enumerate}}||exercise-22=={\begin {enumerate}[label = \alph *)] \item Applying fixed-point method on the four functions \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\) by \(g_1\)} & {\(p_n\) by \(g_2\)} & {\(p_n\) by \(g_3\)} & {\(p_n\) by \(g_4\)} \\ \midrule 0 & 1 & 1 & 1 & 1 \\ 1 & 1.189207115 & 1.224744871 & 1.154700538 & 1.142857143 \\ 2 & 1.080057753 & 0.993666159 & 1.11642741 & 1.12448169 \\ 3 & 1.149671431 & 1.228568645 & 1.126052233 & 1.124123164 \\ 4 & 1.107820053 & 0.987506429 & 1.123638885 & 1.12412303 \\ \bottomrule \end {tabular} \end {table} \par \item \(g_4\) gives the best approximation as it generates the smallest difference between \(p_3\) and \(p_4\): \({\abs {p_4 - p_3} = \num {-134e-7}}\). \end {enumerate}}||exercise-23=={Applying fixed-point method on the four sequences generate the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.9] S[table-format=1.9]} \toprule \(n\) & {\hyperref [exer:2.2.3:a]{a)}} & {\hyperref [exer:2.2.3:b]{b)}} & {\hyperref [exer:2.2.3:c]{c)}} & {\hyperref [exer:2.2.3:d]{d)}} \\ \midrule \endfirsthead \(n\) & {\hyperref [exer:2.2.3:a]{a)}} & {\hyperref [exer:2.2.3:b]{b)}} & {\hyperref [exer:2.2.3:c]{c)}} & {\hyperref [exer:2.2.3:d]{d)}} \\ \midrule \endhead 0 & 1 & 1 & 1 & 1 \\ 1 & 1.952380952 & 7.666666667 & 0 & 4.582575695 \\ 2 & 2.121754174 & 5.230203739 & 0 & 2.140695143 \\ 3 & 2.242849692 & 3.742696919 & & 3.132075595 \\ 4 & 2.334839673 & 2.994853568 & & 2.589366527 \\ 5 & 2.40109338 & 2.777022226 & & 2.847822274 \\ 6 & 2.465059288 & 2.759041866 & & 2.715521253 \\ 7 & 2.512243463 & 2.758924181 & & 2.780885095 \\ 8 & 2.551057096 & 2.758924176 & & 2.748008838 \\ 9 & 2.583237767 & 2.758924176 & & 2.764398093 \\ 10 & 2.610081445 & & & 2.756191284 \\ 11 & 2.632580301 & & & 2.760291639 \\ 12 & 2.651509504 & & & 2.758240699 \\ 13 & 2.667484488 & & & 2.759265978 \\ 14 & 2.681000202 & & & 2.758753291 \\ 15 & 2.692458887 & & & 2.759009623 \\ 16 & 2.702190249 & & & 2.758881454 \\ 17 & 2.710466453 & & & 2.758945538 \\ 18 & 2.717513483 & & & 2.758913496 \\ 19 & 2.723519902 & & & 2.758929517 \\ \bottomrule \end {longtable} \par Apparently, the speed of convergence is ranked in descending order as follow: \hyperref [exer:2.2.3:b]{b)}, \hyperref [exer:2.2.3:d]{d)}, \hyperref [exer:2.2.3:a]{a)}. \hyperref [exer:2.2.3:c]{c)} does not converge.}||exercise-24=={Applying fixed-point method on the four sequences generate the following table: \par \begin {longtable}{r c c S[table-format=1.9] S[table-format=1.9]} \toprule \(n\) & {\hyperref [exer:2.2.4:a]{a)}} & {\hyperref [exer:2.2.4:b]{b)}} & {\hyperref [exer:2.2.4:c]{c)}} & {\hyperref [exer:2.2.4:d]{d)}} \\ \midrule \endfirsthead \(n\) & {\hyperref [exer:2.2.4:a]{a)}} & {\hyperref [exer:2.2.4:b]{b)}} & {\hyperref [exer:2.2.4:c]{c)}} & {\hyperref [exer:2.2.4:d]{d)}} \\ \midrule \endhead 0 & 1 & 1 & 2.2 & 1 \\ 1 & 343 & 7 & 1.819763677 & 1.5 \\ 2 & \num {-2.25e25} & \num {-335.857} & 1.58347483 & 1.450520833 \\ 3 & & \num {37884356} & 1.489460974 & 1.498749661 \\ 4 & & & 1.476022436 & 1.451903535 \\ 5 & & & 1.475773246 & 1.497577067 \\ 6 & & & 1.475773162 & 1.45319229 \\ 7 & & & 1.475773162 & 1.496475364 \\ 9 & & & & 1.454396119 \\ 8 & & & & 1.495438587 \\ 10 & & & & 1.45552281 \\ 11 & & & & 1.494461513 \\ 12 & & & & 1.456579138 \\ 13 & & & & 1.493539533 \\ 14 & & & & 1.457571031 \\ 15 & & & & 1.49266856 \\ 16 & & & & 1.458803715 \\ 17 & & & & 1.491844948 \\ 18 & & & & 1.459381814 \\ 19 & & & & 1.491065425 \\ \bottomrule \end {longtable} \par Apparently, the speed of convergence is ranked in descending order as follow: \hyperref [exer:2.2.4:c]{c)}, \hyperref [exer:2.2.4:d]{d)}. \hyperref [exer:2.2.4:a]{a)} and \hyperref [exer:2.2.4:b]{b)} do not converge.}||exercise-25=={Let \(f(x) = x^4 - 3x^2 - 3\). Let \(p\) be the root of \(f\) in \(\interval {1}{2}\). We need to find a function \(g\) for which \(p = g(p)\) to perform the fixed-point method. \par Extract \(p\) to RHS gives: \par \[p^4 = 3p^2 + 3 \iff \abs {p} = (3p^2 + 3)^{\sfrac {1}{4}}\] \par Then \(g\) is chosen as: \par \[g(x) = (3x^2 + 3)^{\sfrac {1}{4}}\] \par Applying fixed-point method on \(g\) generate the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 1 & 4 & 1.922847844 \\ 1 & 1.56508458 & 5 & 1.93750754 \\ 2 & 1.793572879 & 6 & 1.94331693 \\ 3 & 1.885943743 & & \\ \bottomrule \end {tabular} \end {table} \par We can try the other obvious option \par \[g(x) = \left (\frac {x^4 - 3}{3}\right )^{\num {0.5}}\] \par \noindent which fails on the first iteration. A reasonable explanation for the choice of \(g\) is that we need \(\abs {g'}\) to be as small as possible. On \(\interval {1}{2}\), the \(O(x^{\num {0.5}})\) of the first choice clearly has an advantage over \(O(x^2)\) of the second choice of \(g\). \par We conclude that \(p \approx \num {1.943}\).}||exercise-26=={Let \(f(x) = x^3 - x - 1 = 0\). Let \(p\) be the root of \(f\) in \(\interval {1}{2}\). We need to find a function \(g\) for which \(p = g(p)\) to perform the fixed-point method. \par Extract \(p\) to RHS gives: \par \[p^3 = p + 1 \iff p = (p + 1)^{\sfrac {1}{3}}\] \par Then \(g\) is chosen as: \par \[g(x) = (p + 1)^{\sfrac {1}{3}}\] \par Applying fixed-point method on \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 1 & 3 & 1.322353819 \\ 1 & 1.25992105 & 4 & 1.324268745 \\ 2 & 1.312293837 & & \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.324}\).}||exercise-27=={From the formula of \(g\): \par \begin {align*} g(x) & = \pi + \num {0.5} \sin {\num {0.5} x} \\ \Rightarrow g(x) & \in \interval {\pi - \num {0.5}}{\pi + \num {0.5}} \, \forall x \end {align*} \par Consider the interval \(I = \interval {\pi - \num {0.5}}{\pi + \num {0.5}} \in \interval {0}{2 \pi }\). From the above equations, we know that: \par \begin {itemize} \item \(g \in C I\) \item \(g(x) \in I \, \forall x \in I\) \end {itemize} \par According to Theorem 2.3, there exists a fixed point of \(g\) on \(I\). \par Differentiating \(g\) gives: \par \[g'(x) = -\num {0.25} \cos {\num {0.5} x} \Rightarrow \abs {g'(x)} \leq k = \num {0.25} < 1 \, \forall x\] \par Again, according to Theorem 2.3, there exists one and only one fixed point of \(g\) on \(I\). \par Applying fixed-point method on \(g\), with \(p_0 = \pi \), generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 3.141592654 & 2 & 3.626048864 \\ 1 & 3.641592654 & 3 & 3.626995622 \\ \bottomrule \end {tabular} \end {table} \par Using corollary 2.5, the number of iterations \(n\) required to achieve \(10^{-2}\) accuracy is \par \[\abs {p_n - p} \leq k^n \num {0.5} < 10^{-2} \iff n \geq 3\] \par \noindent which is in line with the number of iteration actually performed.}||exercise-28=={From the formula of \(g\): \par \begin {align*} g(x) & = 2^{-x} \\ \Rightarrow g'(x) & = - 2^{-x} \ln {2} \end {align*} \par It is clear that \(g \in C^1 R\). \par Consider the interval \(I = \interval {\frac {1}{3}}{1}\), \(I_{open} = \interval [open]{\frac {1}{3}}{1}\): \par \begin {align*} &g'(x) < 0 \forall x \in I \\ &\Rightarrow 1 > g(\frac {1}{3}) = 2^{-\sfrac {1}{3}} \geq g(x) \geq g(1) = 2^{-1} > \frac {1}{3} \\ &\Rightarrow g(x) \in I \, \forall x \in I \end {align*} \par So far, we know that: \par \begin {itemize} \item \(g \in C I\) (\(g \in C R\) even) \item \(g(x) \in I \, \forall x \in I\) \end {itemize} \par According to Theorem 2.3, there exists a fixed point of \(g\) on \(I\). \par Consider \(g'\): \par \begin {gather*} -1 < - \ln {2} \leq g'(x) \leq - \frac {1}{3} \ln {2} < 0 \, \forall x \in I \\ \Rightarrow \abs {g'(x)} \leq k = \ln {2} < 1 \, \forall x \in I \end {gather*} \par Again, according to Theorem 2.3, there exists one and only one fixed point of \(g\) on \(I\). \par Applying fixed-point method on \(g\), with \(p_0 = \frac {2}{3}\), generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 0.666666667 & 5 & 0.640746653 \\ 1 & 0.629960525 & 6 & 0.641380922 \\ 2 & 0.646194096 & 7 & 0.641099006 \\ 3 & 0.638963711 & 8 & 0.641224295 \\ 4 & 0.642174057 & 9 & 0.641168611 \\ \bottomrule \end {tabular} \end {table} \par Using Corollary 2.5, the number of iterations \(n\) required to achieve \(10^{-4}\) accuracy is \par \[\abs {p_n - p} \leq k^n \frac {1}{3} < 10^{-4} \iff n \geq 23\] \par \noindent which is quit a bit higher than the number of iteration actually performed.}||exercise-29=={Let \(f(x) = x^2 - 3\), \(p > 0\) is a zero of \(f\). Then \(p = \sqrt {3}\), and an approximation of \(p\) is an approximation of \(\sqrt {3}\). \par Consider \(g(x) = \frac {3}{x}\). It is clear that this is a bad choice, as applying \(g\) on any \(p_0\) generates a sequence that jumps between \(p_0\) and \(\frac {3}{p_0}\). \par From the textbook examples, we choose \(g(x) = x - \frac {x^2 - 3}{x^2}\). Applying fixed-point method on \(g\) with \(p_0 = \num {1.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 1.5 & 4 & 1.73189858 \\ 1 & 1.83333333 & 5 & 1.73207438 \\ 2 & 1.72589532 & 6 & 1.73204716 \\ 3 & 1.73304114 & & \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(\sqrt {3} \approx \num {1.73205}\). In exercise 12 of section 2.1, 14 iteration is needed, much higher than that of this method.}||exercise-30=={Let \(f(x) = x^3 - 25\), \(p > 0\) is a zero of \(f\). Then \(p = \sqrt [3]{25}\), and an approximation of \(p\) is an approximation of \(\sqrt [3]{25}\). \par We choose \(g(x) = x - \frac {x^3 - 25}{x^3}\). Applying fixed-point method on \(g\) with \(p_0 = \num {2.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 2.5 & 3 & 2.92378369 \\ 1 & 3.1 & 4 & 2.92402386 \\ 2 & 2.93917962 & 5 & 2.92401758 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(\sqrt [3]{25} \approx \num {2.92402}\). In exercise 13 of section 2.1, 14 iteration is needed, much higher than that of this method.}||exercise-31=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} &\quad & g(x) &= \frac {2 - e^x + x^2}{3} \\ \Rightarrow && g'(x) &= \frac {2x - e^x}{3} \\ \Rightarrow && g''(x) &= \frac {2 - e^x}{3} \end {align*} \par It is clear that \(g\) is continuous in \(\mathbb {R}\). \par Consider \(g''\): \par \begin {itemize} \item \(g''(x) > 0 \iff x < \ln {2}\) \item \(g''(x) = 0 \iff x = \ln {2}\) \item \(g''(x) < 0 \iff x > \ln {2}\) \end {itemize} \par So, \(\max g'(x) = g'(\ln {2}) = \dfrac {\ln {4} - 2}{3} < 0\). So \(g\) is monotonically decreasing in \(\mathbb {R}\). \par Consider the interval \(I = \interval {0}{1}\): \par \[\] \begin {align*} 1 > g(0) = \frac {1}{3} > &g(x) > g(1) = \frac {3 - e}{3} > 0 \, \forall x \in I \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \num {0.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 0.5 & 5 & 0.257265636 \\ 1 & 0.200426243 & 6 & 0.257598985 \\ 2 & 0.272749065 & 7 & 0.257512455 \\ 3 & 0.253607157 & 8 & 0.257534914 \\ 4 & 0.258550376 & 9 & 0.257529084 \\ \bottomrule \end {tabular} \end {table} \par We conclude that the fixed point \(p \approx \num {0.257529}\). \par \item Let \par \[g = \frac {5}{x^2} + 2\] \par Consider the interval \(I = \interval {\num {2.5}}{3}\). \(0 \notin I\), so \(g\) is continuous in \(I\). \par \(x^2\) is monotonically increasing in \(I\), so \(g\) is monotonically decreasing in \(I\). So that: \par \begin {align*} 3 > g(\num {2.5}) = \num {2.8} > &g(x) > g(3) = \sfrac {23}{9} > \num {2.5} \, \forall x \in I \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \num {2.75}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 2.75 & 6 & 2.69171092 & 12 & 2.69066691 \\ 1 & 2.66115702 & 7 & 2.69010182 & 13 & 2.69063746 \\ 2 & 2.7060395 & 8 & 2.69092764 & 14 & 2.69065258 \\ 3 & 2.68281293 & 9 & 2.69050363 & 15 & 2.69064482 \\ 4 & 2.69468708 & 10 & 2.69072129 & & \\ 5 & 2.68857829 & 11 & 2.69060954 & & \\ \bottomrule \end {tabular} \end {table} \par We conclude that the fixed point \(p \approx \num {2.690645}\). \par \item Let \par \[g(x) = \left (\frac {e^x}{3}\right )^{\sfrac {1}{2}}\] \par It is clear that \(g\) is continuous in \(\mathbb {R}\). \par \(g\) is monotonically increasing in \(\mathbb {R}\). Consider the interval \(I = \interval {0}{1}\): \par \begin {align*} 0 < g(0) = \dfrac {1}{\sqrt {3}} < &g(x) < g(1) = \sqrt {\dfrac {e}{3}} < 1 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \num {0.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 0.5 & 5 & 0.903281143 & 10 & 0.909876791 \\ 1 & 0.74133242 & 6 & 0.906952163 & 11 & 0.909948068 \\ 2 & 0.836407007 & 7 & 0.908618411 & 12 & 0.909980498 \\ 3 & 0.87712774 & 8 & 0.909375718 & 13 & 0.909995254 \\ 4 & 0.895169428 & 9 & 0.909720122 & 14 & 0.910001967 \\ \bottomrule \end {tabular} \end {table} \par We conclude that the fixed point \(p \approx \num {0.910002}\). \par \item Let \(g(x) = 5^{-x}\). It is clear that \(g\) is continuous in \(\mathbb {R}\). \par \(5^x\) is monotonically increasing in \(\mathbb {R}\), so \(g\) is monotonically decreasing in \(\mathbb {R}\). \par Consider the interval \(I = \interval {0}{1}\): \par \begin {align*} 0 < g(1) = \num {0.2} < &g(x) < g(0) = 1 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \num {0.5}\) generates the following table: \par \begin {longtable}{r S[table-format=1.9] r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endhead 0 & 0.5 & 11 & 0.468245559 & 22 & 0.469685261 \\ 1 & 0.447213595 & 12 & 0.470663369 & 23 & 0.469574052 \\ 2 & 0.486867866 & 13 & 0.468835429 & 24 & 0.469658106 \\ 3 & 0.456766207 & 14 & 0.470216753 & 25 & 0.469594575 \\ 4 & 0.479439843 & 15 & 0.469172549 & 26 & 0.469642593 \\ 5 & 0.462259591 & 16 & 0.469961695 & 27 & 0.4696063 \\ 6 & 0.475219673 & 17 & 0.469365184 & 28 & 0.469633731 \\ 7 & 0.465409992 & 18 & 0.469816013 & 29 & 0.469612998 \\ 8 & 0.47281623 & 19 & 0.469475247 & 30 & 0.469628669 \\ 9 & 0.467213774 & 20 & 0.469732798 & 31 & 0.469616824 \\ 10 & 0.4714456 & 21 & 0.469538128 & 32 & 0.469625777 \\ \bottomrule \end {longtable} \par We conclude that the fixed point \(p \approx \num {0.469626}\). \par \item Let \(g(x) = 6^{-x}\). It is clear that \(g\) is continuous in \(\mathbb {R}\). \par \(6^x\) is monotonically increasing in \(\mathbb {R}\), so \(g\) is monotonically decreasing in \(\mathbb {R}\). \par Consider the interval \(I = \interval {0}{1}\): \par \begin {align*} 0 < g(1) = \frac {1}{6} < &g(x) < g(0) = 1 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \num {0.5}\) generates the following table: \par \begin {longtable}{r S[table-format=1.9] r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endhead 0 & 0.5 & 15 & 0.446190464 & 30 & 0.448132603 \\ 1 & 0.40824829 & 16 & 0.449568975 & 31 & 0.448007263 \\ 2 & 0.481194974 & 17 & 0.446855739 & 32 & 0.448107887 \\ 3 & 0.422238208 & 18 & 0.449033402 & 33 & 0.448027103 \\ 4 & 0.469282988 & 19 & 0.447284756 & 34 & 0.448091958 \\ 5 & 0.431347074 & 20 & 0.448688365 & 35 & 0.448039891 \\ 6 & 0.461686032 & 21 & 0.447561363 & 36 & 0.448081691 \\ 7 & 0.437258678 & 22 & 0.448466044 & 37 & 0.448048133 \\ 8 & 0.456821582 & 23 & 0.447739682 & 38 & 0.448075074 \\ 9 & 0.441086448 & 24 & 0.44832278 & 39 & 0.448053445 \\ 10 & 0.453699216 & 25 & 0.44785463 & 40 & 0.448070809 \\ 11 & 0.443561035 & 26 & 0.448230453 & 41 & 0.448056869 \\ 12 & 0.451692029 & 27 & 0.447928723 & 42 & 0.44806806 \\ 13 & 0.445159128 & 28 & 0.448170951 & 43 & 0.448059076 \\ 14 & 0.450400504 & 29 & 0.447976481 & & \\ \bottomrule \end {longtable} \par We conclude that the fixed point \(p \approx \num {0.448059}\). \par \item Let \(g(x) = \num {0.5}(\sin {x} + \cos {x})\). It is clear that \(g\) is continuous in \(\mathbb {R}\). \par Manipulating \(g\) gives: \par \begin {align*} \sin {x} + \cos {x} &= \sqrt {2} \left (\frac {1}{\sqrt {2}} \sin {x} + \frac {1}{\sqrt {2}} \cos {x}\right ) \\ &= \sqrt {2} \left (\cos {\frac {\pi }{4}} \sin {x} + \sin {\frac {\pi }{4}} \cos {x}\right ) \\ &= \sqrt {2} \sin \left (x + \frac {\pi }{4}\right ) \\ \Rightarrow g(x) &= \num {0.5}(\sin {x} + \cos {x}) \\ &= \frac {1}{\sqrt {2}} \sin \left (x + \frac {\pi }{4}\right ) \end {align*} \par Consider the interval \(I = \interval {0}{\frac {\pi }{4}}\). \(sin{x}\) is monotonically increasing in \(\interval {0}{\frac {\pi }{2}}\), so \(\sin {x + \frac {\pi }{4}}\) also is monotonically increasing in \(I\). It follows that: \par \begin {align*} 0 < g(0) = \num {0.5} < g(x) < &g(\frac {\pi }{4}) = \frac {1}{\sqrt {2}} < \frac {\pi }{4} \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par So, \(I\) is an interval in which a fixed point \(p\) of \(g\) exists. Applying fixed-point method on \(g\) with \(p_0 = \frac {\pi }{8}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 0.392699082 & 4 & 0.704799153 \\ 1 & 0.653281482 & 5 & 0.704811271 \\ 2 & 0.700944543 & 6 & 0.70481196 \\ 3 & 0.70458659 & & \\ \bottomrule \end {tabular} \end {table} \par We conclude that the fixed point \(p \approx \num {0.704812}\). \end {enumerate}}||exercise-32=={\begin {enumerate}[label = \alph *)] \item Let \(I = \interval {2}{3}\) and \par \begin {align*} g(x) &= \sin {x} + 2 \\ \Rightarrow g'(x) &= \cos {x} \end {align*} \par A fixed point \(p\) of \(g\) is also a root of the problem. \par Consider \(g\). It is clear that \(g\) is continuous on \(\mathbb {R}\). \(\sin {x}\) is monotonically decreasing in \(I\), so that: \par \[2 < g(3) = \sin {3} + 2 < g(x) < g(2) = \sin {2} + 2 < 3\] \par Consider \(g'\). \(\cos {x}\) is monotonically decreasing in \(I\), so that: \par \begin {gather*} \cos {3} \leq g'(x) \leq \cos {2} < 0 \, \forall x \in I \\ \Rightarrow \abs {g'(x)} \leq k = - \cos {3} < 1 \end {gather*} \par Therefore, all the conditions in Corollary 2.5 hold. Using Corollary 2.5, with \(p_0 = \num {2.5}\), the number of iteration \(n\) required to obtain approximations accurate to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq k^n \num {0.5} < 10^{-5} \iff n \geq 1076\] \par Applying fixed-point method on \(g\) generates the following table: \par \begin {longtable}{r S[table-format=1.8] r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endhead 0 & 2.5 & 18 & 2.55222543 & 36 & 2.55412346 \\ 1 & 2.59847214 & 19 & 2.55583511 & 37 & 2.55425629 \\ 2 & 2.51680997 & 20 & 2.5528308 & 38 & 2.55414573 \\ 3 & 2.58492102 & 21 & 2.55533177 & 39 & 2.55423776 \\ 4 & 2.52836328 & 22 & 2.55325015 & 40 & 2.55416115 \\ 5 & 2.57551141 & 23 & 2.55498297 & 41 & 2.55422492 \\ 6 & 2.5363287 & 24 & 2.55354068 & 42 & 2.55417184 \\ 7 & 2.56897915 & 25 & 2.55474128 & 43 & 2.55421602 \\ 8 & 2.54183051 & 26 & 2.55374195 & 44 & 2.55417925 \\ 9 & 2.56444615 & 27 & 2.5545738 & 45 & 2.55420986 \\ 10 & 2.54563487 & 28 & 2.5538814 & 46 & 2.55418438 \\ 11 & 2.56130168 & 29 & 2.55445776 & 47 & 2.55420559 \\ 12 & 2.5482673 & 30 & 2.55397801 & 48 & 2.55418793 \\ 13 & 2.55912111 & 31 & 2.55437735 & 49 & 2.55420263 \\ 14 & 2.55008961 & 32 & 2.55404495 & 50 & 2.5541904 \\ 15 & 2.55760933 & 33 & 2.55432164 & 51 & 2.55420058 \\ 16 & 2.55135148 & 34 & 2.55409133 & 52 & 2.5541921 \\ 17 & 2.55656141 & 35 & 2.55428304 & & \\ \bottomrule \end {longtable} \par So one solution of the problem is \(p \approx \num {2.554192}\). \par \item Let \(I = \interval {2}{3}\) and \par \begin {align*} g(x) &= \sqrt [3]{2x + 5} \\ \Rightarrow g'(x) &= \dfrac {2}{3} (2x + 5)^{- \sfrac {2}{3}} \end {align*} \par A fixed point \(p\) of \(g\) is also a solution of the problem. \par Consider \(g\). It is clear that \(g\) is continuous and monotonically increasing on \(\mathbb {R}\), so that: \par \begin {align*} 2 < g(2) = \sqrt [3]{9} < &g(x) < g(3) = \sqrt [3]{11} < 3 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par Consider \(g'\). Since \(- \sfrac {2}{3} < 0\) and \(I > 0\), \(g'(x)\) is monotonically decreasing in \(I\), so that: \par \begin {gather*} g'(2) = \frac {2}{9 \sqrt [3]{3}} \geq g'(x) \geq g'(3) = \frac {2}{3 \sqrt [3]{121}} \\ \Rightarrow \abs {g'(x)} \leq k = \frac {2}{9 \sqrt [3]{3}} < 1 \end {gather*} \par Therefore, all the conditions in Corollary 2.5 hold. Using Corollary 2.5, with \(p_0 = \num {2.5}\), the number of iteration \(n\) required to obtain approximations accurate to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq k^n \num {0.5} < 10^{-5} \iff n \geq 6\] \par Applying fixed-point method on \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 2.5 & 4 & 2.09476055 \\ 1 & 2.15443469 & 5 & 2.09458325 \\ 2 & 2.10361203 & 6 & 2.09455631 \\ 3 & 2.09592741 & 7 & 2.09455222 \\ \bottomrule \end {tabular} \end {table} \par So one solution of the problem is \(p \approx \num {2.094552}\). \par \item Let \(I = \interval {3}{4}\) and \par \begin {align*} g(x) &= \ln {3x^2} = 2 \ln {x} + \ln {3} \\ \Rightarrow g'(x) &= \frac {2}{x} \end {align*} \par A fixed point \(p\) of \(g\) is also a solution of the problem. \par Consider \(g\). It is clear that \(g\) is continuous and monotonically increasing on \(I\), so that: \par \begin {align*} 3 < g(3) = \ln {27} < &g(x) < g(4) = \ln {48} < 4 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par Consider \(g'\). Since \(I > 0\), \(g'(x)\) is monotonically decreasing in \(I\), so that: \par \begin {gather*} g'(3) = \frac {2}{3} \geq g'(x) \geq g'(4) = \frac {1}{2} \\ \Rightarrow \abs {g'(x)} \leq k = \frac {2}{3} < 1 \end {gather*} \par Therefore, all the conditions in Corollary 2.5 hold. Using Corollary 2.5, with \(p_0 = \num {3.5}\), the number of iteration \(n\) required to obtain approximations accurate to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq k^n \num {0.5} < 10^{-5} \iff n \geq 27\] \par Applying fixed-point method on \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 3.5 & 6 & 3.72717712 & 12 & 3.73293923 \\ 1 & 3.60413823 & 7 & 3.72991458 & 13 & 3.73300413 \\ 2 & 3.66277767 & 8 & 3.73138295 & 14 & 3.7330389 \\ 3 & 3.69505586 & 9 & 3.73217015 & 15 & 3.73305753 \\ 4 & 3.71260363 & 10 & 3.73259204 & 16 & 3.73306751 \\ 5 & 3.72207913 & 11 & 3.7328181 & & \\ \bottomrule \end {tabular} \end {table} \par So one solution of the problem is \(p \approx \num {3.733068}\). \par \item Let \(I = \interval {0}{1}\) and \par \begin {align*} g(x) &= \cos {x} \\ \Rightarrow g'(x) &= - \sin {x} \end {align*} \par A fixed point \(p\) of \(g\) is also a solution of the problem. \par Consider \(g\). It is clear that \(g\) is continuous and monotonically decreasing on \(I\), so that: \par \begin {align*} 1 = g(0) \geq &g(x) \geq g(1) = \cos {1} > 0 \\ \Rightarrow &g(x) \in I \, \forall x \in I \end {align*} \par Consider \(g'\). Since \(I > 0\), \(g'(x)\) is monotonically decreasing in \(I\), so that: \par \begin {gather*} g'(0) = 0 \geq g'(x) \geq g'(1) = - \sin {1} \\ \Rightarrow \abs {g'(x)} \leq k = \sin {1} < 1 \end {gather*} \par Therefore, all the conditions in Corollary 2.5 hold. Using Corollary 2.5, with \(p_0 = \num {0.5}\), the number of iteration \(n\) required to obtain approximations accurate to within \(10^{-5}\) is: \par \[\abs {p_n - p} \leq k^n \num {0.5} < 10^{-5} \iff n \geq 63\] \par Applying fixed-point method on \(g\) generates the following table: \par \begin {longtable}{r S[table-format=1.9] r S[table-format=1.9] r S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule \endhead 0 & 0.5 & 10 & 0.735006309 & 20 & 0.73900678 \\ 1 & 0.877582562 & 11 & 0.741826523 & 21 & 0.739137911 \\ 2 & 0.639012494 & 12 & 0.737235725 & 22 & 0.739049581 \\ 3 & 0.802685101 & 13 & 0.740329652 & 23 & 0.739109081 \\ 4 & 0.694778027 & 14 & 0.738246238 & 24 & 0.739069001 \\ 5 & 0.768195831 & 15 & 0.739649963 & 25 & 0.739096 \\ 6 & 0.719165446 & 16 & 0.738704539 & 26 & 0.739077813 \\ 7 & 0.752355759 & 17 & 0.739341452 & 27 & 0.739090064 \\ 8 & 0.730081063 & 18 & 0.738912449 & 28 & 0.739081812 \\ 9 & 0.745120341 & 19 & 0.739201444 & & \\ \bottomrule \end {longtable} \par So one root of the problem is \(p \approx \num {0.739082}\). \end {enumerate}}||exercise-33=={Consider \(f = 0\). Since \(x^2 \geq 0\), \(\cos {x}\) must be negative for the equation to hold, so that: \par \begin {equation*}\label {eq:exer:2.2.13:bound_x_by_cos} x \in I_k = \interval {\frac {\pi }{2} + k 2 \pi }{\frac {3 \pi }{2} + k 2 \pi } \, \forall k \in \mathbb {N} \tag {1} \end {equation*} \par Also, since \(10 \cos {x} \in \interval {-10}{0}\): \par \begin {equation*}\label {eq:exer:2.2.13:bound_x_by_x^2} x \in \interval {- \sqrt {10}}{\sqrt {10}} \tag {2} \end {equation*} \par Combining \eqref {eq:exer:2.2.13:bound_x_by_cos} and \eqref {eq:exer:2.2.13:bound_x_by_x^2} gives: \par \[x \in I = I_a \cup I_b \text { where } I_a = \interval {- \sqrt {10}}{- \frac {\pi }{2}} \text { and } I_b = \interval {\frac {\pi }{2}}{\sqrt {10}}\] \par As \(x^2\) and \(\cos {x}\) take \(Oy\) as a symmetry axis, each zero \(z_b\) of \(f\) in \(I_b\) results in another zero \(z_a = - z_b\) in \(I_a\). Hence, from now on, we just need to examine on \(I_b\). \par Differentiating \(f\) gives: \par \[f'(x) = 2x - 10 \sin {x}\] \par \(x\) is monotonically increasing on \(I_b\), \(\sin {x}\) is monotonically decreasing on \(I_b\). It follows that \(f'\) is monotonically increasing on \(I_b\), which means: \par \[f'(\frac {\pi }{2}) = \pi - 10 \leq f'(x) \leq f'(\sqrt {10}) = 2 \sqrt {10} - 10 \sin {\sqrt {10}}\] \par Combining with the fact that \(f'\) is continuous on \(I_b\), according to Intermediate Value Theorem, \(f'\) has one zero in \(I_b\). It follows that \(f\) has at most two zeros in \(I_b\). \par Let \par \[g(x) = x - \frac {-10 \cos {x}}{x^2} + 1 = x + \frac {10 \cos {x}}{x^2} + 1\] \par A fixed point of \(g\) is also a zero of \(f\). \emph {Try} applying fixed-point method on \(g\) with several \(p_0\), we found two fixed points: \par \begin {itemize} \item \(p_0 = \frac {\pi }{2}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 1.57079633 & 4 & 1.95354867 & 8 & 1.96859328 \\ 1 & 2.57079633 & 5 & 1.9749308 & 9 & 1.96897439 \\ 2 & 2.29757529 & 6 & 1.96675733 & 10 & 1.96883622 \\ 3 & 2.03884343 & 7 & 1.96964871 & 11 & 1.96888624 \\ \bottomrule \end {tabular} \end {table} \par \item \(p_0 = -\sqrt {10}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.8]} \toprule \(n\) & {\(p_n\)} \\ \midrule 0 & -3.16227766 \\ 1 & -3.16206373 \\ 2 & -3.16198949 \\ \bottomrule \end {tabular} \end {table} \end {itemize} \par The second fixed point is interesting. It is indeed a fixed point of \(g\), a zero of \(f\), but it belongs to \(I_a\). Due to the symmetry property, we conclude that \(f\) has 4 zeros: \(\pm \num {1.96889}\) and \(\pm \num {3.16199}\).}||exercise-34=={Let \par \[g(x) = x - \sqrt [3]{\frac {\tan {x}}{x}} + 1\] \par A fixed point \(p\) of \(g\) is also a solution of the problem. Applying fixed-point method on \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 4 & 4 & 4.49534411 & 8 & 4.49352955 \\ 1 & 4.33850407 & 5 & 4.49242947 & 9 & 4.49334961 \\ 2 & 4.50097594 & 6 & 4.49389301 & 10 & 4.49343923 \\ 3 & 4.48937873 & 7 & 4.4931677 & & \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {4.49344}\) is a solution of the problem in \(\interval {4}{5}\).}||exercise-35=={Consider \(f\): \par \begin {align*} &\quad & f(x) &= 0 \\ \iff && 2 \sin {\pi x} &= -x \\ \iff && \pi x &= \arcsin {\num {-0.5} x} + k 2 \pi \text { (\(k \in \mathbb {N}\))} \\ \iff && x &= \frac {\arcsin {\num {-0.5} x}}{\pi } + 2k \end {align*} \par Let \par \[g(x) = \frac {\arcsin \num {-0.5} x}{\pi } + 2\] \par \(\arcsin \) is chosen as it ``behaves'' nicer than normal \(\sin \). Since \(\arcsin \) returns values in principal branch \(\interval {-\frac {\pi }{2}}{\frac {\pi }{2}}\), we need to use \(k = 1\) to shift the value to cover \(\interval {1}{2}\). \par A fixed point \(p\) of \(g\) is also a solution of the problem. Applying fixed-point method on \(g\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 1 & 3 & 1.696498 \\ 1 & 1.83333333 & 4 & 1.67765706 \\ 2 & 1.63086925 & 5 & 1.68324099 \\ \bottomrule \end {tabular} \end {table} \par So \(p \approx \num {1.683}\) is a solution of the problem in \(\interval {1}{2}\). \par }||exercise-36=={\begin {enumerate}[label = \alph *)] \item If fixed-point iteration converges to a nonzero limit \(p\), then: \par \begin {align*} p &= \lim _{n \to \infty } p_n \\ &= \lim _{n \to \infty } g(p_{n - 1}) \\ &= \lim _{n \to \infty } \left (2p_{n - 1} - Ap_{n - 1}^2\right ) \\ &= 2p - Ap^2 \\ \iff p &= Ap^2 \iff p = \frac {1}{A} \end {align*} \par \item We try to find \(\delta > 0\) such that fixed-point method converges on \(I = \interval {\sfrac {1}{A} - \delta }{\sfrac {1}{A} + \delta }\) using Fixed Point Theorem. \par The condition that \(g\) is continuous on \(I\) is satisfied with any \(\delta \). \par Consider \(g\): \par \[g(x) = -Ax^2 + 2x = -A \left (x - \frac {1}{A}\right )^2 + \frac {1}{A}\] \par So \(x = \frac {1}{A}\) is the axis of symmetry for \(g\). \par Differentiating \(g\) gives: \par \[g'(x) = 2 - 2Ax\] \par It follows that: \par \begin {itemize} \item \(g'(x) < 0 \iff x > \frac {1}{A}\) \item \(g'(x) = 0 \iff x = \frac {1}{A}\) \item \(g'(x) > 0 \iff x < \frac {1}{A}\) \end {itemize} \par Combining with the fact that \(x = \frac {1}{A}\) is the symmetry axis of \(g\) gives: \par \begin {gather*} g \left (\frac {1}{A} + \delta \right ) = g \left (\frac {1}{A} - \delta \right ) = g \left (\frac {1}{A} \pm \delta \right ) \leq g(x) \leq g \left (\frac {1}{A}\right ) \, \forall x \in I \\ \iff \frac {2}{A} - A \delta ^2 \leq g(x) \leq \frac {1}{A} \end {gather*} \par Then, to satisfy the condition that \(g(x) \in I \, \forall x \in I\), \(\delta \) must satisfy the following: \par \begin {align*} &\quad & \frac {2}{A} - A \delta ^2 &\geq \frac {1}{A} - \delta \\ \iff && (A \delta )^2 - A \delta - 1 &\leq 0 \\ \iff && 0 < \delta &\leq \frac {1 + \sqrt {5}}{2A} \text { (as \(\delta > 0\))} \tag {1}\label {eq:exer:2.2.17:1} \end {align*} \par Consider \(g'\). \(g'\) is monotonically decreasing on \(\mathbb {R}\), so: \par \begin {gather*} g' \left (\frac {1}{A} - \delta \right ) = 2 A \delta \geq g'(x) \geq g' \left (\frac {1}{A} - \delta \right ) = -2 A \delta \\ \iff \abs {g'(x)} \leq 2 A \delta \text { (equal sign only at either end)} \tag {2}\label {eq:exer:2.2.17:2} \end {gather*} \par Then, to satisfy the condition that \(\abs {g'(x)} < 1 \, \forall x \in I_{open} = \interval [open]{\sfrac {1}{A} - \delta }{\sfrac {1}{A} + \delta }\), \(\delta \) must satisfy the following: \par \[2 A \delta \leq 1 \iff \delta \leq \frac {1}{2A}\] \par From \eqref {eq:exer:2.2.17:1} and \eqref {eq:exer:2.2.17:2}: \par \[0 < \delta < \frac {1}{2A}\] \par As all the conditions needed for Fixed Point Theorem hold, we conclude that for any \(\delta \in \interval [open left]{0}{\frac {1}{2A}}\), applying fixed-point method on \(g\) with \(p_0 \in I\) converges to the fixed point. \end {enumerate}}||exercise-37=={Let \(I = \interval {0}{1}\), \(g = \dfrac {1}{x + \num {0.5}}\). \par Consider \(g\). \(g\) is defined on \(\mathbb {R} \setminus \{\num {-0.5}\}\), so it is defined on \(I\). \par \(g(x) > 1 \, \forall x \in \interval {\num {-0.5}}{\num {0.5}}\), so the condition that \(g(x) \in I \, \forall x \in I\) does not hold. \par Differentiating \(g\) gives: \par \[g'(x) = - \dfrac {1}{(x + \num {0.5})^2} < -1 \iff x \in \interval [open]{\num {-1.5}}{\num {0.5}} \setminus \{\num {-0.5}\}\] \par So the condition that \(\abs {g'(x)} < 1 \, \forall x \in I\) does not hold. \par Yet, \(g\) has a fixed point at \(x = \dfrac {\sqrt {17} - 1}{4}\).}||exercise-38=={\begin {enumerate}[label = \alph *)] \item Where the fuck is Theorem 2.2 in the fucking book? \item In the proof of Theorem 2.3, if \(\abs {g'(x) \leq k}\) is replaced with \(g'(x) \leq k\), then there is a chance that \(g'(\xi ) = -1\). In that case, the assumption is no longer a contradiction, therefore the proof is invalid, and the theorem doesn't hold. \end {enumerate}}||exercise-39=={\begin {enumerate}[label = \alph *)] \item Let \(g\) be the function that generates the sequence \(\{x_n\}\): \par \begin {align*} g(x) &= \frac {x}{2} + \frac {1}{x} = \frac {x^2 + 2}{2x} \\ \Rightarrow g'(x) &= \frac {1}{2} - \frac {1}{x^2} = \frac {x^2 - 2}{2x^2} \end {align*} \par Consider \(I = \interval {\sqrt {2}}{b}\), for any \(b > \sqrt {2}\). It is clear that \(g\) and \(g'\) exists on \(I\). Since \(g'(x) \leq 0 \, \forall x \in I\), \(g\) is monotonically increasing on \(I\). \par Consider \(g'\). \(x^2\) is strictly increasing on \(I\), so \(g'\) is strictly decreasing on \(I\), therefore: \par \begin {gather*} \frac {1}{2} > g'(x) \leq g'(\sqrt {2}) = 0 \, \forall x \in I \\ \Rightarrow \abs {g'(x)} < 1 \, \forall x \in I \end {gather*} \par Let \par \[f(x) = g(x) - x = \frac {1}{x} - \frac {x}{2}\] \par \(\sfrac {1}{x}\) is strictly decreasing on \(I\), and so is \(-x\). Therefore, \(f\) is strictly decreasing on \(I\), so: \par \[f(\sqrt {2}) = 0 \leq f(x) \, \forall x \in I\] \par In other words, \(g(x) \leq x \, \forall x \in I\). It means that for any \(b\), \(g(b) < b\). Combining with the fact that \(g(\sqrt {2}) = \sqrt {2}\), it is guaranteed that: \par \[g(x) \in I \, \forall x \in I\] \par All the conditions of Theorem 2.4 hold, so we can apply it here: for any \(x_0 \in I\), applying fixed-point method on \(g\) converges to the unique fixed point in \(I\), using any \(x_0 \in I\). \par Trivially, \(\sqrt {2}\) is a fixed point of \(g\), therefore it must be the unique fixed point on \(I\). \par We can conclude that for any \(x_0 > \sqrt {2}\), the sequence converges to \(\sqrt {2}\). \par \item When \(0 < x < \sqrt {2}\), \(g'(x) < 0\), which means \(g\) is monotonically decreasing. Applying this on \(0 < x_0 < \sqrt {2}\) gives: \par \[x_1 = g(x_0) > g(\sqrt {2}) = \sqrt {2}\] \par \item We have: \par \begin {itemize} \item If \(x_0 > \sqrt {2}\): proven. \item If \(x_0 = \sqrt {2}\): it is exactly the fixed point. \item If \(0 < x_0 < \sqrt {2}\): \(x_1 = g(x_0) > \sqrt {2}\), then from \(x_1\) onwards, the sequence converges to \(\sqrt {2}\), as proven with the case \(x_0 > \sqrt {2}\). \end {itemize} \par Therefore, we can conclude that the sequence converges to \(\sqrt {2}\) whenever \(x_0 > 0\). \end {enumerate}}||exercise-40=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} g(x) &= \frac {x}{2} + \frac {A}{2x} = \frac {x^2 + A}{2x}\\ \Rightarrow g'(x) &= \frac {1}{2} - \frac {A}{2x^2} = \frac {x^2 - A}{2x^2} \end {align*} \par Trivially, we can find out that \(\sqrt {A}\) is a fixed point of \(g\). \par Let \par \begin {align*} f(x) &= g(x) - x = \frac {A}{2x} - \frac {x}{2} = \frac {A - x^2}{2x} \\ \Rightarrow f'(x) &= - \frac {A}{2x^2} - \frac {1}{2} = - \frac {x^2 + A}{2x^2} \end {align*} \par Since \(f'(x) < 0 \, \forall x \neq 0\), \(f(x)\) is monotonically increasing when \(x > 0\). \par Consider the sign of \(g'\): \par \begin {itemize} \item \(g'(x) < 0 \iff \abs {x} < \sqrt {A}\) \item \(g'(x) = 0 \iff \abs {x} = \sqrt {A}\) \item \(g'(x) > 0 \iff \abs {x} > \sqrt {A}\) \end {itemize} \par If \(x > \sqrt {A}\), then: \par \begin {itemize} \item \(g' > 0\), which means \(g\) is monotonically increasing. It follows that: \par \[g(x) > g(\sqrt {A}) = \sqrt {A}\] \par \item \(f(x) < f(\sqrt {A}) = 0\), which means \(g(x) < x\), making \(\{x_n\}\) a decreasing sequence. \end {itemize} \par From both of the above, we know that \(\{x_n\}\) is a lower-bounded decreasing sequence, and therefore must converge: \par \begin {align*} x &= \lim _{n \to \infty } x_n \\ &= \lim _{n \to \infty } g(x_{n - 1}) \\ &= \lim _{n \to \infty } \frac {x_{n - 1}}{2} + \frac {A}{2 x_{n - 1}} \\ &= \frac {x}{2} + \frac {A}{2x} \\ \iff x &= \sqrt {A} \end {align*} \par So, for all \(x_0 > \sqrt {A}\), the sequence converges to \(\sqrt {A}\). \par If \(x = \sqrt {A}\), then \(g(x) = x = \sqrt {A}\). Hence \(x_n = \sqrt {A} \, \forall n \geq 0\). So, for \(x_0 = \sqrt {A}\), the sequence converges to \(\sqrt {A}\). \par If \(0 < x < \sqrt {A}\), then \(g' < 0\), which means \(g\) is monotonically decreasing. It follows that: \par \[g(x) > g(\sqrt {A}) = \sqrt {A}\] \par So, for \(0 < x_0 < \sqrt {A}\), \(x_1 = g(x_0) > \sqrt {A}\), then from \(x_1\) onwards, the sequence converges to \(\sqrt {A}\), as proven with the case \(x_0 > \sqrt {A}\). \par We can conclude that the sequence \(\{x_n\}\) converges to \(\sqrt {2}\) whenever \(x_0 > 0\). \par \item If \(x_0 < 0\), then similar to the above proof, we conclude that the sequence converges to \(-\sqrt {A}\). \end {enumerate}}||exercise-41=={\(g\) satisfies a Lipschitz condition on the interval \(\interval {a}{b}\) with Lipschitz constant \(L < 1\) means that: \par \[\frac {g(x_1) - g(x_2)}{x_1 - x_2} \leq L \, \forall x_1, x_2 \in \interval {a}{b} \tag {*}\label {eq:exer:2.2.21}\] \par In the proof of Theorem 2.4, we see that: \par \[\abs {p - p_n} = \abs {g(p) - g(p_{n - 1})}\] \par From the previous section of the proof, we already proved that \(p\) and \(p_{n - 1}\) is in \(\interval {a}{b}\). Applying \eqref {eq:exer:2.2.21} with \(x_1 = p\), \(x_2 = p_{n - 1}\) gives: \par \[\abs {p - p_n} = \abs {g(p) - g(p_{n - 1})} \leq L \abs {p - p_{n - 1}}\] \par Then the proof proceeds normally, replacing \(k\) with \(L\).}||exercise-42=={\label {exer:2.2.22} Since \(p\) is a fixed point in \(\interval [open]{c}{d}\) of \(g\), \(g(p) = p\). \par Since \(g'\) is continuous at \(p\), according to the definition of continuity and limit, for every \(\varepsilon > 0\), there exist \(\delta > 0\) such that: \par \begin {gather*} \abs {g'(x) - g'(p)} < \varepsilon \, \forall x \in D = \interval {p - \delta }{p + \delta } \\ \iff g'(x) \in E = \interval {g'(p) - \varepsilon }{g'(p) + \varepsilon } \, \forall x \in D \end {gather*} \par We can always choose a \(\varepsilon \) such that \(E \subset \interval [open]{-1}{1}\). Then the proof proceeds normally, replacing \(\interval {a}{b}\) with \(E\).}||exercise-43=={Replacing symbols in \(s(t)\) with number gives: \par \[s(t) = \num {501.0625} - \num {80.425} t - \num {201.0625} e^{\num {-0.4} t}\] \par Let \par \[g(t) = \frac {1}{80.425} (501.0625 - \num {201.0625} e^{\num {-0.4} t})\] \par A fixed point \(p\) of \(g\) is also a root of \(s(t) = 0\), which is the time it takes the quarter-pounder to hit the ground. \par Applying fixed-point method on \(g\) with \(p_0 = 3\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] r S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & \(n\) & {\(p_n\)} \\ \midrule 0 & 3 & 3 & 5.99886594 \\ 1 & 5.47719787 & 4 & 6.00328561 \\ 2 & 5.9506374 & & \\ \bottomrule \end {tabular} \end {table} \par We conclude that it takes approximately \SI {6.003}{\second } for the quarter-pounder to hit the ground.}||exercise-44=={This problem is similar to \hyperref [exer:2.2.22]{Exercise 22}. \par Since \(g'\) is continuous at \(p\), according to the definition of continuity and limit, for every \(\varepsilon > 0\), there exist \(\delta > 0\) such that: \par \begin {gather*} \abs {g'(x) - g'(p)} < \varepsilon \, \forall x \in D = \interval {p - \delta }{p + \delta } \\ \iff g'(x) \in E = \interval {g'(p) - \varepsilon }{g'(p) + \varepsilon } \, \forall x \in D \end {gather*} \par We can always choose a \(\varepsilon \) such that \(E \subset \interval [open]{1}{\infty }\). \par If \(p_0 \in D\), then according to Mean Value Theorem, there exist a \(\xi \in D\) such that: \par \[\abs {p_1 - p} = \abs {g(p_0) - g(p)} = \abs {g'(\xi )} \abs {p_0 - p} > \abs {p_0 - p}\]}||exercise-45=={\(f'(x) = 2x\). Therefore, \(p_1 = \num {3.5}\), \(p_2 = \num {2.607142}\).}||exercise-46=={\(f'(x) = -3x^2 + \sin {x}\). Therefore, \(p_1 = \num {-0.880333}\), \(p_2 = \num {-0.865684}\). \par \(p_0 = 0\) can't be used, as \(f'(p_0) = 0\), therefore \(p_1\) can't be calculated.}||exercise-47=={\begin {enumerate}[label = \alph *)] \item Applying Secant method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3 & 3 \\ 1 & 2 & -2 \\ 2 & 2.4 & -0.24 \\ 3 & 2.454545 & 0.024793 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {2.454545}\). \par \item Applying False Position method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.6] S[table-format=-1.6]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3 & 3 \\ 1 & 2 & -2 \\ 2 & 2.4 & -0.24 \\ 3 & 2.454545 & 2.444444 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {2.444444}\). \par \item \(p_3\) produced by Secant method is better. \end {enumerate}}||exercise-48=={\begin {enumerate}[label = \alph *)] \item Applying Secant method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & -1 & 0.459697694 \\ 1 & 0 & -1 \\ 2 & -0.685073357 & -0.452850234 \\ 3 & -1.252076489 & 1.649523592 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {-1.252076}\). \par \item Applying False Position method generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & -1 & 0.459697694 \\ 1 & 0 & -1 \\ 2 & -0.685073357 & -0.452850234 \\ 3 & -0.841355126 & -0.070875968 \\ \bottomrule \end {tabular} \end {table} \par So \(p_3 = \num {-0.841355}\). \par \end {enumerate}}||exercise-49=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} f(x) &= x^3 - 2x^2 - 5 \\ \Rightarrow f'(x) &= 3x^2 - 4x \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {2.5}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 2.5 & -1.875 & 8.75 \\ 1 & 2.714285714 & 0.262390671 & 11.24489796 \\ 2 & 2.690951571 & 0.003331987 & 10.95985413 \\ 3 & 2.690647499 & 0.000000561 & 10.9561619 \\ 4 & 2.690647448 & 0 & 10.95616128 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.69065}\) is a solution of the problem. \par \item Let \par \begin {align*} f(x) &= x^3 + 3x^2 - 1 \\ \Rightarrow f'(x) &= 3x^2 + 6x \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {-2.5}\) gives: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-1.9] S[table-format=2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -2.5 & 2.125 & 3.75 \\ 1 & -3.06666667 & -1.626962963 & 9.81333333 \\ 2 & -2.900875604 & -0.165860349 & 7.839984184 \\ 3 & -2.879719904 & -0.002542819 & 7.600040757 \\ 4 & -2.879385325 & -0.000000631 & 7.596267596 \\ 5 & -2.879385242 & 0 & 7.596266659 \\ \bottomrule \end {longtable} \par We conclude that \(p \approx \num {2.69065}\) is a solution of the problem. \par \item Let \par \begin {align*} f(x) &= x - \cos {x} \\ \Rightarrow f'(x) &= 1 + \sin {x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.739}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.739 & -0.000142477 & 1.673549106 \\ 1 & 0.739085135 & 0.000000002 & 1.67361203 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.73909}\) is a solution of the problem. \par \item Let \par \begin {align*} f(x) &= x - \num {0.8} - \num {0.2} \sin {x} \\ \Rightarrow f'(x) &= 1 - \num {0.2} \cos {x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.964}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.964 & -0.000295817 & 0.885952272 \\ 1 & 0.964333898 & -0.000000009 & 0.886007136 \\ 2 & 0.964333888 & 0 & 0.886007135 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.96433}\) is a solution of the problem. \end {enumerate}}||exercise-50=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} f(x) &= e^x + 2^{-x} + 2 \cos {x} - 6 \\ \Rightarrow f'(x) &= e^x - \ln {2} \cdot 2^{-x} - 2 \sin {x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {1.829}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 1.829 & -0.001572837 & 4.098862489 \\ 1 & 1.829383725 & 0.000000506 & 4.101500646 \\ 2 & 1.829383602 & 0 & 4.101499798 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.829384}\) is a solution of the problem. \par \item Let \par \begin {align*} f(x) &= \ln (x - 1) + \cos (x - 1) \\ \Rightarrow f'(x) &= \frac {1}{x - 1} - \sin (x - 1) \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {1.398}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 1.398 & 0.000534714 & 1.527454989 \\ 1 & 1.397649931 & -0.00020962 & 1.52972716 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.39765}\) is a solution of the problem. \par \item Let \par \begin {align*} f(x) &= 2 x \cos (2x) - (x - 2)^2 \\ \Rightarrow f'(x) &= 2(\cos {x} - x \sin (2x) * 2) - 2(x - 2) \\ &= 2(\cos {x} - 2 x \sin (2x) - x + 2) \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {2.371}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 2.371 & 0.002753936 & 7.30284651 \\ 1 & 2.3706229 & -0.000563086 & 7.30282746 \\ 2 & 2.3707 & 0.000115071 & 7.30283178 \\ 3 & 2.37068424 & -0.000023518 & 7.30283091 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.722}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=1.9] S[table-format=-2.8]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 3.722 & 0.001838451 & -18.77068249 \\ 1 & 3.722097943 & 0.000241783 & -18.77229246 \\ 2 & 3.722110823 & 0.000031801 & -18.77250414 \\ 3 & 3.722112517 & 0.000004182 & -18.77253198 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.370684}\) and \(p \approx \num {3.722113}\) are solutions of the problem. \par \item Let \par \begin {align*} f(x) &= (x - 2)^2 - \ln {x} \\ \Rightarrow f'(x) &= 2(x - 2) - \frac {1}{x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {1.412}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 1.412 & 0.00073686 & -1.884215297 \\ 1 & 1.41239107 & 0.000000191 & -1.883237062 \\ 2 & 1.412391172 & 0 & -1.883236808 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.057}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 3.057 & -0.000185043 & 1.78688191 \\ 1 & 3.05710356 & 0.000000011 & 1.7871001 \\ 2 & 3.05710355 & 0 & 1.78710009 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.412391}\) and \(p \approx \num {3.057104}\) are solutions of the problem. \par \item Let \par \begin {align*} f(x) &= e^x - 3x^2 \\ \Rightarrow f'(x) &= e^x - 6x \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.91}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.91 & 0.000022533 & -2.97567747 \\ 1 & 0.910007573 & 0 & -2.97570409 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.733}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 3.733 & -0.001533768 & 19.4063332 \\ 1 & 3.73307903 & 0.000000112 & 19.4091631 \\ 2 & 3.73307903 & 0 & 19.4091629 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.910008}\) and \(p \approx \num {3.733079}\) are solutions of the problem. \par \item Let \par \begin {align*} f(x) &= \sin {x} - e^{-x} \\ \Rightarrow f'(x) &= \cos {x} + e^{-x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.588}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9] S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.588 & -0.000739019 & 1.38748879 \\ 1 & 0.58853263 & -0.000000157 & 1.38689746 \\ 2 & 0.588532744 & 0 & 1.38689733 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.096}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 3.096 & 0.0003471 & -0.953731075 \\ 1 & 3.09636394 & -0.000000601 & -0.953764054 \\ 2 & 3.09636393 & 0 & -0.953764053 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {6.285}\) gives: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9] S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 6.285 & -0.000049365 & 1.00186241 \\ 1 & 6.28504927 & 0 & 1.00186223 \\ 2 & 6.28504927 & 0 & 1.00186223 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.58853}\), \(p \approx \num {3.09636}\) and \(p = 6.285049\) are solutions of the problem. \end {enumerate}}||exercise-51=={\begin {enumerate}[label= \alph *)] \item Applying Secant method with \(p_0 = \num {2.6}\) and \(p_1 = \num {2.7}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 2.6 & -0.944 \\ 1 & 2.7 & 0.103 \\ 2 & 2.690162369 & -0.005313179 \\ 3 & 2.690644942 & -0.000027451 \\ 4 & 2.690647449 & 0.000000007 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.69065}\) is a solution of the problem. \par \item Applying Secant method with \(p_0 = \num {-2.8}\) and \(p_1 = \num {-2.9}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & -2.8 & 0.568 \\ 1 & -2.9 & -0.159 \\ 2 & -2.878129298 & 0.009531586 \\ 3 & -2.879366233 & 0.000144394 \\ 4 & -2.879385259 & -0.000000134 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {-2.87939}\) is a solution of the problem. \par \item Applying Secant method with \(p_0 = \num {0.73}\) and \(p_1 = \num {0.74}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.73 & -0.015174402 \\ 1 & 0.74 & 0.001531441 \\ 2 & 0.73908329 & -0.000003084 \\ 3 & 0.739085133 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.73909}\) is a solution of the problem. \par \item Applying Secant method with \(p_0 = \num {0.96}\) and \(p_1 = \num {0.97}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.96 & -0.003838313 \\ 1 & 0.97 & -0.005022857 \\ 2 & 0.96433161 & -0.000002018 \\ 3 & 0.964333887 & -0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.96433}\) is a solution of the problem. \end {enumerate}}||exercise-52=={\begin {enumerate}[label = \alph *)] \item Applying Secant method with \(p_0 = \num {1.82}\) and \(p_1 = \num {1.83}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.82 & -0.038185199 \\ 1 & 1.83 & 0.002529463 \\ 2 & 1.829378734 & -0.000019965 \\ 3 & 1.829383599 & 0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.829384}\) is a solution of the problem. \par \item Applying Secant method with \(p_0 = \num {1.39}\) and \(p_1 = \num {1.4}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.39 & -0.01669948 \\ 1 & 1.4 & 0.004770262 \\ 2 & 1.397778147 & 0.0000631 \\ 3 & 1.397748362 & -0.000000242 \\ 4 & 1.397748476 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.397748}\) is a solution of the problem. \par \item Applying Secant method with \(p_0 = \num {2.37}\) and \(p_1 = \num {2.375}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 2.37 & -0.006040395 \\ 1 & 2.375 & 0.037985226 \\ 2 & 2.370686009 & -0.00000799 \\ 3 & 2.370686916 & -0.000000001 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {3.72}\) and \(p_1 = \num {3.73}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.72 & 0.034398018 \\ 1 & 3.73 & -0.129244414 \\ 2 & 3.722102023 & 0.000175259 \\ 3 & 3.722112719 & 0.000000889 \\ 4 & 3.722112773 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.37069}\) and \(p \approx \num {3.722113}\) are solutions of the problem. \par \item Applying Secant method with \(p_0 = \num {1.41}\) and \(p_1 = \num {1.42}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.41 & 0.004510296 \\ 1 & 1.42 & -0.014256872 \\ 2 & 1.41240329 & -0.000022822 \\ 3 & 1.41239111 & 0.000000116 \\ 4 & 1.41239117 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {3.05}\) and \(p_1 = \num {3.06}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.05 & -0.012641591 \\ 1 & 3.06 & 0.005185084 \\ 2 & 3.05709139 & -0.000021731 \\ 3 & 3.05710353 & -0.000000037 \\ 4 & 3.05710355 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.412391}\) and \(p \approx \num {3.057104}\) are solutions of the problem. \par \item Applying Secant method with \(p_0 = \num {0.91}\) and \(p_1 = \num {0.92}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.91 & 0.000022533 \\ 1 & 0.92 & -0.02990961 \\ 2 & 0.910007528 & 0.000000132 \\ 3 & 0.910007572 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {3.73}\) and \(p_1 = \num {3.74}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.73 & -0.059591836 \\ 1 & 3.74 & 0.135190165 \\ 2 & 3.73305941 & -0.000380739 \\ 3 & 3.7330789 & -0.000002422 \\ 4 & 3.73307903 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.910008}\) and \(p \approx \num {3.733079}\) are solutions of the problem. \par \item Applying Secant method with \(p_0 = \num {0.58}\) and \(p_1 = \num {0.59}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.58 & -0.01187443 \\ 1 & 0.59 & 0.002033738 \\ 2 & 0.588537738 & 0.000006927 \\ 3 & 0.588532741 & -0.000000004 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {3.09}\) and \(p_1 = \num {3.1}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.09 & 0.006067814 \\ 1 & 3.1 & -0.00346854 \\ 2 & 3.09636282 & 0.000001057 \\ 3 & 3.09636393 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {6.28}\) and \(p_1 = \num {6.29}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 6.28 & -0.005058702 \\ 1 & 6.29 & 0.00495988 \\ 2 & 6.28504932 & 0.000000046 \\ 3 & 6.28504927 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.588533}\), \(p \approx \num {3.096364}\) and \(p \approx \num {6.285049}\) are solutions of the problem. \end {enumerate}}||exercise-53=={\begin {enumerate}[label= \alph *)] \item Applying False Position method with \(p_0 = \num {2.6}\) and \(p_1 = \num {2.7}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 2.6 & -0.944 \\ 1 & 2.7 & 0.103 \\ 2 & 2.690162369 & -0.005313179 \\ 3 & 2.690644942 & -0.000027451 \\ 4 & 2.690647435 & -0.000000141 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.690647}\) is a solution of the problem. \par \item Applying False Position method with \(p_0 = \num {-2.8}\) and \(p_1 = \num {-2.9}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & -2.8 & 0.568 \\ 1 & -2.9 & -0.159 \\ 2 & -2.878129298 & 0.009531586 \\ 3 & -2.879366233 & 0.000144394 \\ 4 & -2.87938526 & -0.000000135 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {-2.87939}\) is a solution of the problem. \par \item Applying False Position method with \(p_0 = \num {0.73}\) and \(p_1 = \num {0.74}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.73 & -0.015174402 \\ 1 & 0.74 & 0.001531441 \\ 2 & 0.73908329 & -0.000003084 \\ 3 & 0.739085133 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.73909}\) is a solution of the problem. \par \item Applying False Position method with \(p_0 = \num {0.96}\) and \(p_1 = \num {0.97}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.96 & -0.003838313 \\ 1 & 0.97 & -0.005022857 \\ 2 & 0.96433161 & -0.000002018 \\ 3 & 0.964333887 & -0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.96433}\) is a solution of the problem. \end {enumerate}}||exercise-54=={\begin {enumerate}[label = \alph *)] \item Applying False Position method with \(p_0 = \num {1.82}\) and \(p_1 = \num {1.83}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.82 & -0.038185199 \\ 1 & 1.83 & 0.002529463 \\ 2 & 1.829378734 & -0.000019965 \\ 3 & 1.829383599 & 0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.829384}\) is a solution of the problem. \par \item Applying False Position method with \(p_0 = \num {1.39}\) and \(p_1 = \num {1.4}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.39 & -0.01669948 \\ 1 & 1.4 & 0.004770262 \\ 2 & 1.39777815 & 0.0000631 \\ 3 & 1.39774887 & 0.000000831 \\ 4 & 1.39774848 & 0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.397748}\) is a solution of the problem. \par \item Applying False Position method with \(p_0 = \num {2.37}\) and \(p_1 = \num {2.375}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 2.37 & -0.006040395 \\ 1 & 2.375 & 0.037985226 \\ 2 & 2.370686009 & -0.00000799 \\ 3 & 2.370686916 & -0.000000001 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {3.72}\) and \(p_1 = \num {3.73}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.72 & 0.034398018 \\ 1 & 3.73 & -0.129244414 \\ 2 & 3.722102023 & 0.000175259 \\ 3 & 3.722112719 & 0.000000889 \\ 4 & 3.72211277 & 0.000000001 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {2.37069}\) and \(p \approx \num {3.722113}\) are solutions of the problem. \par \item Applying False Position method with \(p_0 = \num {1.41}\) and \(p_1 = \num {1.42}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.41 & 0.004510296 \\ 1 & 1.42 & -0.014256872 \\ 2 & 1.41240329 & -0.000022822 \\ 3 & 1.41239119 & -0.000000036 \\ 4 & 1.41239117 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {3.05}\) and \(p_1 = \num {3.06}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.05 & -0.012641591 \\ 1 & 3.06 & 0.005185084 \\ 2 & 3.05709139 & -0.000021731 \\ 3 & 3.05710353 & -0.000000037 \\ 4 & 3.05710355 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {1.412391}\) and \(p \approx \num {3.057104}\) are solutions of the problem. \par \item Applying False Position method with \(p_0 = \num {0.91}\) and \(p_1 = \num {0.92}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.91 & 0.000022533 \\ 1 & 0.92 & -0.02990961 \\ 2 & 0.910007528 & 0.000000132 \\ 3 & 0.910007572 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {3.73}\) and \(p_1 = \num {3.74}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.73 & -0.059591836 \\ 1 & 3.74 & 0.135190165 \\ 2 & 3.73305941 & -0.000380739 \\ 3 & 3.7330789 & -0.000002422 \\ 4 & 3.73307903 & -0.000000015 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.910008}\) and \(p \approx \num {3.733079}\) are solutions of the problem. \par \item Applying False Position method with \(p_0 = \num {0.58}\) and \(p_1 = \num {0.59}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 0.58 & -0.01187443 \\ 1 & 0.59 & 0.002033738 \\ 2 & 0.588537738 & 0.000006927 \\ 3 & 0.588532761 & 0.000000024 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {3.09}\) and \(p_1 = \num {3.1}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.09 & 0.006067814 \\ 1 & 3.1 & -0.00346854 \\ 2 & 3.09636282 & 0.000001057 \\ 3 & 3.09636393 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {6.28}\) and \(p_1 = \num {6.29}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 6.28 & -0.005058702 \\ 1 & 6.29 & 0.00495988 \\ 2 & 6.28504932 & 0.000000046 \\ 3 & 6.28504927 & 0 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \approx \num {0.588533}\), \(p \approx \num {3.096364}\) and \(p \approx \num {6.285049}\) are solutions of the problem. \end {enumerate}}||exercise-55=={\begin {enumerate}[label = \alph *)] \item Such math... much difficult... \item Let \par \begin {align*} f(x) &= 2x + 3 \cos {x} - e^x \\ \Rightarrow f'(x) &= 2 - 3 \sin {x} - e^x \end {align*} \par \(\sin {x}\) and \(e^x\) are both monotonically increasing in \(I = \interval {0}{1}\), therefore \(f'(x)\) is monotonically decreasing \(I\). It follows that \par \[f'(0) = 2 \geq f'(x) \geq f'(1) \approx \num {-0.5244129544}\] \par \noindent and that \(f'(x)\) has exactly one zero \(p\) in \(I\). Since the sign of \(f'(x)\) changes from positive to negative as \(x\) passes \(p\), the local maximum of \(f\) in \(I\) is at \(p\). Then the minimum value of \(f\) in \(I\) is achieved at either end: \par \[f(x) \geq \min \{f(0), f(1)\} \approx \num {0.9026250891} > 0\] \par Then \(f\) has no zero in \(I\). \end {enumerate}}||exercise-56=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} f(x) &= x^2 - 4x + 4 - \ln {x} \\ \Rightarrow f'(x) &= 2x - 4 - \frac {1}{x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {1.41}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 1.41 & 0.00451029561 & -1.88921985816 \\ 1 & 1.41238738524 & 0.00000713142 & -1.88324627986 \\ 2 & 1.41239117201 & 0.00000000002 & -1.88323680804 \\ 3 & 1.41239117202 & 0 & -1.88323680802 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.05}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11] S[table-format=1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 3.05 & -0.01264159062 & 1.77213114754 \\ 1 & 3.05713355252 & 0.00005361847 & 1.78716330575 \\ 2 & 3.05710355053 & 0.00000000095 & 1.7871000916 \\ 3 & 3.05710354999 & 0 & 1.78710009048 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {1.41}\) and \(p_1 = \num {1.42}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.41 & 0.00451029561 \\ 1 & 1.42 & -0.01425687161 \\ 2 & 1.41240329057 & -0.00002282192 \\ 3 & 1.41239111052 & 0.00000011582 \\ 4 & 1.41239117202 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {3.05}\) and \(p_1 = \num {3.06}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.05 & -0.01264159062 \\ 1 & 3.06 & 0.00518508404 \\ 2 & 3.05709139021 & -0.00002173059 \\ 3 & 3.05710352927 & -0.00000003704 \\ 4 & 3.05710354999 & 0 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {1.41}\) and \(p_1 = \num {1.42}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 1.41 & 0.00451029561 \\ 1 & 1.42 & -0.01425687161 \\ 2 & 1.41240329057 & -0.00002282192 \\ 3 & 1.41239119124 & -0.00000003619 \\ 4 & 1.41239117205 & -0.00000000006 \\ \bottomrule \end {tabular} \end {table} \par Applying False Position method with \(p_0 = \num {3.05}\) and \(p_1 = \num {3.06}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule 0 & 3.05 & -0.01264159062 \\ 1 & 3.06 & 0.00518508404 \\ 2 & 3.05709139021 & -0.00002173059 \\ 3 & 3.05710352927 & -0.00000003704 \\ 4 & 3.05710354996 & 0 \\ \bottomrule \end {tabular} \end {table} \par \item Let \par \begin {align*} f(x) &= x + 1 - 2 \sin {\pi x} \\ \Rightarrow f'(x) &= 1 - 2 \pi \cos {\pi x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.21}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.21 & -0.01581410731 & -3.96469036415 \\ 1 & 0.20601126296 & 0.0000957226 & -4.01255625306 \\ 2 & 0.20603511873 & 0.00000000339 & -4.01227230982 \\ 3 & 0.20603511957 & 0 & -4.01227229977 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.68}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.11] S[table-format=-1.11] S[table-format=1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.68 & -0.008655851 & 4.36669904541 \\ 1 & 0.68198224126 & 0.00003270017 & 4.39967030778 \\ 2 & 0.68197480884 & 0.00000000046 & 4.39954692747 \\ 3 & 0.68197480874 & 0 & 4.39954692574 \\ \bottomrule \end {tabular} \end {table} \par Applying Secant method with \(p_0 = \num {0.21}\) and \(p_1 = \num {0.22}\) generates the following table: \par \begin {longtable}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0.21 & -0.01581410731 \\ 1 & 0.22 & -0.0548479795 \\ 2 & 0.20594861939 & 0.00034710682 \\ 3 & 0.20603698468 & -0.0000074833 \\ 4 & 0.20603511981 & -0.00000000096 \\ 5 & 0.20603511957 & 0 \\ \bottomrule \end {longtable} \par Applying Secant method with \(p_0 = \num {0.68}\) and \(p_1 = \num {0.69}\) generates the following table: \par \begin {longtable}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0.68 & -0.008655851 \\ 1 & 0.69 & 0.03583885145 \\ 2 & 0.68194536665 & -0.00012952468 \\ 3 & 0.68197437195 & -0.00000192166 \\ 4 & 0.68197480876 & 0.00000000107 \\ 5 & 0.68197480874 & 0 \\ \bottomrule \end {longtable} \par Applying False Position method with \(p_0 = \num {0.21}\) and \(p_1 = \num {0.22}\) generates the following table: \par \begin {longtable}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0.21 & -0.01581410731 \\ 1 & 0.22 & -0.0548479795 \\ 2 & 0.20594861939 & 0.00034710682 \\ 3 & 0.20603698468 & -0.0000074833 \\ 4 & 0.20603511981 & -0.00000000096 \\ 5 & 0.20603511957 & 0 \\ \bottomrule \end {longtable} \par Applying False Position method with \(p_0 = \num {0.68}\) and \(p_1 = \num {0.69}\) generates the following table: \par \begin {longtable}{r S[table-format=1.11] S[table-format=-1.11]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0.68 & -0.008655851 \\ 1 & 0.69 & 0.03583885145 \\ 2 & 0.68194536665 & -0.00012952467 \\ 3 & 0.68197437195 & -0.00000192166 \\ 4 & 0.68197480226 & -0.00000002851 \\ 5 & 0.68197480864 & -0.00000000042 \\ \bottomrule \end {longtable} \end {enumerate}}||exercise-57=={Let \(d\) be the squared distance between the point \((x, x^2)\) of the graph and \((1, 0)\). \par \begin {align*} d(x) &= (x - 1)^2 + x^4 \\ \Rightarrow d'(x) &= 4x^3 + 2(x - 1) \\ \Rightarrow d''(x) &= 12x^2 + 2 \end {align*} \par We need to find \(x\) that minimizes \(d\). First we have to examine \(d'\). As \(d''(x) \geq 2 > 0 \, \forall x \in \mathbb {R}\), \(d'\) is monotonically increasing in \(\mathbb {R}\). It follows that \(d'\) has at most one zero in \(\mathbb {R}\). \par Applying Newton's method on \(d'\) with \(p_0 = 0.59\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.9] S[table-format=1.9] S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & {\(d'(p_n)\)} & {\(d''(p_n)\)} \\ \midrule 0 & 0.59 & 0.001516 & 6.1772 \\ 1 & 0.589754581 & 0.000000426 & 6.17372559 \\ 2 & 0.589754512 & 0 & 6.17372462 \\ \bottomrule \end {tabular} \end {table} \par Then \(p \approx \num {0.58975}\) is the only zero of \(d'\). Since the sign of \(d'\) changes from negative to positive as \(x\) passes \(p\), the global minimum of \(d\) is achieved at \(p\). \par We conclude that \(x \approx \num {0.58975}\) produces the point on the graph of \(y = x^2\) that is closest to \((1, 0)\).}||exercise-58=={Let \(d\) be the squared distance between the point \((x, \frac {1}{x})\) of the graph and \((2, 1)\). \par \begin {align*} d(x) &= (x - 2)^2 + \left (\frac {1}{x} - 1\right )^2 \\ \Rightarrow d'(x) &= 2(x - 2) - 2 \left (\frac {1}{x} - 1\right ) \frac {1}{x^2} = \frac {2(x^4 - 2x^3 + x - 1)}{x^3} \\ \Rightarrow d''(x) &= 2 \left (\frac {3}{x} - 2\right ) \frac {1}{x^3} + 2 = \frac {2(x^4 - 2x + 3)}{x^4} \end {align*} \par Let \par \begin {align*} f(x) &= x^4 - 2x + 3 \\ \Rightarrow f'(x) &= 4x^3 - 2 \end {align*} \par \(f'\) has exactly one zero at \(\num {0.5}^{\sfrac {1}{3}}\). Since \(f'\) is monotonically increasing in \(\mathbb {R}\), the sign of \(f'\) changes from negative to positive as \(x\) passes \(\num {0.5}^{\sfrac {1}{3}}\). It follows that the global minimum of \(f\) is achieved at \(\num {0.5}^{\sfrac {1}{3}}\): \par \[f(x) \geq f(\num {0.5}^{\sfrac {1}{3}}) \approx \num {1.809449211} > 0\] \par Then, \(d''(x) > 0 \, \forall x \in \mathbb {R} \setminus {0}\). It follows that \(d'\) is monotonically increasing in \(D^+ = \mathbb {R}_{> 0}\) and \(D^- = \mathbb {R}_{< 0}\), which means it has at most one zero in \(D^+\) and \(D^-\) alike. \par Let \par \begin {align*} g(x) &= x^4 - 2x^3 + x - 1 \\ \Rightarrow g'(x) &= 4x^3 - 6x^2 + 1 \end {align*} \par Every zero of \(g\) is also a zero of \(d'\). Applying Newton's method on \(g\) with \(p_0 = \num {1.86}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.8] S[table-format=-1.9] S[table-format=1.8]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule 0 & 1.86 & -0.04087984 & 5.981824 \\ 1 & 1.86683401 & 0.000449982 & 6.11376765 \\ 2 & 1.86676041 & 0.000000053 & 6.11233849 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method on \(g\) with \(p_0 = \num {-0.86}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=-1.9] S[table-format=-1.8]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule 0 & -0.86 & -0.04087984 & -5.981824 \\ 1 & -0.866834009 & 0.000449982 & -6.11376765 \\ 2 & -0.866760408 & 0.000000053 & -6.11233849 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(x \approx \num {1.86676}\) and \(x \approx \num {-0.86676}\) produce the points on the graph of \(y = x^2\) that are closest to \((1, 0)\).}||exercise-59=={The equation of the line tangent to \(f\) at \((p_{n - 1}, f(p_{n - 1}))\) is: \par \[y = f'(p_{n - 1}) (x - p_{n - 1}) + f(p_{n - 1})\] \par Then its x-intercept is: \par \[x = p_{n - 1} - \frac {f(p_{n - 1})}{f'(p_{n - 1})}\] \par Then the formula describing the sequence generated by the procedure is: \par \[\{p_n\} \mid p_n = p_{n - 1} - \frac {f(p_{n - 1})}{f'(p_{n - 1})}\]}||exercise-60=={Let \par \begin {align*} f(x) &= \frac {1}{2} + \frac {1}{4} x^2 - x \sin {x} - \frac {1}{2} \cos {2x} \\ \Rightarrow f'(x) &= \frac {1}{2} x - \sin {x} + x \cos {x} + \sin {2x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \frac {\pi }{2}\) generates the following table: \par \begin {longtable}{r S[table-format=1.8] S[table-format=1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 1.57079633 & 0.046053948 & -0.214601837 \\ 1 & 1.78539816 & 0.007116978 & -0.120293455 \\ 2 & 1.84456163 & 0.001638544 & -0.062366566 \\ 3 & 1.87083442 & 0.000396329 & -0.031675918 \\ 4 & 1.88334643 & 0.000097601 & -0.015954846 \\ 5 & 1.88946376 & 0.000024225 & -0.008005932 \\ 6 & 1.89248962 & 0.000006035 & -0.004010008 \\ 7 & 1.89399457 & 0.000001506 & -0.002006754 \\ 8 & 1.89474507 & 0.000000376 & -0.001003813 \\ 9 & 1.89511983 & 0.000000094 & -0.000502015 \\ 10 & 1.89530709 & 0.000000023 & -0.000251035 \\ 11 & 1.89540069 & 0.000000006 & -0.000125524 \\ 12 & 1.89544748 & 0.000000001 & -0.000062764 \\ 13 & 1.89547087 & 0 & -0.000031382 \\ 14 & 1.89548257 & 0 & -0.000015691 \\ 15 & 1.89548842 & 0 & -0.000007846 \\ \bottomrule \end {longtable} \par It's clear that the number of iteration is unusually large. \par Applying Newton's method on \(f\) with \(p_0 = 5 \pi \) generates the following table: \par \begin {longtable}{r S[table-format=2.8] S[table-format=3.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 15.7079633 & 61.6850275 & 23.5619449 \\ 1 & 13.0899694 & 36.54184 & -4.42523593 \\ 2 & 21.347572 & 101.479949 & 26.1907751 \\ 3 & 17.4729273 & 94.4331539 & 5.96762372 \\ 4 & 1.64867992 & 0.029800649 & -0.199491346 \\ 5 & 1.79806309 & 0.005663214 & -0.109166251 \\ 6 & 1.84994006 & 0.001319265 & -0.056337315 \\ 7 & 1.87335731 & 0.000320334 & -0.028563789 \\ 8 & 1.884572 & 0.000079014 & -0.014376187 \\ 9 & 1.89006817 & 0.000019626 & -0.007211151 \\ 10 & 1.8927898 & 0.00000489 & -0.003611278 \\ 11 & 1.89414416 & 0.00000122 & -0.001807057 \\ 12 & 1.89481974 & 0.000000305 & -0.000903882 \\ 13 & 1.89515714 & 0.000000076 & -0.000452029 \\ 14 & 1.89532573 & 0.000000019 & -0.000226037 \\ 15 & 1.89541001 & 0.000000005 & -0.000113024 \\ 16 & 1.89545214 & 0.000000001 & -0.000056513 \\ 17 & 1.8954732 & 0 & -0.000028257 \\ 18 & 1.89548374 & 0 & -0.000014129 \\ 19 & 1.895489 & 0 & -0.000007064 \\ \bottomrule \end {longtable} \par For \(p_0 = 10 \pi \), the sequence converges and diverges back and forth, then finally stops at \(p_154 \approx \num {-0.000006}\).}||exercise-61=={\begin {enumerate}[label = \alph *)] \item Applying False Position method with \(p_0 = -1\) and \(p_1 = 0\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=3.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & -1 & 433 \\ 1 & 0 & -9 \\ 2 & -0.020361991 & -4.49638093 \\ 3 & -0.030430247 & -2.26689137 \\ 4 & -0.035479814 & -1.14807119 \\ 5 & -0.038030414 & -0.58277074 \\ 6 & -0.03932338 & -0.296160751 \\ 7 & -0.039980008 & -0.150595231 \\ 8 & -0.040313782 & -0.076599144 \\ 9 & -0.040483524 & -0.038967468 \\ 10 & -0.040569867 & -0.019825027 \\ 11 & -0.040613793 & -0.010086543 \\ 12 & -0.040636141 & -0.005131916 \\ 13 & -0.040647511 & -0.002611086 \\ 14 & -0.040653296 & -0.00132851 \\ 15 & -0.04065624 & -0.000675943 \\ 16 & -0.040657737 & -0.000343918 \\ 17 & -0.040658499 & -0.000174985 \\ \bottomrule \end {longtable} \par Applying False Position method with \(p_0 = 0\) and \(p_1 = 1\) generates the following table: \par \begin {longtable}{r S[table-format=1.9] S[table-format=-2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0 & -9 \\ 1 & 1 & 27 \\ 2 & 0.25 & -62.5078125 \\ 3 & 0.773762765 & -83.8305203 \\ 4 & 0.944885169 & -11.2651302 \\ 5 & 0.961110797 & -0.855867823 \\ 6 & 0.962305662 & -0.061802369 \\ 7 & 0.962391747 & -0.004446181 \\ 8 & 0.962397939 & -0.000319781 \\ 9 & 0.962398384 & -0.000022999 \\ \bottomrule \end {longtable} \par \item Applying Secant method with \(p_0 = -1\) and \(p_1 = 0\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=3.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & -1 & 433 \\ 1 & 0 & -9 \\ 2 & -0.020361991 & -4.49638093 \\ 3 & -0.040691256 & 0.007087483 \\ 4 & -0.040659263 & -0.000005706 \\ 5 & -0.040659288 & 0 \\ \bottomrule \end {longtable} \par Applying Secant method with \(p_0 = 0\) and \(p_1 = 1\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-3.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0 & -9 \\ 1 & 1 & 27 \\ 2 & 0.25 & -62.5078125 \\ 3 & 0.773762765 & -83.8305203 \\ 4 & -1.28541778 & 879.638986 \\ 5 & 0.59459552 & -104.691389 \\ 6 & 0.394641105 & -88.1289404 \\ 7 & -0.669318136 & 183.71316 \\ 8 & 0.049714398 & -19.9610216 \\ 9 & -0.020754151 & -4.40957429 \\ 10 & -0.040735333 & 0.016859473 \\ 11 & -0.040659228 & -0.000013318 \\ 12 & -0.040659288 & 0 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = \num {-0.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=-1.9] S[table-format=3.9] S[table-format=-3.6]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule 0 & -0.5 & 115.875 & -331.5 \\ 1 & -0.150452489 & 24.510271 & -225.618988 \\ 2 & -0.041816814 & 0.256640771 & -221.725549 \\ 3 & -0.040659344 & 0.000012234 & -221.704436 \\ 4 & -0.040659288 & 0 & -221.704435 \\ \bottomrule \end {tabular} \end {table} \par Applying Newton's method with \(p_0 = \num {0.5}\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-3.9] S[table-format=-3.6]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule \endhead 0 & 0.5 & -100.625 & -83.5 \\ 1 & -0.70508982 & 201.836304 & -529.339073 \\ 2 & -0.323791114 & 65.4184267 & -252.397607 \\ 3 & -0.064603131 & 5.31400707 & -222.185539 \\ 4 & -0.040686151 & 0.005955616 & -221.704923 \\ 5 & -0.040659288 & 0.000000007 & -221.704435 \\ 6 & -0.040659288 & 0 & -221.704435 \\ \bottomrule \end {longtable} \end {enumerate}}||exercise-62=={\begin {enumerate}[label = \alph *)] \item Applying Bisection method on \(f\) with \(a = 0\), \(b = \num {0.48}\) generates the following table: \par \begin {longtable}{r S[table-format=1.7] S[table-format=1.2] S[table-format=1.8] S[table-format=-2.7]} \toprule \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(a_n\)} & {\(b_n\)} & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 1 & 0 & 0.48 & 0.24 & -60.5096832 \\ 2 & 0.24 & 0.48 & 0.36 & -82.6906752 \\ 3 & 0.36 & 0.48 & 0.42 & -91.7419152 \\ 4 & 0.42 & 0.48 & 0.45 & -95.5558125 \\ 5 & 0.45 & 0.48 & 0.465 & -97.2559241 \\ 6 & 0.465 & 0.48 & 0.4725 & -98.0504281 \\ 7 & 0.4725 & 0.48 & 0.47625 & -98.4332975 \\ 8 & 0.47625 & 0.48 & 0.478125 & -98.6210739 \\ 9 & 0.478125 & 0.48 & 0.4790625 & -98.7140395 \\ 10 & 0.4790625 & 0.48 & 0.47953125 & -98.7602908 \\ \bottomrule \end {longtable} \par The method indeed does not produce the root in this case, as \(f(a_1)\) and \(f(b_1)\) have the same sign. \par \item Applying method of False Position on \(f\) with \(p_0 = 0\) and \(p_1 = \num {0.48}\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0 & -9 \\ 1 & 0.48 & -98.8063872 \\ 2 & -0.048103483 & 1.65092314 \\ 3 & -0.03942459 & -0.273724354 \\ 4 & -0.040658906 & -0.000084697 \\ 5 & -0.040659288 & -0.000000026 \\ \bottomrule \end {longtable} \par \item Applying Secant method on \(f\) with \(p_0 = 0\) and \(p_1 = \num {0.48}\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-2.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} \\ \midrule \endhead 0 & 0 & -9 \\ 1 & 0.48 & -98.8063872 \\ 2 & -0.048103483 & 1.65092314 \\ 3 & -0.03942459 & -0.273724354 \\ 4 & -0.040658906 & -0.000084697 \\ 5 & -0.040659288 & 0.000000004 \\ \bottomrule \end {longtable} \end {enumerate} \par Clearly, Secant method is the most successful one in this case.}||exercise-63=={In both formulas, the denominator is close to \(0\) as consecutive \(p_n\) is close to each other. \par In the above formula, the numerator is also close to \(0\) for the same reason. Therefore, both numerator and denominator are close to \(0\), which can lead to losing digits. \par The formula provided in the text book circumvents this situation by having the difference of 2 consecutive \(p_n\) multiplied with \(f\) \emph {before} dividing. \par As a consequence, the formula should be written in the specific way that it is printed in the text book, as it implies the multiplication should be done before division.}||exercise-64=={Let \par \begin {align*} f(x) &= x^2 - 10 \cos {x} \\ \Rightarrow f'(x) &= 2x + 10 \sin {x} \end {align*} \par \begin {enumerate}[label = \alph *)] \item Applying Newton's method with \(p_0 = -100\) generates the following table: \par \begin {longtable}{r S[table-format=-3.10] S[table-format=4.10] S[table-format=-3.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -100 & 9991.3768112771 & -194.9363435889 \\ 1 & -48.7454384989 & 2375.6104686195 & -87.503753248 \\ 2 & -21.596769094 & 475.6527869722 & -47.0358919679 \\ 3 & -11.4842195691 & 127.1929976708 & -14.1387429948 \\ 4 & -2.4881583409 & 14.1309390157 & -11.0554850027 \\ 5 & -1.2099747957 & -2.0663908208 & -11.7760206276 \\ 6 & -1.3854492523 & 0.076592885 & -12.5996219873 \\ 7 & -1.3793702695 & 0.0000713728 & -12.5760796699 \\ 8 & -1.3793645942 & 0.0000000001 & -12.5760575214 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = -50\) generates the following table: \par \begin {longtable}{r S[table-format=-2.10] S[table-format=4.10] S[table-format=-2.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -50 & 2490.3503397151 & -97.376251463 \\ 1 & -24.4254856569 & 589.0028702885 & -42.3534708223 \\ 2 & -10.5186473541 & 115.2324542098 & -12.1531966041 \\ 3 & -1.0369893209 & -4.0127969624 & -10.6827411852 \\ 4 & -1.4126229615 & 0.4203572492 & -12.7004124469 \\ 5 & -1.3795250404 & 0.0020178304 & -12.5766835597 \\ 6 & -1.3793645982 & 0.0000000502 & -12.576057537 \\ 7 & -1.3793645942 & 0 & -12.5760575214 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = -25\) generates the following table: \par \begin {longtable}{r S[table-format=-2.10] S[table-format=3.10] S[table-format=-2.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -25 & 615.0879718814 & -48.676482499 \\ 1 & -12.3637547271 & 143.0669956648 & -22.7151855357 \\ 2 & -6.0654572538 & 27.0258643344 & -9.9707957587 \\ 3 & -3.3549550042 & 21.0289678026 & -4.5924380275 \\ 4 & 1.2240872555 & -1.8996558667 & 11.8531352735 \\ 5 & 1.3843533642 & 0.0627874198 & 12.5954047231 \\ 6 & 1.3793684177 & 0.0000480838 & 12.5760724428 \\ 7 & 1.3793645942 & 0 & 12.5760575214 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 25\) generates the following table: \par \begin {longtable}{r S[table-format=-1.10] S[table-format=3.10] S[table-format=-2.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 25 & 615.0879718814 & 48.676482499 \\ 1 & 12.3637547271 & 143.0669956648 & 22.7151855357 \\ 2 & 6.0654572538 & 27.0258643344 & 9.9707957587 \\ 3 & 3.3549550042 & 21.0289678026 & 4.5924380275 \\ 4 & -1.2240872555 & -1.8996558667 & -11.8531352735 \\ 5 & -1.3843533642 & 0.0627874198 & -12.5954047231 \\ 6 & -1.3793684177 & 0.0000480838 & -12.5760724428 \\ 7 & -1.3793645942 & 0 & -12.5760575214 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 50\) generates the following table: \par \begin {longtable}{r S[table-format=2.10] S[table-format=4.10] S[table-format=2.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 50 & 2490.3503397151 & 97.376251463 \\ 1 & 24.4254856569 & 589.0028702885 & 42.3534708223 \\ 2 & 10.5186473541 & 115.2324542098 & 12.1531966041 \\ 3 & 1.0369893209 & -4.0127969624 & 10.6827411852 \\ 4 & 1.4126229615 & 0.4203572492 & 12.7004124469 \\ 5 & 1.3795250404 & 0.0020178304 & 12.5766835597 \\ 6 & 1.3793645982 & 0.0000000502 & 12.576057537 \\ 7 & 1.3793645942 & 0 & 12.5760575214 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 100\) generates the following table: \par \begin {longtable}{r S[table-format=3.10] S[table-format=4.10] S[table-format=3.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 100 & 9991.3768112771 & 194.9363435889 \\ 1 & 48.7454384989 & 2375.6104686195 & 87.503753248 \\ 2 & 21.596769094 & 475.6527869722 & 47.0358919679 \\ 3 & 11.4842195691 & 127.1929976708 & 14.1387429948 \\ 4 & 2.4881583409 & 14.1309390157 & 11.0554850027 \\ 5 & 1.2099747957 & -2.0663908208 & 11.7760206276 \\ 6 & 1.3854492523 & 0.076592885 & 12.5996219873 \\ 7 & 1.3793702695 & 0.0000713728 & 12.5760796699 \\ 8 & 1.3793645942 & 0.0000000001 & 12.5760575214 \\ \bottomrule \end {longtable} \end {enumerate}}||exercise-65=={Let \par \begin {align*} f(x) &= 4x^2 - e^x - e^{-x} \\ \Rightarrow f'(x) &= 8x - e^x + e^{-x} \end {align*} \par \begin {enumerate}[label = \alph *)] \item Applying Newton's method with \(p_0 = -10\) generates the following table: \par \begin {longtable}{r S[table-format=-2.10] S[table-format=-5.10] S[table-format=5.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -10 & -21626.4658402066 & 21946.4657494068 \\ 1 & -9.0145809313 & -7897.0494558112 & 8149.9832425813 \\ 2 & -8.0456158156 & -2861.1584947403 & 3055.7206626145 \\ 3 & -7.1092872664 & -1021.1083215684 & 1166.4002502262 \\ 4 & -6.2338516504 & -354.2732875489 & 459.8421761797 \\ 5 & -5.4634280009 & -116.5127783823 & 192.1930584606 \\ 6 & -4.8572001833 & -34.3016609642 & 89.7980895533 \\ 7 & -4.4752136496 & -7.7145986461 & 52.0002627102 \\ 8 & -4.3268567329 & -0.8324004204 & 41.0778853008 \\ 9 & -4.3065927778 & -0.0137992441 & 39.7210636401 \\ 10 & -4.3062453741 & -0.0000039943 & 39.6980697257 \\ 11 & -4.3062452735 & 0 & 39.6980630673 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = -5\) generates the following table: \par \begin {longtable}{r S[table-format=-1.10] S[table-format=-2.10] S[table-format=3.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -5 & -48.4198970496 & 108.4064211556 \\ 1 & -4.5533484407 & -12.0284142159 & 58.5124910196 \\ 2 & -4.3477784161 & -1.7067559697 & 42.5113662274 \\ 3 & -4.3076301894 & -0.0550419721 & 39.7897810066 \\ 4 & -4.3062468701 & -0.0000633809 & 39.6981687205 \\ 5 & -4.3062452735 & -0.0000000001 & 39.6980630674 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = -3\) generates the following table: \par \begin {longtable}{r S[table-format=-1.10] S[table-format=2.10] S[table-format=-1.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -3 & 15.8646760084 & -3.9642501452 \\ 1 & 1.0019361613 & 0.9247864701 & 5.6591071879 \\ 2 & 0.8385205483 & 0.0671745913 & 4.82757152 \\ 3 & 0.8246057692 & 0.0005095513 & 4.754272591 \\ 4 & 0.8244985917 & 0.0000000303 & 4.7537066175 \\ 5 & 0.8244985853 & 0 & 4.7537065838 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = -1\) generates the following table: \par \begin {longtable}{r S[table-format=-1.10] S[table-format=1.10] S[table-format=-1.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -1 & 0.9138387304 & -5.6495976127 \\ 1 & -0.8382471119 & 0.065854754 & -4.8261346213 \\ 2 & -0.824601667 & 0.0004900484 & -4.7542509289 \\ 3 & -0.8244985912 & 0.0000000281 & -4.753706615 \\ 4 & -0.8244985853 & 0 & -4.7537065838 \\ \bottomrule \end {longtable} \par \item The method fails in this case as \(f'(0) = 0\). \par \item Applying Newton's method with \(p_0 = 1\) generates the following table: \par \begin {longtable}{r S[table-format=1.10] S[table-format=1.10] S[table-format=1.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 1 & 0.9138387304 & 5.6495976127 \\ 1 & 0.8382471119 & 0.065854754 & 4.8261346213 \\ 2 & 0.824601667 & 0.0004900484 & 4.7542509289 \\ 3 & 0.8244985912 & 0.0000000281 & 4.753706615 \\ 4 & 0.8244985853 & 0 & 4.7537065838 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 3\) generates the following table: \par \begin {longtable}{r S[table-format=-1.10] S[table-format=2.10] S[table-format=-1.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 3 & 15.8646760084 & 3.9642501452 \\ 1 & -1.0019361613 & 0.9247864701 & -5.6591071879 \\ 2 & -0.8385205483 & 0.0671745913 & -4.82757152 \\ 3 & -0.8246057692 & 0.0005095513 & -4.754272591 \\ 4 & -0.8244985917 & 0.0000000303 & -4.7537066175 \\ 5 & -0.8244985853 & 0 & -4.7537065838 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 5\) generates the following table: \par \begin {longtable}{r S[table-format=1.10] S[table-format=-2.10] S[table-format=-3.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 5 & -48.4198970496 & -108.4064211556 \\ 1 & 4.5533484407 & -12.0284142159 & -58.5124910196 \\ 2 & 4.3477784161 & -1.7067559697 & -42.5113662274 \\ 3 & 4.3076301894 & -0.0550419721 & -39.7897810066 \\ 4 & 4.3062468701 & -0.0000633809 & -39.6981687205 \\ 5 & 4.3062452735 & -0.0000000001 & -39.6980630674 \\ \bottomrule \end {longtable} \par \item Applying Newton's method with \(p_0 = 10\) generates the following table: \par \begin {longtable}{r S[table-format=2.10] S[table-format=-5.10] S[table-format=-5.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 10 & -21626.4658402066 & -21946.4657494068 \\ 1 & 9.0145809313 & -7897.0494558112 & -8149.9832425813 \\ 2 & 8.0456158156 & -2861.1584947403 & -3055.7206626145 \\ 3 & 7.1092872664 & -1021.1083215684 & -1166.4002502262 \\ 4 & 6.2338516504 & -354.2732875489 & -459.8421761797 \\ 5 & 5.4634280009 & -116.5127783823 & -192.1930584606 \\ 6 & 4.8572001833 & -34.3016609642 & -89.7980895533 \\ 7 & 4.4752136496 & -7.7145986461 & -52.0002627102 \\ 8 & 4.3268567329 & -0.8324004204 & -41.0778853008 \\ 9 & 4.3065927778 & -0.0137992441 & -39.7210636401 \\ 10 & 4.3062453741 & -0.0000039943 & -39.6980697257 \\ 11 & 4.3062452735 & 0 & -39.6980630673 \\ \bottomrule \end {longtable} \end {enumerate}}||exercise-66=={Python FTW: 51 iterations.}||exercise-67=={Differentiating \(f\) gives: \par \[f'(x) = \frac {2x}{x^2 + 1} - e^{\num {0.4} x} (\num {0.4} \cos {\pi x} - \pi \sin {\pi x})\] \par Consider each term of \(f\): \par \begin {itemize} \item \(\ln (x^2 + 1) \geq 0 \, \forall x \in \mathbb {R}\) \item \(e^{\num {0.4} x} > 0 \, \forall x \in \mathbb {R}\) \item \(\cos {\pi x} > 0 \iff \num {-0.5} + 2k < x < \num {0.5} + 2k\), with \(k \in \mathbb {N}\) \end {itemize} \par \noindent which means that every zero of \(f\) must be in \(\interval {2k - \num {0.5}}{2k + \num {0.5}}\), \(k \in \mathbb {N}\). \par \begin {enumerate}[label = \alph *)] \item \(e^x\) is monotonically increasing in \(\mathbb {R}\). It follows that: \par \[0 < e^{\num {0.4} x} \cos {\pi x} < e^{\num {0.4} \cdot 0} 1 \, \forall x < 0\] \par \(\ln {x}\) is monotonically increasing in \(\mathbb {R}_{> 0}\). Therefore \(\ln (x^2 + 1)\) is monotonically decreasing in \(\mathbb {R}_{< 0}\). Also, \(e^{x}\) is monotonically increasing in \(\mathbb {R}\). Therefore, if \(f\) has a negative zero, it must satisfy: \par \[\ln (x^2 + 1) < e^{\num {0.4} \cdot 0} \iff - \sqrt {e - 1} \approx \num {-1.310832494} < x < 0\] \par Combining the above points, it is clear that if \(f\) has a negative zero, it must be in \(D_1 = \interval {\num {-0.5}}{0}\). \par As \(\ln (x^2 + 1)\) is monotonically decreasing in \(D_1\), it follows that: \par \[\ln (\num {-0.5}^2 + 1) \geq \ln (x^2 + 1) \geq \ln {1} = 0 \, \forall x \in D_1\] \par As both \(e^x\) and \(\cos {\pi x}\) is monotonically increasing in \(D_1\), it follows that: \par \[0 \leq e^{\num {0.4} x} \cos {\pi x} \leq 1 \, \forall x \in D_1\] \par From the above points, there must be exactly one zero of \(f\) in \(D_1\). \par Applying Newton method on \(f\) with \(p_0 = \num {-0.25}\) generates the following table: \par \begin {longtable}{r S[table-format=-1.9] S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & -0.25 & -0.579192052 & -2.797220033 \\ 1 & -0.457059883 & 0.077693927 & -3.74279653 \\ 2 & -0.436301627 & 0.007306593 & -3.691332860 \\ 3 & -0.434322236 & 0.000606405 & -3.685958212 \\ 4 & -0.434157718 & 0.000049647 & -3.685507782 \\ 5 & -0.434144247 & 0.00000406 & -3.685470876 \\ 6 & -0.434143145 & 0.000000332 & -3.685467857 \\ 7 & -0.434143055 & 0.000000027 & -3.68546761 \\ \bottomrule \end {longtable} \par We conclude that the sole negative zero of \(f\) is \(p \approx \num {-0.4341431}\). \end {enumerate} \par \begin {center} \textbf {not yet finished} \end {center}}||exercise-68=={Let \par \begin {align*} f(x) &= \num {1000} e^\lambda + \frac {\num {435}}{\lambda } (e^\lambda - 1) - \num {1564} \\ \Rightarrow f'(x) &= \num {1000} e^\lambda + \num {435} \left (\frac {1 - e^\lambda }{\lambda ^2} + \frac {e^\lambda }{\lambda }\right ) \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {0.1}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.10] S[table-format=-1.10] S[table-format=4.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.1 & -1.3355882953 & 1337.729475414 \\ 1 & 0.1009983994 & 0.000628932 & 1338.9895592632 \\ 2 & 0.1009979297 & 0.0000000001 & 1338.988966158 \\ \bottomrule \end {tabular} \end {table} \par So \(\lambda \approx \num {0.1009979}\). \par Since \par \[N(t) = N_0 e^{\lambda t} + \frac {v}{\lambda } (e^{\lambda t} - 1)\] \par \noindent then the population predicted at the end of the second year \(N(2) \approx \num {2187.938632} \cdot 1000 = \num {2187938.632}\).}||exercise-69=={Let one number is \(x \in \interval {0}{20}\), and the other is \(20 - x\). We have: \par \[(x + \sqrt {x}) (20 - x + \sqrt {20 - x}) = \num {155.55}\] \par Let \par \begin {align*} f(x) &= (x + \sqrt {x}) (20 - x + \sqrt {20 - x}) - \num {155.55} \\ \Rightarrow f'(x) &= \frac {2 \sqrt {x} + 1}{2 \sqrt {x}} (20 - x + \sqrt {20 - x}) - \frac {2 \sqrt {20 - x} + 1}{2 \sqrt {20 - x}} (x + \sqrt {x}) \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {6.5}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.10] S[table-format=-1.10] S[table-format=2.10]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 6.5 & -0.1315962935 & 10.261387078 \\ 1 & 6.5128244157 & -0.0002485155 & 10.2226328622 \\ 2 & 6.512848726 & -0.0000000009 & 10.2225594124 \\ \bottomrule \end {tabular} \end {table} \par We conclude that the two numbers are approximately \num {6.51285} and \num {13.48715}.}||exercise-70=={Replacing symbols with numbers gives: \par \[A = \frac {\num {1500}}{i} [(1 + i)^{20 \cdot 12} - 1]\] \par Find the minimal interest rate is finding \(i > 0\) such that \(A \geq \num {750000}\): \par \begin {gather*} \frac {\num {1500}}{i} [(1 + i)^{240} - 1] \geq \num {750000} \\ \iff \num {1500} (1 + i)^{240} - \num {750000} i - \num {1500} \geq 0 \tag {*}\label {eq:exer:2.3.26} \end {gather*} \par Let \par \begin {align*} f(x) &= (1 + x)^{240} - 500x - 1 \\ \Rightarrow f'(x) &= 240(x + 1)^{239} - 500 \end {align*} \par Consider \(f'\). \par \[f'(x) = 0 \iff x = A = \sqrt [239]{\frac {25}{12}} - 1\] \par As \(f'\) is monotonically increasing in \(\mathbb {R}^+\), it follows that: \par \begin {itemize} \item \(f\) is monotonically decreasing in \(D_1 = \mathbb {R}_{\leq A} \cap \mathbb {R}^+\) \item \(f\) is monotonically increasing in \(\mathbb {R}_{\geq A}\) \end {itemize} \par Consider the set \(D_1\). \par \[f(0) = 0 > f(x) \, \forall x \in D_1\] \par Therefore, \eqref {eq:exer:2.3.26} has no positive zero in \(D_1\). \par Consider the set \(\mathbb {R}_{\geq A}\). \par \[f(A) \approx \num {-0.448119} \leq f(x) \, \forall x \in \mathbb {R}_{\geq A}\] \par Therefore, \(f\) has at most one zero in \(\mathbb {R}_{\geq A}\). Applying Newton's method on \(f\) with \(p_0 = \num {0.005}\) generates the following table: \par \begin {longtable}{r S[table-format=1.13] S[table-format=-1.13] S[table-format=3.13]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 0.005 & -0.1897955241926 & 290.4965912375794 \\ 1 & 0.0056533485415 & 0.0422743720995 & 423.3277805212566 \\ 2 & 0.0055534865101 & 0.0010855795042 & 401.6714997843162 \\ 3 & 0.0055507838551 & 0.0000007825278 & 401.0924808210714 \\ 4 & 0.0055507819041 & 0.0000000000003 & 401.092062972948 \\ 5 & 0.0055507819041 & 0.0000000000001 & 401.0920629728054 \\ \bottomrule \end {longtable} \par We conclude that the minimal monthly interest rate (assuming that the interest is compounded monthly) is approximately \SI {0.555078}{\percent }.}||exercise-71=={Replacing symbols with numbers gives: \par \[A = \frac {\num {1000}}{i} [1 - (1 + i)^{-(30 \cdot 12)}]\] \par Find the maximal interest rate is finding \(i\) such that \(A \leq \num {135000}\): \par \begin {gather*} \frac {\num {1000}}{i} [1 - (1 + i)^{-360}] \leq \num {135000} \\ \iff \num {1000} [1 - (1 + i)^{-360}] - \num {135000} i \leq 0 \tag {*}\label {eq:exer:2.3.27} \end {gather*} \par Let \par \begin {align*} f(x) &= 1 - (1 + x)^{-360} - 135x \\ \Rightarrow f'(x) &= 360(x + 1)^{-361} - 135 \end {align*} \par Consider \(f'\). \par \[f'(x) = 0 \iff x = A = \sqrt [-361]{\num {0.375}} - 1\] \par As \(f'\) is monotonically decreasing in \(\mathbb {R}^+\), it follows that: \par \begin {itemize} \item \(f\) is monotonically increasing in \(D_1 = \mathbb {R}_{\leq A} \cap \mathbb {R}^+\) \item \(f\) is monotonically decreasing in \(\mathbb {R}_{\geq A}\) \end {itemize} \par Consider the set \(D_1\). \par \[f(0) = 0 < f(x) \, \forall x \in D_1\] \par Therefore, \eqref {eq:exer:2.3.27} has no positive zero in \(D_1\). \par Consider the set \(\mathbb {R}_{\geq A}\). \par \[f(A) \approx \num {0.256689} \geq f(x) \, \forall x \in \mathbb {R}_{\geq A}\] \par Therefore, \(f\) has at most one zero in \(\mathbb {R}_{\geq A}\). Applying Newton's method on \(f\) with \(p_0 = \num {0.0067}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.13] S[table-format=-1.13] S[table-format=-3.13]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.0067 & 0.0051401919049 & -102.6869664108261 \\ 1 & 0.0067500569068 & -0.0000144304894 & -103.2618053134924 \\ 2 & 0.0067499171601 & -0.0000000001111 & -103.2602148635103 \\ \bottomrule \end {tabular} \end {table} \par We conclude that the maximal monthly interest rate is approximately \SI {0.674992}{\percent }.}||exercise-72=={\begin {enumerate}[label = \alph *)] \item Let \par \begin {align*} f(x) &= xe^{\frac {-x}{3}} \\ \Rightarrow f'(x) &= \left (1 - \frac {x}{3}\right ) e^{\frac {-x}{3}} \end {align*} \par Consider \(f'\). \par \[f'(x) = 0 \iff x = 3\] \par It's clear that \(f'\) is monotonically decreasing in \(\mathbb {R}\). It follows that: \par \begin {itemize} \item \(f\) is monotonically increasing in \(\mathbb {R}_{\leq 3}\) \item \(f\) is monotonically decreasing in \(\mathbb {R}_{\geq 3}\) \item \(f\) has a global maximum at \num {3} \end {itemize} \par We now know that \(\max {f} = \frac {3}{e}\) is achieved at \num {3}. In other words, the maximum concentration of any injection is reached \num {3} hours later, regardless of the amount administered. \par To reach the maximum safe concentration of \SI {1}{\milli \gram \per \milli \liter }, the amount should be injected is: \par \[A \frac {3}{e} = 1 \iff A = \frac {e}{3} \approx \num {0.9060939428}\] \par We conclude that to reach the maximum safe concentration, approximately \num {0.9060939428} unit should be injected, and the concentration reaches its highest 3 hours after injection. \par \item Let \par \begin {align*} g(t) &= Ate^{\frac {-t}{3}} - 0.25 \\ \Rightarrow g'(t) &= A\left (1 - \frac {t}{3}\right ) e^{\frac {-t}{3}} \end {align*} \par \noindent with \(A = \frac {e}{3}\). \par We want to inject after the concentration of the first injection already reached its highest, therefore the second injection should be no sooner than \num {3} hours since the first one. \par Applying Newton's method on \(g\) with \(p_0 = \num {11.08}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=2.9] S[table-format=-1.9] S[table-format=-1.9]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule 0 & 11.08 & -0.000127362 & -0.060739197 \\ 1 & 11.077903126 & 0.000000028 & -0.060765892 \\ 2 & 11.077903587 & 0 & -0.060765887 \\ \bottomrule \end {tabular} \end {table} \par We conclude that after about 11 hours and 5 minutes since the first injection, the second one can be administered. \par \item Let \par \begin {align*} c_n(t) &= \sum _{i = 1}^{n} A_i (t - t_i) e^{\frac {-(t - t_i)}{3}} \\ \Rightarrow c_n'(t) &= \sum _{i = 1}^{n} A_i \left (1 - \frac {t - t_i}{3}\right ) e^{\frac {-(t - t_i)}{3}} \end {align*} \par \noindent be the function of concentration \(t \geq t_n\) hours since the first injection \emph {and} during that time window another \(n - 1\) shots are administered. \(t_n\) is the number of hours between the first injection and the \(n^{th}\) one, and clearly \(t_1 = 0\). \par From the above parts, we know that \(A_1 = \frac {e}{3}\), \(A_2 = \num {0.75} A_1 = \frac {e}{4}\), \(t_2 = \num {11.077903587}\). \par Consider \(c_2\). \par \begin {gather*} c_2(t) = 0 \\ \iff (1 - \frac {t}{3}) + \num {0.75} (1 - \frac {t - t_2}{3}) B = 0 \text { with } B = e^{\frac {t_2}{3}} \\ \begin {aligned} \iff t - 3 &= \num {2.25} (3 - t + t_2) B \\ \iff t &= \frac {\num {2.25}(t_2 + 3)B}{1 + \num {2.25} B} \approx \num {13.92377483} \end {aligned} \end {gather*} \par We want to inject after the total concentration from the previous injections already reached its highest, therefore the third injection should be no sooner than \num {13.92377483} hours since the first one. \par Applying Newton's method on \(h_2 = c_2 - 0.25\) with \(p_0 = \num {21.25}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=2.13] S[table-format=-1.13] S[table-format=-1.13]} \toprule \(n\) & {\(p_n\)} & {\(h_2(p_n)\)} & {\(h_2'(p_n)\)} \\ \midrule 0 & 21.25 & -0.0009922998726 & -0.0593509605878 \\ 1 & 21.2332808119236 & 0.0000016642222 & -0.0595501020878 \\ 2 & 21.2333087585113 & 0.0000000000047 & -0.0595497689062 \\ 3 & 21.2333087585895 & 0 & -0.0595497689052 \\ \bottomrule \end {tabular} \end {table} \par We conclude that after about 21 hours and 14 minutes since the first injection, the third one can be administered. \end {enumerate}}||exercise-73=={\begin {enumerate} \item Opps, can't help without Maple license. \par \item The graph of \(f\) is as follow: \par \begin {figure}[H] \centering \begingroup \tikzset {every picture/.style={scale=0.9}}\subfile {graphics/exercise_29_graph/exercise_29_graph} \endgroup \end {figure} \par No useful initial point found, every where: MATLAB, Maple, gnuplot,... \par \item Let: \par \begin {align*} f(x) &= 3^{3x + 1} - 7 \cdot 5^{2x} \\ \Rightarrow f'(x) &= 3 (\ln {3}) 3^{3x + 1} - 14 (\ln {5}) 5^{2x} \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = 11\) generates the following table: \par \begin {longtable}{r S[table-format=2.18] S[table-format=-14] S[table-format=16]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 11 & -12118837442806 & 1244484233952568 \\ 1 & 11.00973804015525026 & 396801311654 & 1326632411906544 \\ 2 & 11.009438935966258555 & 386222634 & 1324050511461616 \\ 3 & 11.009438644268449536 & 370 & 1324047995335120 \\ 4 & 11.009438644268170648 & -38 & 1324047995332592 \\ 5 & 11.00943864426819907 & 4 & 1324047995332848 \\ 6 & 11.009438644268195517 & 66 & 1324047995333032 \\ 7 & 11.009438644268145779 & 0 & 1324047995332608 \\ \bottomrule \end {longtable} \par So \(p \approx \num {11.009438644268145779}\). \par \item Manipulating \(f = 0\) gives: \par \begin {align*} f(x) &= 0 \\ \iff 3 \cdot 3^{3x} &= 7 \cdot 5^{2x} \\ \iff \frac {27^x}{25^x} &= \frac {7}{3} \\ \iff x &= \log _{\sfrac {27}{25}}{\frac {7}{3}} \end {align*} \end {enumerate}}||exercise-74=={\begin {enumerate}[label = \alph *)] \item Opps, can't help without Maple license. \par \item The graph of \(f\) is as follow: \par \begin {figure}[H] \centering \begingroup \tikzset {every picture/.style={scale=0.9}}\subfile {graphics/exercise_30_graph/exercise_30_graph} \endgroup \end {figure} \par \item Let: \par \begin {align*} f(x) &= 2^{x^2} - 3 \cdot 7^{x + 1} \\ \Rightarrow f'(x) &= (\ln {2}) 2x 2^{x^2} - 21 (\ln {7}) 7^x \end {align*} \par Applying Newton's method on \(f\) with \(p_0 = \num {3.92}\) generates the following table: \par \begin {longtable}{r S[table-format=1.18] S[table-format=-3.12] S[table-format=6.12]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endfirsthead \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule \endhead 0 & 3.919999999999999929 & -909.989020751884 & 145585.672581531893 \\ 1 & 3.92625053966242632 & 22.625719019627 & 152874.530827350565 \\ 2 & 3.926102537775538082 & 0.013028085261 & 152698.506017085223 \\ 3 & 3.926102452456528891 & 0.000000004293 & 152698.404592337756 \\ 4 & 3.926102452456500913 & 0.000000000095 & 152698.404592304723 \\ 5 & 3.926102452456500469 & -0.000000000015 & 152698.404592304141 \\ \bottomrule \end {longtable} \par So \(p \approx \num {3.926102452456500469}\). \par \item Manipulating \(f = 0\) gives: \par \begin {gather*} f(x) = 0 \\ \begin {aligned} \iff 2^{x^2} &= 21 \cdot 7^x \\ \iff x^2 &= \log _{2}(21 \cdot 7^x) \\ &= \log _{2}{21} + x \log _{2}{7} \\ \end {aligned}\\ \iff x^2 - \log _{2}{7} x - \log _{2}{21} = 0 \\ \iff x = \frac {\log _{2}{7} \pm \sqrt {\Delta }}{2} \text { with } \Delta = (\log _{2} 7)^2 + 4 * \log _{2} 21 = \log _{2} \num {9529569} \end {gather*} \end {enumerate}}||exercise-75=={We have: \par \begin {align*} P(0) = \frac {P_L}{1 - ce^{-k0}} = P_1 &\iff ce^0 = 1 - \frac {P_L}{P_1} \tag {1}\label {eq:exer:2.3.31:1} \\ P(10) = \frac {P_L}{1 - ce^{-k10}} = P_2 &\iff ce^{-10k} = 1 - \frac {P_L}{P_2} \tag {2}\label {eq:exer:2.3.31:2} \\ P(20) = \frac {P_L}{1 - ce^{-k20}} = P_3 &\iff ce^{-20k} = 1 - \frac {P_L}{P_3} \tag {3}\label {eq:exer:2.3.31:3} \\ \end {align*} \par Divide \eqref {eq:exer:2.3.31:1} by \eqref {eq:exer:2.3.31:2} and \eqref {eq:exer:2.3.31:2} by \eqref {eq:exer:2.3.31:3} gives: \par \begin {gather*} e^{10k} = \frac {A - P_2 P_L}{A - P_1 P_L} \text { with } A = P_1 P_2 \\ e^{10k} = \frac {B - P_3 P_L}{B - P_2 P_L} \text { with } B = P_2 P_3 \end {gather*} \par Combining both above equations gives: \par \begin {gather*} \frac {A - P_2 P_L}{A - P_1 P_L} = \frac {B - P_3 P_L}{B - P_2 P_L} \\ \iff (A - P_6 P_L)(B - P_6 P_L) = (A - P_5 P_L) (B - P_7 P_L) \\ \iff (P_6^2 - P_5 P_7) P_L^2 + (-AP_6 - BP_6 + AP_7 + BP_5) P_L = 0 \\ \iff P_L = \frac {A(P_7 - P_6) + B(P_5 - P_6)}{P_5 P_7 - P_6^2} \approx \num {265816.4151} \end {gather*} \par It follows that \(k \approx \num {0.04501750225}\), and \(c \approx \num {-0.7565812558}\). \par We now predict the US population in 1980 and 2010: \par \begin {gather*} P_{1980} = P(30) \approx \num {222248.3277} \\ P_{2010} = P(60) \approx \num {252967.4246} \end {gather*} \par It is predicted, using the above model, that the US population in 1980 is \num {222248323} and in 2010 is \num {252967425}. However, the actual population in 1980 is larger, so the 1980 prediction undershoots.}||exercise-76=={We have: \par \begin {align*} P(0) = P_L e^{-ce^{-k0}} = P_1 &\iff e^{-k0} = \log _{d} \frac {P_1}{P_L} \tag {1}\label {eq:exer:2.3.32:1} \\ P(10) = P_L e^{-ce^{-k10}} = P_2 &\iff e^{-k10} = \log _{d} \frac {P_2}{P_L} \tag {2}\label {eq:exer:2.3.32:2} \\ P(20) = P_L e^{-ce^{-k20}} = P_3 &\iff e^{-k20} = \log _{d} \frac {P_3}{P_L} \tag {3}\label {eq:exer:2.3.32:3} \end {align*} \par \noindent with \(d = e^{-c}\). \par From \eqref {eq:exer:2.3.32:1}, we know that: \par \[e^{-k0} = 1 = \log _{d} \frac {P_1}{P_L} \iff d = \frac {P_1}{P_L}\] \par Divide \eqref {eq:exer:2.3.32:1} by \eqref {eq:exer:2.3.32:2} and \eqref {eq:exer:2.3.32:2} by \eqref {eq:exer:2.3.32:3} gives: \par \begin {gather*} e^{10k} = \frac {\log _{d} \frac {P_1}{P_L}}{\log _{d} \frac {P_2}{P_L}} = \frac {\log _{d} P_1 - \log _{d} P_L}{\log _{d} P_2 - \log _{d} P_L} = \frac {\ln P_1 - \ln P_L}{\ln P_2 - \ln P_L} \\ e^{10k} = \frac {\log _{d} \frac {P_2}{P_L}}{\log _{d} \frac {P_3}{P_L}} = \frac {\log _{d} P_2 - \log _{d} P_L}{\log _{d} P_3 - \log _{d} P_L} = \frac {\ln P_2 - \ln P_L}{\ln P_3 - \ln P_L} \end {gather*} \par Combining both above equations gives: \par \begin {align*} \frac {\ln P_1 - \ln P_L}{\ln P_2 - \ln P_L} &= \frac {\ln P_2 - \ln P_L}{\ln P_3 - \ln P_L} \\ \iff (\ln P_2 - \ln P_L)^2 &= (\ln P_1 - \ln P_L)(\ln P_3 - \ln P_L) \\ \iff (\ln P_2)^2 - 2 \ln P_2 \ln P_L &= \ln P_1 \ln P_3 - \ln (P_1 P_3) \ln P_L \\ \iff \ln P_L &= \frac {(\ln P_2)^2 - \ln P_1 \ln P_3}{2 \ln P_2 - \ln (P_1 P_3)} \\ \iff P_L &\approx \num {290227.8618} \end {align*} \par It follows that \(k \approx \num {0.0302002813}\), \(d = \num {0.5214041101}\), \(c = \num {0.6512298947}\). \par We now predict the US population in 1980 and 2010: \par \begin {gather*} P_{1980} = P(30) \approx \num {223069.2173} \\ P_{2010} = P(60) \approx \num {260943.6839} \end {gather*} \par It is predicted, using the above model, that the US population in 1980 is \num {223069217} and in 2010 is \num {260943684}. However, the actual population in 1980 is larger, so the 1980 prediction undershoots.}||exercise-77=={Let \par \begin {gather*} \begin {aligned} g(x) &= \frac {x}{1 - x + x^2} \\ \Rightarrow g'(x) &= \frac {1 - x^2}{(1 - x + x^2)^2} \\ \end {aligned} \\ \begin {aligned} f(x) &= \frac {1 + x}{2} \left (\frac {x}{1 - x + x^2}\right )^{21} \\ \Rightarrow f'(x) &= \frac {1}{2} \left (\frac {x}{1 - x + x^2}\right )^{21} + \frac {1 + x}{2} 21 \left (\frac {x}{1 - x + x^2}\right )^{20} \frac {1 - x^2}{(1 - x + x^2)^2} \\ &= \frac {1}{2} \left (\frac {x}{1 - x + x^2}\right )^{20} \left [\frac {x}{1 - x + x^2} + \frac {21(1 + x)(1 - x^2)}{(1 - x + x^2)^2}\right ] \\ &= \frac {1}{2} \left (\frac {x}{1 - x + x^2}\right )^{20} \frac {-20x^3 - 22x^2 + 22x + 21}{(1 - x + x^2)^2} \end {aligned} \end {gather*} \par Finding the minimal value of \(p\) that will ensure that A will shut out B in at least half the matches they play is finding the minimal \(x \in D = \interval {0}{1}\) such that \(f(x) \geq \num {0.5}\). \par Consider \(g'\). \par \begin {gather*} g'(x) = 0 \iff x = \pm 1 \\ x^2 - x + 1 = x^2 - 2x\num {0.5} + \num {0.5}^2 + \num {0.75} \geq \num {0.75} > 0 \, \forall x \in \mathbb {R} \end {gather*} \par It follows that the sign of \(g'\) is the sign of \(1 - x^2\). Therefore, in \(D\), \(g' \geq 0\). Therefore, \(g\) and then \(f\) are monotonically increasing in \(D\): \par \[f(0) = 0 \leq f(x) \leq f(1) = 1 \, \forall x \in D\] \par It's clear that \(f(x) \geq \num {0.5}\) is guaranteed to have solution in \(D\). \par Applying Newton's method on \(h = f - \num {0.5}\) with \(p_0 = \num {0.84}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.18] S[table-format=-1.18] S[table-format=1.18]} \toprule \(n\) & {\(p_n\)} & {\(h(p_n)\)} & {\(h'(p_n)\)} \\ \midrule 0 & 0.84 & -0.010231745763236211 & 4.430566512699972925 \\ 1 & 0.842309353834076791 & 0.000020294149810418 & 4.44775767420762147 \\ 2 & 0.842304791051817325 & 0.000000000072282402 & 4.447725988980080203 \\ 3 & 0.84230479103556577 & 0.000000000000000888 & 4.447725988867216707 \\ 4 & 0.842304791035565548 & -0.000000000000000444 & 4.447725988867211377 \\ \bottomrule \end {tabular} \end {table} \par We conclude that \(p \geq \num {0.842304791035565548}\) will ensure that A will shut out B in at least half the matches they play.}||exercise-78=={Let \par \begin {align*} f(x) &= A \sin {x} \cos {x} + B \sin ^{2}{x} - C \cos {x} - E \sin {x} \\ \Rightarrow f'(x) &= A (\cos ^{2}{x} - \sin ^{2}{x}) + 2 B \sin {x} \cos {x} + C \sin {x} - E \cos {x} \end {align*} \par \begin {enumerate}[label = \alph *)] \item Applying Newton's method on \(f\) with \(p_0 = \SI {33}{\degree } \approx \num {0.57595865315813}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.14] S[table-format=1.14] S[table-format=2.14]} \toprule \(n\) & {\(p_n\)} & {\(g(p_n)\)} & {\(g'(p_n)\)} \\ \midrule 0 & 0.57595865315813 & 0.02541130581159 & 52.34290413106125 \\ 1 & 0.5754731755899 & 0.00000854683891 & 52.30768181120521 \\ 2 & 0.57547301219442 & 0.00000000000097 & 52.30766994413587 \\ 3 & 0.5754730121944 & 0 & 52.30766994413455 \\ \bottomrule \end {tabular} \end {table} \par So \(\alpha \approx \num {0.5754730121944} \approx \SI {32.97217482}{\degree }\), which is indeed close to \SI {33}{\degree }. \par \item Applying Newton's method on \(f\) with \(p_0 = \SI {33}{\degree } \approx \num {0.57595865315813}\) generates the following table: \par \begin {table}[H] \centering \begin {tabular}{r S[table-format=1.14] S[table-format=-1.14] S[table-format=2.14]} \toprule \(n\) & {\(p_n\)} & {\(f(p_n)\)} & {\(f'(p_n)\)} \\ \midrule 0 & 0.57595865315813 & -0.15407902197157 & 52.16025344654213 \\ 1 & 0.57891260778432 & 0.00031564555417 & 52.37350858776342 \\ 2 & 0.57890658096727 & 0.00000000130272 & 52.37307627539987 \\ 3 & 0.5789065809424 & 0.00000000000001 & 52.37307627361562 \\ \bottomrule \end {tabular} \end {table} \par So \(\alpha \approx \num {0.5789065809424} \approx \SI {33.16890382}{\degree }\). \end {enumerate}}}
\XSIM{exercise-body}{exercise-1=={Use the Bisection method to find \(p_3\) for \(f(x) = \sqrt {x} - \cos {x}\) on \(\interval {0}{1}\).}||exercise-2=={Let \(f(x) = 3 (x + 1) (x - \dfrac {1}{2}) (x - 1)\). Use the bisection method to find \(p_3\) in the following intervals: \par \begin {multicols}{2} \begin {enumerate}[label = (\alph *)] \item \(\interval {-2}{\num {1.5}}\) \item \(\interval {\num {-1.5}}{\num {2.5}}\) \end {enumerate} \end {multicols}}||exercise-3=={Use the Bisection method to find solutions accurate to within \(10^{-2}\) for \(x^3 - 7x^2 + 14x - 6 = 0\) in the following intervals: \par \begin {multicols}{3} \begin {enumerate}[label = (\alph *)] \item \(\interval {0}{1}\) \item \(\interval {1}{\num {3.2}}\) \item \(\interval {\num {3.2}}{4}\) \end {enumerate} \end {multicols}}||exercise-4=={Use the Bisection method to find solutions accurate to within \(10^{-2}\) for \(x^4 - 2x^3 - 4x^2 + 4x + 4 = 0\) for the following intervals: \par \begin {multicols}{4} \begin {enumerate}[label = (\alph *)] \item \(\interval {-2}{-1}\) \item \(\interval {0}{2}\) \item \(\interval {2}{3}\) \item \(\interval {-1}{0}\) \end {enumerate} \end {multicols}}||exercise-5=={Use the Bisection method to find solutions accurate to within \(10^{-5}\) for the following problems: \par \begin {enumerate}[label = (\alph *)] \item \(x - 2^{-x} = 0\), \(x \in \interval {0}{1}\) \item \(e^x - x^2 + 3x - 2 = 0\), \(x \in \interval {0}{1}\) \item \(2 x \cos {2x} - (x + 1)^2 = 0\), \(x \in \interval {-3}{-2}\) \item \(x \cos {x} - 2x^2 + 3x - 1 = 0\), \(x \in \interval {\num {0.2}}{\num {0.3}}\) \end {enumerate}}||exercise-6=={Use the Bisection method to find solutions accurate to within \(10^{-5}\) for the following problems: \par \begin {multicols}{2} \begin {enumerate}[label = (\alph *)] \item \(3x - e^x = 0\), \(x \in \interval {1}{2}\) \item \(2x + 3 \cos {x} -e^x = 0\), \(x \in \interval {0}{1}\) \item \(x^2 - 4x + 4 - \ln {x} = 0\), \(x \in \interval {1}{2}\) \item \(x + 1 - 2 \sin {\pi x} = 0\), \(x \in \interval {0}{\num {0.5}}\) \end {enumerate} \end {multicols}}||exercise-7=={\begin {enumerate}[label = (\alph *)] \item Sketch the graphs of \(y = x\) and \(y = 2 \sin {x}\). \item Use the Bisection method to find an approximation to within \(10^{−5}\) to the first positive value of \(x\) with \(x = 2 \sin {x}\). \end {enumerate}}||exercise-8=={\begin {enumerate}[label = (\alph *)] \item Sketch the graphs of \(y = x\) and \(y = \tan {x}\). \item Use the Bisection method to find an approximation to within \(10^{-5}\) to the first positive value of \(x\) with \(y = \tan {x}\). \end {enumerate}}||exercise-9=={\begin {enumerate}[label = (\alph *)] \item Sketch the graphs of \(y = e^x - 2\) and \(y = \cos {e^x - 2}\). \item Use the Bisection method to find an approximation to within \(10^{-5}\) to a value in \(\interval {\num {0.5}}{\num {1.5}}\) with \(e^x - 2 = \cos {e^x - 2}\). \end {enumerate}}||exercise-10=={Let \(f(x) = (x + 2) (x+1)^2 x (x - 1)^3 (x - 2)\). To which zero of \(f\) does the Bisection method converge when applied on the following intervals? \par \begin {multicols}{4} \begin {enumerate}[label = (\alph *)] \item \(\interval {\num {-1.5}}{\num {2.5}}\) \item \(\interval {\num {-0.5}}{\num {2.4}}\) \item \(\interval {\num {-0.5}}{3}\) \item \(\interval {-3}{\num {-0.5}}\) \end {enumerate} \end {multicols}}||exercise-11=={Let \(f(x) = (x + 2) (x+1) x (x - 1)^3 (x - 2)\). To which zero of \(f\) does the Bisection method converge when applied on the following intervals? \par \begin {multicols}{2} \begin {enumerate}[label = (\alph *)] \item \(\interval {-3}{\num {2.5}}\) \item \(\interval {\num {-2.5}}{3}\) \item \(\interval {\num {-1.75}}{\num {1.5}}\) \item \(\interval {\num {-1.5}}{\num {-1.75}}\) \end {enumerate} \end {multicols}}||exercise-12=={Find an approximation to \(\sqrt {3}\) correct to within \(10^{−4}\) using the Bisection Algorithm.}||exercise-13=={Find an approximation to \(\sqrt [3]{25}\) correct to within \(10^{−4}\) using the Bisection Algorithm.}||exercise-14=={Use Theorem 2.1 (\emph {Định lí 2.2} in the Lectures.pdf of the project) to find a bound for the number of iterations needed to achieve an approximation with accuracy \(10^{-3}\) to the solution of \(x^3 + x − 4 = 0\) lying in the interval \(\interval {1}{4}\). Find an approximation to the root with this degree of accuracy.}||exercise-15=={Use Theorem 2.1 (\emph {Định lí 2.2} in the Lectures.pdf of the project) to find a bound for the number of iterations needed to achieve an approximation with accuracy \(10^{-4}\) to the solution of \(x^3 - x − 1 = 0\) lying in the interval \(\interval {1}{2}\). Find an approximation to the root with this degree of accuracy.}||exercise-16=={Let \(f(x) = (x − 1)^{10}\), \(p = 1\), and \(p_n = 1 + \frac {1}{n}\). Show that \(\abs {f(p_n)} < 10^{-3}\) whenever \(n > 1\) but that \(\abs {p - p_n} < 10^{-3}\) requires that \(n > 1000\).}||exercise-17=={Let \(\{p_n\}\) be the sequence defined by \(p_n = \sum _{k = 1}^{n} \frac {1}{k}\). Show that \(\{p_n\}\) diverges even though \(\lim _{n \to \infty } (p_n - p_{n - 1}) = 0\).}||exercise-18=={The function defined by \(f(x) = \sin {\pi x}\) has zeros at every integer. Show that when \(−1 < a < 0\) and \(2 < b < 3\), the Bisection method converges to \par \begin {multicols}{3} \begin {enumerate}[label = (\alph *)] \item \(0\) if \(a + b < 2\) \item \(2\) if \(a + b > 2\) \item \(1\) if \(a + b = 2\) \end {enumerate} \end {multicols}}||exercise-19=={A trough of length \(L\) has a cross section in the shape of a semicircle with radius \(r\). When filled with water to within a distance \(h\) of the top, the volume \(V\) of water is: \par \[V = L (\num {0.5} \pi r^2 - r^2 \arcsin {\frac {h}{r}} - h \sqrt {r^2 - h^2})\] \par Suppose \(L = \SI {10}{ft}\), \(r = \SI {1}{ft}\), and \(V = \SI {12.4}{ft^3}\). Find the depth of water in the trough to within \(\SI {0.01}{ft}\).}||exercise-20=={A particle starts at rest on a smooth inclined plane whose angle \(\theta \) is changing at a constant rate \(\omega \) such that: \par \[\od {\theta }{t} = \omega < 0\] \par At the end of \(t\) seconds, the position of the object is given by: \par \[x(t) = - \frac {g}{2 \omega ^2} \left (\frac {e^{\omega t} - e^{- \omega t}}{x} - \sin \omega t\right )\] \par Suppose the particle has moved \SI {1.7}{ft} in \SI {1}{s}. Find, to within \(10^{5}\), the rate \(\omega \) at which \(\theta \) changes. Assume that \(g = \SI {32.17}{ft \per \second \squared }\).}||exercise-21=={\label {exer:2.2.1} Use algebraic manipulation to show that each of the following functions has a fixed-point at \(p\) precisely when \(f(p) = 0\), where \(f(x) = x^4 + 2x^2 - x - 3\). \par \begin {tasks}(2) \task \(g_1(x) = (3 + x - 2x^2)^{\sfrac {1}{4}}\) \task \(g_2(x) = \left (\dfrac {x + 3 - x^4}{2}\right )^{\sfrac {1}{2}}\) \task \(g_3(x) = \left (\dfrac {x + 3}{x^2 + 2}\right )^{\sfrac {1}{2}}\) \task \(g_4(x) = \dfrac {3x^4 + 2x^2 + 3}{4x^3 + 4x - 1}\) \end {tasks}}||exercise-22=={\begin {tasks} \task Perform four iterations, if possible, on each of the functions \(g\) defined in \hyperref [exer:2.2.1]{Exercise 1}. Let \(p_0 = 1\) and \(p_{n + 1} = g(p_n)\), for \(n = 0, 1, 2, 3\). \task Which function do you think gives the best approximation to the solution? \end {tasks}}||exercise-23=={The following four methods are proposed to compute \(21^{\sfrac {1}{3}}\). Rank them in order, based on their apparent speed of convergence, assuming \(p_0 = 1\). \par \begin {tasks}(2) \task \(p_n = \dfrac {20 p_{n - 1} + \sfrac {21}{p_{n - 1}^2}}{21}\) \label {exer:2.2.3:a} \task \(p_n = p_{n - 1} - \dfrac {p_{n - 1}^3 - 21}{3 p_{n - 1}^2}\) \label {exer:2.2.3:b} \task \(p_n = p_{n - 1} - \dfrac {p_{n - 1}^4 - 21 p_{n - 1}}{p_{n - 1}^2 - 21}\) \label {exer:2.2.3:c} \task \(p_n = \left (\dfrac {21}{p_{n-1}}\right )^{\sfrac {1}{2}}\) \label {exer:2.2.3:d} \end {tasks}}||exercise-24=={The following four methods are proposed to compute \(7^{\sfrac {1}{5}}\). Rank them in order, based on their apparent speed of convergence, assuming \(p_0 = 1\). \par \begin {tasks}(2) \task \(p_n = p_{n - 1} - \left (1 + \frac {7 - p_{n - 1}^5}{p_{n - 1}^2}\right )^3\) \label {exer:2.2.4:a} \task \(p_n = p_{n - 1} - \dfrac {p_{n - 1}^5 - 7}{p_{n - 1}^2}\) \label {exer:2.2.4:b} \task \(p_n = p_{n - 1} - \dfrac {p_{n - 1}^5 - 7}{5 p_{n - 1}^4}\) \label {exer:2.2.4:c} \task \(p_n = p_{n - 1} - \dfrac {p_{n - 1}^5 - 7}{12}\) \label {exer:2.2.4:d} \end {tasks}}||exercise-25=={Use a fixed-point iteration method to determine a solution accurate to within \(10^{-2}\) for \(x^4 - 3x^2 - 3 = 0\) on \(\interval {1}{2}\). Use \(p_0 = 1\).}||exercise-26=={Use a fixed-point iteration method to determine a solution accurate to within \(10^{-2}\) for \(x^3 - x - 1 = 0\) on \(\interval {1}{2}\). Use \(p_0 = 1\).}||exercise-27=={Use Theorem 2.3 (Định lý 2.3 in the accompanying Lectures.pdf) to show that \(g(x) = \pi + \num {0.5} \sin {\num {0.5} x}\) has a unique fixed point on \(\interval {0}{2 \pi }\). Use fixed-point iteration to find an approximation to the fixed point that is accurate to within \(10^{-2}\). Use Corollary 2.5 (Hệ quả 2.1) to estimate the number of iterations required to achieve \(10^{-2}\) accuracy, and compare this theoretical estimate to the number actually needed.}||exercise-28=={Use Theorem 2.3 (Định lý 2.3 in the accompanying Lectures.pdf) to show that \(g(x) = 2^{-x}\) has a unique fixed point on \(\interval {\frac {1}{3}}{1}\). Use fixed-point iteration to find an approximation to the fixed point that is accurate to within \(10^{-4}\). Use Corollary 2.5 (Hệ quả 2.1) to estimate the number of iterations required to achieve \(10^{-4}\) accuracy, and compare this theoretical estimate to the number actually needed.}||exercise-29=={Use a fixed-point iteration method to find an approximation to \(\sqrt {3}\) that is accurate to within \(10^{-4}\). Compare your result and the number of iterations required with the answer obtained in Exercise 12 of Section 2.1.}||exercise-30=={Use a fixed-point iteration method to find an approximation to \(\sqrt [3]{25}\) that is accurate to within \(10^{-4}\). Compare your result and the number of iterations required with the answer obtained in Exercise 13 of Section 2.1.}||exercise-31=={For each of the following equations, determine an interval \(\interval {a}{b}\) on which fixed-point iteration converges. Estimate the number of iterations necessary to obtain approximations accurate to within \(10^{-5}\), and perform the calculations. \par \begin {tasks}(2) \task \(x = \dfrac {2 - e^x + x^2}{3}\) \task \(x = \dfrac {5}{x^2} + 2\) \task \(x = (\sfrac {e^x}{3})^{\sfrac {1}{2}}\) \task \(x = 5^{-x}\) \task \(x = 6^{-x}\) \task \(x = \num {0.5}(\sin {x} + \cos {x})\) \end {tasks}}||exercise-32=={For each of the following equations, use the given interval or determine an interval \(\interval {a}{b}\) on which fixed-point iteration will converge. Estimate the number of iterations necessary to obtain approximations accurate to within \(10^{-5}\), and perform the calculations. \par \begin {tasks}(2) \task \(2 + \sin {x} - x = 0\) on \(\interval {2}{3}\) \task \(x^3 - 3x - 5 = 0\) on \(\interval {2}{3}\) \task \(3x^2 - e^x = 0\) \task \(x - \cos {x} = 0\) \end {tasks}}||exercise-33=={Find all the zeros of \(f(x) = x^2 + 10 \cos {x}\) by using the fixed-point iteration method for an appropriate iteration function \(g\). Find the zeros accurate to within \(10^{-4}\).}||exercise-34=={Use a fixed-point iteration method to determine a solution accurate to within \(10^{-4}\) for \(x = \tan {x}\), for \(x \in \interval {4}{5}\).}||exercise-35=={Use a fixed-point iteration method to determine a solution accurate to within \(10^{-2}\) for \(2 \sin {\pi x} + x = 0\) on \(\interval {1}{2}\). Use \(p_0 = 1\).}||exercise-36=={Let \(A\) be a given positive constant and \(g(x) = 2x - Ax^2\). \par \begin {tasks} \task Show that if fixed-point iteration converges to a nonzero limit, then the limit is \(p = \sfrac {1}{A}\), so the inverse of a number can be found using only multiplications and subtractions. \task Find an interval about \(\sfrac {1}{A}\) for which fixed-point iteration converges, provided \(p_0\) is in that interval. \end {tasks}}||exercise-37=={Find a function \(g\) defined on \(\interval {0}{1}\) that satisfies none of the hypotheses of Theorem 2.3 but still has a unique fixed point on \(\interval {0}{1}\).}||exercise-38=={\begin {tasks} \task Show that Theorem 2.2 is true if the inequality \(\abs {g'(x)} \leq k\) is replaced by \(g'(x) \leq k\), for all \(x \in \interval [open]{a}{b}\). [Hint: Only uniqueness is in question.] \task Show that Theorem 2.3 may not hold if inequality \(\abs {g'(x)} \leq k\) is replaced by \(g'(x) \leq k\). \end {tasks}}||exercise-39=={\begin {tasks} \task Use Theorem 2.4 (Định lí 2.5 in the accompanying Lectures.pdf) to show that the sequence defined by: \par \[x_n = \frac {1}{2} x_{n - 1} + \frac {1}{x_{n - 1}} \text {, for \(n \leq 1\)}\] \par converges to \(\sqrt {2}\) whenever \(x_0 > \sqrt {2}\). \par \task Use the fact that \(0 < (x_0 - \sqrt {2})^2\) whenever \(x_0 \neq \sqrt {2}\) to show that if \(0 < x_0 < \sqrt {2}\), then \(x_1 > \sqrt {2}\). \par \task Use the above results to show that the sequence in (a) converges to \(\sqrt {2}\) whenever \(x_0 > 0\). \end {tasks}}||exercise-40=={\begin {tasks} \task Show that if A is any positive number, then the sequence defined by \par \[x_n = \frac {1}{2} x_{n - 1} + \frac {A}{2x_{n - 1}} \text {, for } n \geq 1\] \par converges to \(\sqrt {A}\) whenever \(x_0 > 0\). \task What happens if \(x_0 < 0\)? \end {tasks}}||exercise-41=={Replace the assumption in Theorem 2.4 that ``a positive number \(k < 1\) exists with \(\abs {g(x)} \leq k\)'' with ``g satisfies a Lipschitz condition on the interval \(\interval {a}{b}\) with Lipschitz constant \(L < 1\)'' (See Exercise 27, Section 1.1.) Show that the conclusions of this theorem are still valid.}||exercise-42=={Suppose that \(g\) is continuously differentiable on some interval \(\interval [open]{c}{d}\) that contains the fixed point \(p\) of \(g\). Show that if \(\abs {g'(p)} < 1\), then there exists a \(\delta > 0\) such that if \(\abs {p_0 - p} \leq \delta \), then the fixed-point iteration converges.}||exercise-43=={An object falling vertically through the air is subjected to viscous resistance as well as to the force of gravity. Assume that an object with mass \(m\) is dropped from a height \(s_0\) and that the height of the object after \(t\) seconds is: \par \[s(t) = s_0 - \frac {mg}{k} t + \frac {m^2g}{k^2} (1 - e^{- \sfrac {kt}{m}})\] \par \noindent where \(g = \SI {32.17}{ft \per \second \squared }\) and \(k\) represents the coefficient of air resistance in lb/s. Suppose \(s_0 = \SI {300}{ft}\), \(m = \SI {0.25}{lb}\), and \(k = \SI {0.1}{lb \per \second }\). Find, to within \(\SI {0.01}{\second }\), the time it takes this quarter-pounder to hit the ground.}||exercise-44=={Let \(g \in C^1 \interval {a}{b}\) and \(p\) be in \(\interval [open]{a}{b}\) with \(g(p) = p\) and \(\abs {g'(p)} > 1\). Show that there exists a \(\delta > 0\) such that if \(0 < \abs {p_0 - p} < \delta \), then \(\abs {p_0 - p} < \abs {p_1 - p}\). Thus, no matter how close the initial approximation \(p_0\) is to \(p\), the next iterate \(p_1\) is farther away, so the fixed-point iteration does not converge if \(p_0 \neq p\).}||exercise-45=={Let \(f(x) = x^2 - 6\) and \(p_0 = 1\). Use Newton's method to find \(p_2\).}||exercise-46=={Let \(f(x) = -x^3 - \cos {x}\) and \(p_0 = -1\). Use Newton's method to find \(p_2\). Could \(p_0 = 0\) be used?}||exercise-47=={Let \(f(x) = x^2 - 6\). With \(p_0 = 3\) and \(p_1 = 2\), find \(p_3\). \par \begin {tasks} \task Use the Secant method. \task Use the method of False Position. \task Which of the above is closer to \(\sqrt {6}\)? \end {tasks}}||exercise-48=={Let \(f(x) = -x^3 - \cos {x}\). With \(p_0 = -1\) and \(p_1 = 0\), find \(p_3\). \par \begin {tasks}(2) \task Use the Secant method. \task Use the method of False Position. \end {tasks}}||exercise-49=={\label {exer:2.3.5} Use Newton's method to find solutions accurate to within \(10^{-4}\) for the following problems. \par \begin {tasks} \task \(x^3 - 2x^2 - 5 = 0\) in \(\interval {1}{4}\) \task \(x^3 + 3x^2 - 1 = 0\) in \(\interval {-3}{-2}\) \task \(x - \cos {x} = 0\) in \(\interval {0}{\sfrac {\pi }{2}}\) \task \(x - \num {0.8} - \num {0.2} \sin {x} = 0\) in \(\interval {0}{\sfrac {\pi }{2}}\) \end {tasks}}||exercise-50=={\label {exer:2.3.6} Use Newton's method to find solutions accurate to within \(10^{-5}\) for the following problems. \par \begin {tasks} \task \(e^x + 2^{-x} + 2 \cos {x} - 6 = 0\) for \(x \in \interval {1}{2}\) \task \(\ln (x - 1) + \cos (x - 1) = 0\) for \(x \in \interval {\num {1.3}}{2}\) \task \(2 x \cos (2x) - (x - 2)^2 = 0\) for \(x \in \interval {2}{3}\) and \(x \in \interval {3}{4}\) \task \((x - 2)^2 - \ln {x} = 0\) for \(x \in \interval {1}{2}\) and \(x \in \interval {e}{4}\) \task \(e^x - 3x^2 = 0\) for \(x \in \interval {0}{1}\) and \(x \in \interval {3}{5}\) \task \(\sin {x} - e^x = 0\) for \(x \in \interval {0}{1}\), \(x \in \interval {3}{4}\) and \(x \in \interval {6}{7}\) \end {tasks}}||exercise-51=={Repeat \hyperref [exer:2.3.5]{Exercise 5} using the Secant method.}||exercise-52=={Repeat \hyperref [exer:2.3.6]{Exercise 6} using the Secant method.}||exercise-53=={Repeat \hyperref [exer:2.3.5]{Exercise 5} using the method of False Position.}||exercise-54=={Repeat \hyperref [exer:2.3.6]{Exercise 6} using the False Position method.}||exercise-55=={Use all three methods in this Section to find solutions to within \(10^{-5}\) for the following problems. \par \begin {tasks} \task \(3xe^x = 0\) for \(x \in \interval {1}{2}\) \task \(2x + 3 \cos {x} - e^x\) for \(x \in \interval {0}{1}\) \end {tasks}}||exercise-56=={Use all three methods in this Section to find solutions to within \(10^{-7}\) for the following problems. \par \begin {tasks} \task \(x^2 - 4x + 4 - \ln {x} = 0\) for \(x \in \interval {1}{2}\) and \(x \in \interval {2}{4}\) \task \(x + 1 - 2 \sin {\pi x} = 0\) for \(x \in \interval {0}{\sfrac {1}{2}}\) and \(x \in \interval {\sfrac {1}{2}}{1}\) \end {tasks}}||exercise-57=={Use Newton's method to approximate, to within \(10^{-4}\), the value of \(x\) that produces the point on the graph of \(y = x^2\) that is closest to \((1, 0)\).}||exercise-58=={Use Newton's method to approximate, to within \(10^{-4}\), the value of \(x\) that produces the point on the graph of \(y = \frac {1}{x}\) that is closest to \((2, 1)\).}||exercise-59=={The following describes Newton's method graphically: \par Suppose that \(f'(x)\) exists on \(\interval {a}{b}\) and that \(f'(x) \neq 0 \, \forall x \in \interval {a}{b}\). Further, suppose there exists one \(p \in \interval {a}{b}\) such that \(f(p) = 0\). \par Let \(p_0 \in \interval {a}{b}\) be arbitrary. Let \(p_1\) be the point at which the tangent line to \(f\) at \((p_0, f(p_0))\) crosses the x-axis. For each \(n \geq 1\), let \(p_n\) be the x-intercept of the line tangent to \(f\) at \((p_{n - 1}, f(p_{n - 1}))\). Derive the formula describing this method.}||exercise-60=={Use Newton's method to solve the equation \par \[0 = \frac {1}{2} + \frac {1}{4} x^2 - x \sin {x} - \frac {1}{2} \cos {2x} \text { with } p_0 = \frac {\pi }{2}\] \par Iterate using Newton's method until an accuracy of \(10^{-5}\) is obtained. Explain why the result seems unusual for Newton's method. Also, solve the equation with \(p_0 = 5 \pi \) and \(p_0 = 10 \pi \).}||exercise-61=={The fourth-degree polynomial \par \[f(x) = 230x^4 + 18x^3 + 9x^2 - 221x - 9\] \par \noindent has two real zeros, one in \(\interval {-1}{0}\) and the other in \(\interval {0}{1}\). Attempt to approximate these zeros to within \(10^{-6}\) using the \par \begin {tasks} \task Method of False Position \task Secant method \task Newton's method \end {tasks} \par Use the endpoints of each interval as the initial approximations in a) and b) and the midpoints as the initial approximation in c).}||exercise-62=={The function \(f(x) = \tan {\pi x} - 6\) has a zero at \(\frac {\arctan (6)}{\pi } \approx \num {0.447431543}\). Let \(p_0 = 0\) and \(p_1 = \num {0.48}\), and use ten iterations of each of the following methods to approximate this root. Which method is most successful and why? \par \begin {tasks}(3) \task Bisection \task False Position \task Secant \end {tasks}}||exercise-63=={The iteration equation for the Secant method can be written in the simpler form: \par \[p_n = \frac {f(p_{n - 1}) p_{n - 2} - f(p_{n - 2}) p_{n - 1}}{f(p_{n - 1}) - f(p_{n - 2})}\] \par Explain why, in general, this iteration equation is likely to be less accurate than the one given in the text book.}||exercise-64=={The equation \(x^2 - 10 \cos {x} = 0\) has two solutions, \(\pm \num {1.3793646}\). Use Newton's method to approximate the solutions to within \(10^{-5}\) with the following values of \(p_0\). \par \begin {tasks}(3) \task \(p_0 = -100\) \task \(p_0 = -50\) \task \(p_0 = -25\) \task \(p_0 = 25\) \task \(p_0 = 50\) \task \(p_0 = 100\) \end {tasks}}||exercise-65=={The equation \(4x^2 - e^x - e^{-x} = 0\) has two positive solutions \(x_1\) and \(x_2\). Use Newton's method to approximate the solution to within \(10^{-5}\) with the following values of \(p_0\). \par \begin {tasks}(3) \task \(p_0 = -10\) \task \(p_0 = -5\) \task \(p_0 = -3\) \task \(p_0 = -1\) \task \(p_0 = 0\) \task \(p_0 = 1\) \task \(p_0 = 3\) \task \(p_0 = 5\) \task \(p_0 = 10\) \end {tasks}}||exercise-66=={Use Maple to determine how many iterations of Newton's method with \(p_0 = \sfrac {\pi }{4}\) are needed to find a root of \(f(x) = \cos {x} - x\) to within \(10^{-100}\).}||exercise-67=={The function described by \(f(x) = \ln (x^2 + 1) - e^{\num {0.4} x} \cos {\pi x}\) has an infinite number of zeros. \par \begin {tasks} \task Determine, within \(10^{-6}\), the only negative zero. \task Determine, within \(10^{-6}\), the four smallest positive zeros. \par \task Determine a reasonable initial approximation to find the \(n^{th}\) smallest positive zero of \(f\). [Hint: Sketch an approximate graph of \(f\).] \task Use part c) to determine, within \(10^{-6}\), the \(25^{th}\) smallest positive zero of \(f\). \end {tasks}}||exercise-68=={Find an approximation for \(\lambda \), accurate to within \(10^{-4}\), for the population equation \par \[\num {1564000} = \num {1000000} e^\lambda + \frac {\num {435000}}{\lambda } (e^\lambda - 1)\] \par \noindent discussed in the introduction to this chapter. Use this value to predict the population at the end of the second year, assuming that the immigration rate during this year remains at \num {435000} individuals per year.}||exercise-69=={The sum of two numbers is \(20\). If each number is added to its square root, the product of the two sums is \num {155.55}. Determine the two numbers to within \(10^{-4}\).}||exercise-70=={The accumulated value of a savings account based on regular periodic payments can be determined from the \emph {annuity due equation}: \par \[A = \frac {P}{i} [(1 + i)^n - 1]\] \par In this equation, \(A\) is the amount in the account, \(P\) is the amount regularly deposited, and \(i\) is the rate of interest per period for the \(n\) deposit periods. An engineer would like to have a savings account valued at \$\num {750000} upon retirement in 20 years and can afford to put \$\num {1500} per month toward this goal. What is the minimal interest rate at which this amount can be invested, assuming that the interest is compounded monthly?}||exercise-71=={Problems involving the amount of money required to pay off a mortgage over a fixed period of time involve the formula \par \[A = \frac {P}{i} [1 - (1 + i)^{-n}]\] \par \noindent known as an \emph {ordinary annuity equation}. In this equation, \(A\) is the amount of the mortgage, \(P\) is the amount of each payment, and \(i\) is the interest rate per period for the \(n\) payment periods. Suppose that a 30-year home mortgage in the amount of \$\num {135000} is needed and that the borrower can afford house payments of at most \$\num {1000} per month. What is the maximal interest rate the borrower can afford to pay?}||exercise-72=={A drug administered to a patient produces a concentration in the blood stream given by \(c(t) = A t e^{\frac {-t}{3}}\) milligrams per milliliter, \(t\) hours after \(A\) units have been injected. The maximum safe concentration is \SI {1}{\milli \gram \per \milli \liter }. \par \begin {tasks} \task What amount should be injected to reach this maximum safe concentration, and when does this maximum occur? \par \task An additional amount of this drug is to be administered to the patient after the concentration falls to \SI {0.25}{\milli \gram \per \milli \liter }. Determine, to the nearest minute, when this second injection should be given. \par \task Assume that the concentration from consecutive injections is additive and that \SI {75}{\percent } of the amount originally injected is administered in the second injection. When is it time for the third injection? \end {tasks}}||exercise-73=={\label {exer:2.3.29} Let \par \[f(x) = 3^{3x + 1} - 7 \cdot 5^{2x}\] \par \begin {tasks} \task Use the Maple commands \lstinline |solve| and \lstinline |fsolve| to try to find all roots of \(f\). \task Plot \(f\) to find initial approximations to roots of \(f\). \task Use Newton's method to find roots of \(f\) to within \(10^{-16}\). \task Find the exact solutions of \(f(x) = 0\) without using Maple. \end {tasks}}||exercise-74=={Repeat \hyperref [exer:2.3.29]{Exercise 29} using \(f(x) = 2^{x^2} - 3 \cdot 7^{x + 1}\).}||exercise-75=={\label {exer:2.3.31} The logistic population growth model is described by an equation of the form \par \[P(t) = \frac {P_L}{1 - ce^{-kt}}\] \par \noindent where \(P_L\), \(c\), and \(k > 0\) are constants, and \(P(t)\) is the population at time \(t\). \(P_L\) represents the limiting value of the population since \(\lim _{t \to \infty } P(t) = P_L\). Use the census data for the years 1950, 1960, and 1970 listed in the table on page 105 to determine the constants \(P_L\), \(c\), and \(k\) for a logistic growth model. Use the logistic model to predict the population of the United States in 1980 and in 2010, assuming \(t = 0\) at 1950. Compare the 1980 prediction to the actual value.}||exercise-76=={The Gompertz population growth model is described by \par \[P(t) = P_L e^{-ce^{-kt}}\] \par \noindent where \(P_L\), \(c\), and \(k > 0\) are constants, and \(P(t)\) is the population at time \(t\). Repeat \hyperref [exer:2.3.31]{Exercise 31} using the Gompertz growth model in place of the logistic model.}||exercise-77=={Player A will shut out (win by a score of 21-0) player B in a game of racquetball with probability \par \[P = \frac {1 + p}{2} \left (\frac {p}{1 - p + p^2}\right )^{21}\] \par \noindent where \(p\) denotes the probability A will win any specific rally (independent of the server). Determine, to within \(10^{-3}\), the minimal value of \(p\) that will ensure that A will shut out B in at least half the matches they play.}||exercise-78=={In the design of all-terrain vehicles, it is necessary to consider the failure of the vehicle when attempting to negotiate two types of obstacles. One type of failure is called \emph {hang-up failure} and occurs when the vehicle attempts to cross an obstacle that causes the bottom of the vehicle to touch the ground. The other type of failure is called \emph {nose-in failure} and occurs when the vehicle descends into a ditch and its nose touches the ground. \par The accompanying figure shows the components associated with the nose-in failure of a vehicle. It is shown that the maximum angle \(\alpha \) that can be negotiated by a vehicle when \(\beta \) is the maximum angle at which hang-up failure does \emph {not} occur satisfies the equation \par \[A \sin {\alpha } \cos {\alpha } + B \sin ^{2}{\alpha } - C \cos {\alpha } - E \sin {\alpha } = 0\] \par \noindent where \par \[\begin {cases} D: \text {wheel diameter} \\ A = l \sin {\beta _1} \\ B = l \cos {\beta _1} \\ C = (h + \num {0.5} D) \sin {\beta _1} - \num {0.5} D \tan {\beta _1} \\ E = (h + \num {0.5} D) \cos {\beta _1} - \num {0.5} D \end {cases}\] \par \begin {tasks} \task It is stated that when \(l = \SI {89}{in}\), \(h = \SI {49}{in}\), \(D = \SI {55}{in}\), and \(\beta _1 = \SI {11.5}{\degree }\), angle \(\alpha \) is approximately \SI {33}{\degree }. Verify this result. \par \task Find \(\alpha \) for the situation when \(l\), \(h\), and \(\beta _1\) are the same as in part a) but \(D = \SI {30}{in}\). \end {tasks}}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11||exercise-12||exercise-13||exercise-14||exercise-15||exercise-16||exercise-17||exercise-18||exercise-19||exercise-20||exercise-21||exercise-22||exercise-23||exercise-24||exercise-25||exercise-26||exercise-27||exercise-28||exercise-29||exercise-30||exercise-31||exercise-32||exercise-33||exercise-34||exercise-35||exercise-36||exercise-37||exercise-38||exercise-39||exercise-40||exercise-41||exercise-42||exercise-43||exercise-44||exercise-45||exercise-46||exercise-47||exercise-48||exercise-49||exercise-50||exercise-51||exercise-52||exercise-53||exercise-54||exercise-55||exercise-56||exercise-57||exercise-58||exercise-59||exercise-60||exercise-61||exercise-62||exercise-63||exercise-64||exercise-65||exercise-66||exercise-67||exercise-68||exercise-69||exercise-70||exercise-71||exercise-72||exercise-73||exercise-74||exercise-75||exercise-76||exercise-77||exercise-78}
\XSIM{total-number}{78}
\XSIM{exercise}{78}
\XSIM{types}{exercise}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={21}||exercise-22=={22}||exercise-23=={23}||exercise-24=={24}||exercise-25=={25}||exercise-26=={26}||exercise-27=={27}||exercise-28=={28}||exercise-29=={29}||exercise-30=={30}||exercise-31=={31}||exercise-32=={32}||exercise-33=={33}||exercise-34=={34}||exercise-35=={35}||exercise-36=={36}||exercise-37=={37}||exercise-38=={38}||exercise-39=={39}||exercise-40=={40}||exercise-41=={41}||exercise-42=={42}||exercise-43=={43}||exercise-44=={44}||exercise-45=={45}||exercise-46=={46}||exercise-47=={47}||exercise-48=={48}||exercise-49=={49}||exercise-50=={50}||exercise-51=={51}||exercise-52=={52}||exercise-53=={53}||exercise-54=={54}||exercise-55=={55}||exercise-56=={56}||exercise-57=={57}||exercise-58=={58}||exercise-59=={59}||exercise-60=={60}||exercise-61=={61}||exercise-62=={62}||exercise-63=={63}||exercise-64=={64}||exercise-65=={65}||exercise-66=={66}||exercise-67=={67}||exercise-68=={68}||exercise-69=={69}||exercise-70=={70}||exercise-71=={71}||exercise-72=={72}||exercise-73=={73}||exercise-74=={74}||exercise-75=={75}||exercise-76=={76}||exercise-77=={77}||exercise-78=={78}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={1}||exercise-22=={2}||exercise-23=={3}||exercise-24=={4}||exercise-25=={5}||exercise-26=={6}||exercise-27=={7}||exercise-28=={8}||exercise-29=={9}||exercise-30=={10}||exercise-31=={11}||exercise-32=={12}||exercise-33=={13}||exercise-34=={14}||exercise-35=={15}||exercise-36=={16}||exercise-37=={17}||exercise-38=={18}||exercise-39=={19}||exercise-40=={20}||exercise-41=={21}||exercise-42=={22}||exercise-43=={23}||exercise-44=={24}||exercise-45=={1}||exercise-46=={2}||exercise-47=={3}||exercise-48=={4}||exercise-49=={5}||exercise-50=={6}||exercise-51=={7}||exercise-52=={8}||exercise-53=={9}||exercise-54=={10}||exercise-55=={11}||exercise-56=={12}||exercise-57=={13}||exercise-58=={14}||exercise-59=={15}||exercise-60=={16}||exercise-61=={17}||exercise-62=={18}||exercise-63=={19}||exercise-64=={20}||exercise-65=={21}||exercise-66=={22}||exercise-67=={23}||exercise-68=={24}||exercise-69=={25}||exercise-70=={26}||exercise-71=={27}||exercise-72=={28}||exercise-73=={29}||exercise-74=={30}||exercise-75=={31}||exercise-76=={32}||exercise-77=={33}||exercise-78=={34}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}||exercise-12=={12}||exercise-13=={13}||exercise-14=={14}||exercise-15=={15}||exercise-16=={16}||exercise-17=={17}||exercise-18=={18}||exercise-19=={19}||exercise-20=={20}||exercise-21=={1}||exercise-22=={2}||exercise-23=={3}||exercise-24=={4}||exercise-25=={5}||exercise-26=={6}||exercise-27=={7}||exercise-28=={8}||exercise-29=={9}||exercise-30=={10}||exercise-31=={11}||exercise-32=={12}||exercise-33=={13}||exercise-34=={14}||exercise-35=={15}||exercise-36=={16}||exercise-37=={17}||exercise-38=={18}||exercise-39=={19}||exercise-40=={20}||exercise-41=={21}||exercise-42=={22}||exercise-43=={23}||exercise-44=={24}||exercise-45=={1}||exercise-46=={2}||exercise-47=={3}||exercise-48=={4}||exercise-49=={5}||exercise-50=={6}||exercise-51=={7}||exercise-52=={8}||exercise-53=={9}||exercise-54=={10}||exercise-55=={11}||exercise-56=={12}||exercise-57=={13}||exercise-58=={14}||exercise-59=={15}||exercise-60=={16}||exercise-61=={17}||exercise-62=={18}||exercise-63=={19}||exercise-64=={20}||exercise-65=={21}||exercise-66=={22}||exercise-67=={23}||exercise-68=={24}||exercise-69=={25}||exercise-70=={26}||exercise-71=={27}||exercise-72=={28}||exercise-73=={29}||exercise-74=={30}||exercise-75=={31}||exercise-76=={32}||exercise-77=={33}||exercise-78=={34}}
\XSIM{print}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}}
\XSIM{use}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}||exercise-12=={true}||exercise-13=={true}||exercise-14=={true}||exercise-15=={true}||exercise-16=={true}||exercise-17=={true}||exercise-18=={true}||exercise-19=={true}||exercise-20=={true}||exercise-21=={true}||exercise-22=={true}||exercise-23=={true}||exercise-24=={true}||exercise-25=={true}||exercise-26=={true}||exercise-27=={true}||exercise-28=={true}||exercise-29=={true}||exercise-30=={true}||exercise-31=={true}||exercise-32=={true}||exercise-33=={true}||exercise-34=={true}||exercise-35=={true}||exercise-36=={true}||exercise-37=={true}||exercise-38=={true}||exercise-39=={true}||exercise-40=={true}||exercise-41=={true}||exercise-42=={true}||exercise-43=={true}||exercise-44=={true}||exercise-45=={true}||exercise-46=={true}||exercise-47=={true}||exercise-48=={true}||exercise-49=={true}||exercise-50=={true}||exercise-51=={true}||exercise-52=={true}||exercise-53=={true}||exercise-54=={true}||exercise-55=={true}||exercise-56=={true}||exercise-57=={true}||exercise-58=={true}||exercise-59=={true}||exercise-60=={true}||exercise-61=={true}||exercise-62=={true}||exercise-63=={true}||exercise-64=={true}||exercise-65=={true}||exercise-66=={true}||exercise-67=={true}||exercise-68=={true}||exercise-69=={true}||exercise-70=={true}||exercise-71=={true}||exercise-72=={true}||exercise-73=={true}||exercise-74=={true}||exercise-75=={true}||exercise-76=={true}||exercise-77=={true}||exercise-78=={true}}
\XSIM{solution}{}
\XSIM{chapter-value}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={1}||exercise-14=={1}||exercise-15=={1}||exercise-16=={1}||exercise-17=={1}||exercise-18=={1}||exercise-19=={1}||exercise-20=={1}||exercise-21=={1}||exercise-22=={1}||exercise-23=={1}||exercise-24=={1}||exercise-25=={1}||exercise-26=={1}||exercise-27=={1}||exercise-28=={1}||exercise-29=={1}||exercise-30=={1}||exercise-31=={1}||exercise-32=={1}||exercise-33=={1}||exercise-34=={1}||exercise-35=={1}||exercise-36=={1}||exercise-37=={1}||exercise-38=={1}||exercise-39=={1}||exercise-40=={1}||exercise-41=={1}||exercise-42=={1}||exercise-43=={1}||exercise-44=={1}||exercise-45=={1}||exercise-46=={1}||exercise-47=={1}||exercise-48=={1}||exercise-49=={1}||exercise-50=={1}||exercise-51=={1}||exercise-52=={1}||exercise-53=={1}||exercise-54=={1}||exercise-55=={1}||exercise-56=={1}||exercise-57=={1}||exercise-58=={1}||exercise-59=={1}||exercise-60=={1}||exercise-61=={1}||exercise-62=={1}||exercise-63=={1}||exercise-64=={1}||exercise-65=={1}||exercise-66=={1}||exercise-67=={1}||exercise-68=={1}||exercise-69=={1}||exercise-70=={1}||exercise-71=={1}||exercise-72=={1}||exercise-73=={1}||exercise-74=={1}||exercise-75=={1}||exercise-76=={1}||exercise-77=={1}||exercise-78=={1}}
\XSIM{chapter}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={1}||exercise-14=={1}||exercise-15=={1}||exercise-16=={1}||exercise-17=={1}||exercise-18=={1}||exercise-19=={1}||exercise-20=={1}||exercise-21=={1}||exercise-22=={1}||exercise-23=={1}||exercise-24=={1}||exercise-25=={1}||exercise-26=={1}||exercise-27=={1}||exercise-28=={1}||exercise-29=={1}||exercise-30=={1}||exercise-31=={1}||exercise-32=={1}||exercise-33=={1}||exercise-34=={1}||exercise-35=={1}||exercise-36=={1}||exercise-37=={1}||exercise-38=={1}||exercise-39=={1}||exercise-40=={1}||exercise-41=={1}||exercise-42=={1}||exercise-43=={1}||exercise-44=={1}||exercise-45=={1}||exercise-46=={1}||exercise-47=={1}||exercise-48=={1}||exercise-49=={1}||exercise-50=={1}||exercise-51=={1}||exercise-52=={1}||exercise-53=={1}||exercise-54=={1}||exercise-55=={1}||exercise-56=={1}||exercise-57=={1}||exercise-58=={1}||exercise-59=={1}||exercise-60=={1}||exercise-61=={1}||exercise-62=={1}||exercise-63=={1}||exercise-64=={1}||exercise-65=={1}||exercise-66=={1}||exercise-67=={1}||exercise-68=={1}||exercise-69=={1}||exercise-70=={1}||exercise-71=={1}||exercise-72=={1}||exercise-73=={1}||exercise-74=={1}||exercise-75=={1}||exercise-76=={1}||exercise-77=={1}||exercise-78=={1}}
\XSIM{section-value}{exercise-1=={1}||exercise-2=={1}||exercise-3=={1}||exercise-4=={1}||exercise-5=={1}||exercise-6=={1}||exercise-7=={1}||exercise-8=={1}||exercise-9=={1}||exercise-10=={1}||exercise-11=={1}||exercise-12=={1}||exercise-13=={1}||exercise-14=={1}||exercise-15=={1}||exercise-16=={1}||exercise-17=={1}||exercise-18=={1}||exercise-19=={1}||exercise-20=={1}||exercise-21=={2}||exercise-22=={2}||exercise-23=={2}||exercise-24=={2}||exercise-25=={2}||exercise-26=={2}||exercise-27=={2}||exercise-28=={2}||exercise-29=={2}||exercise-30=={2}||exercise-31=={2}||exercise-32=={2}||exercise-33=={2}||exercise-34=={2}||exercise-35=={2}||exercise-36=={2}||exercise-37=={2}||exercise-38=={2}||exercise-39=={2}||exercise-40=={2}||exercise-41=={2}||exercise-42=={2}||exercise-43=={2}||exercise-44=={2}||exercise-45=={3}||exercise-46=={3}||exercise-47=={3}||exercise-48=={3}||exercise-49=={3}||exercise-50=={3}||exercise-51=={3}||exercise-52=={3}||exercise-53=={3}||exercise-54=={3}||exercise-55=={3}||exercise-56=={3}||exercise-57=={3}||exercise-58=={3}||exercise-59=={3}||exercise-60=={3}||exercise-61=={3}||exercise-62=={3}||exercise-63=={3}||exercise-64=={3}||exercise-65=={3}||exercise-66=={3}||exercise-67=={3}||exercise-68=={3}||exercise-69=={3}||exercise-70=={3}||exercise-71=={3}||exercise-72=={3}||exercise-73=={3}||exercise-74=={3}||exercise-75=={3}||exercise-76=={3}||exercise-77=={3}||exercise-78=={3}}
\XSIM{section}{exercise-1=={1.1}||exercise-2=={1.1}||exercise-3=={1.1}||exercise-4=={1.1}||exercise-5=={1.1}||exercise-6=={1.1}||exercise-7=={1.1}||exercise-8=={1.1}||exercise-9=={1.1}||exercise-10=={1.1}||exercise-11=={1.1}||exercise-12=={1.1}||exercise-13=={1.1}||exercise-14=={1.1}||exercise-15=={1.1}||exercise-16=={1.1}||exercise-17=={1.1}||exercise-18=={1.1}||exercise-19=={1.1}||exercise-20=={1.1}||exercise-21=={1.2}||exercise-22=={1.2}||exercise-23=={1.2}||exercise-24=={1.2}||exercise-25=={1.2}||exercise-26=={1.2}||exercise-27=={1.2}||exercise-28=={1.2}||exercise-29=={1.2}||exercise-30=={1.2}||exercise-31=={1.2}||exercise-32=={1.2}||exercise-33=={1.2}||exercise-34=={1.2}||exercise-35=={1.2}||exercise-36=={1.2}||exercise-37=={1.2}||exercise-38=={1.2}||exercise-39=={1.2}||exercise-40=={1.2}||exercise-41=={1.2}||exercise-42=={1.2}||exercise-43=={1.2}||exercise-44=={1.2}||exercise-45=={1.3}||exercise-46=={1.3}||exercise-47=={1.3}||exercise-48=={1.3}||exercise-49=={1.3}||exercise-50=={1.3}||exercise-51=={1.3}||exercise-52=={1.3}||exercise-53=={1.3}||exercise-54=={1.3}||exercise-55=={1.3}||exercise-56=={1.3}||exercise-57=={1.3}||exercise-58=={1.3}||exercise-59=={1.3}||exercise-60=={1.3}||exercise-61=={1.3}||exercise-62=={1.3}||exercise-63=={1.3}||exercise-64=={1.3}||exercise-65=={1.3}||exercise-66=={1.3}||exercise-67=={1.3}||exercise-68=={1.3}||exercise-69=={1.3}||exercise-70=={1.3}||exercise-71=={1.3}||exercise-72=={1.3}||exercise-73=={1.3}||exercise-74=={1.3}||exercise-75=={1.3}||exercise-76=={1.3}||exercise-77=={1.3}||exercise-78=={1.3}}
\XSIM{sectioning}{exercise-1=={{1}{1}{0}{0}{0}}||exercise-2=={{1}{1}{0}{0}{0}}||exercise-3=={{1}{1}{0}{0}{0}}||exercise-4=={{1}{1}{0}{0}{0}}||exercise-5=={{1}{1}{0}{0}{0}}||exercise-6=={{1}{1}{0}{0}{0}}||exercise-7=={{1}{1}{0}{0}{0}}||exercise-8=={{1}{1}{0}{0}{0}}||exercise-9=={{1}{1}{0}{0}{0}}||exercise-10=={{1}{1}{0}{0}{0}}||exercise-11=={{1}{1}{0}{0}{0}}||exercise-12=={{1}{1}{0}{0}{0}}||exercise-13=={{1}{1}{0}{0}{0}}||exercise-14=={{1}{1}{0}{0}{0}}||exercise-15=={{1}{1}{0}{0}{0}}||exercise-16=={{1}{1}{0}{0}{0}}||exercise-17=={{1}{1}{0}{0}{0}}||exercise-18=={{1}{1}{0}{0}{0}}||exercise-19=={{1}{1}{0}{0}{0}}||exercise-20=={{1}{1}{0}{0}{0}}||exercise-21=={{1}{2}{0}{0}{0}}||exercise-22=={{1}{2}{0}{0}{0}}||exercise-23=={{1}{2}{0}{0}{0}}||exercise-24=={{1}{2}{0}{0}{0}}||exercise-25=={{1}{2}{0}{0}{0}}||exercise-26=={{1}{2}{0}{0}{0}}||exercise-27=={{1}{2}{0}{0}{0}}||exercise-28=={{1}{2}{0}{0}{0}}||exercise-29=={{1}{2}{0}{0}{0}}||exercise-30=={{1}{2}{0}{0}{0}}||exercise-31=={{1}{2}{0}{0}{0}}||exercise-32=={{1}{2}{0}{0}{0}}||exercise-33=={{1}{2}{0}{0}{0}}||exercise-34=={{1}{2}{0}{0}{0}}||exercise-35=={{1}{2}{0}{0}{0}}||exercise-36=={{1}{2}{0}{0}{0}}||exercise-37=={{1}{2}{0}{0}{0}}||exercise-38=={{1}{2}{0}{0}{0}}||exercise-39=={{1}{2}{0}{0}{0}}||exercise-40=={{1}{2}{0}{0}{0}}||exercise-41=={{1}{2}{0}{0}{0}}||exercise-42=={{1}{2}{0}{0}{0}}||exercise-43=={{1}{2}{0}{0}{0}}||exercise-44=={{1}{2}{0}{0}{0}}||exercise-45=={{1}{3}{0}{0}{0}}||exercise-46=={{1}{3}{0}{0}{0}}||exercise-47=={{1}{3}{0}{0}{0}}||exercise-48=={{1}{3}{0}{0}{0}}||exercise-49=={{1}{3}{0}{0}{0}}||exercise-50=={{1}{3}{0}{0}{0}}||exercise-51=={{1}{3}{0}{0}{0}}||exercise-52=={{1}{3}{0}{0}{0}}||exercise-53=={{1}{3}{0}{0}{0}}||exercise-54=={{1}{3}{0}{0}{0}}||exercise-55=={{1}{3}{0}{0}{0}}||exercise-56=={{1}{3}{0}{0}{0}}||exercise-57=={{1}{3}{0}{0}{0}}||exercise-58=={{1}{3}{0}{0}{0}}||exercise-59=={{1}{3}{0}{0}{0}}||exercise-60=={{1}{3}{0}{0}{0}}||exercise-61=={{1}{3}{0}{0}{0}}||exercise-62=={{1}{3}{0}{0}{0}}||exercise-63=={{1}{3}{0}{0}{0}}||exercise-64=={{1}{3}{0}{0}{0}}||exercise-65=={{1}{3}{0}{0}{0}}||exercise-66=={{1}{3}{0}{0}{0}}||exercise-67=={{1}{3}{0}{0}{0}}||exercise-68=={{1}{3}{0}{0}{0}}||exercise-69=={{1}{3}{0}{0}{0}}||exercise-70=={{1}{3}{0}{0}{0}}||exercise-71=={{1}{3}{0}{0}{0}}||exercise-72=={{1}{3}{0}{0}{0}}||exercise-73=={{1}{3}{0}{0}{0}}||exercise-74=={{1}{3}{0}{0}{0}}||exercise-75=={{1}{3}{0}{0}{0}}||exercise-76=={{1}{3}{0}{0}{0}}||exercise-77=={{1}{3}{0}{0}{0}}||exercise-78=={{1}{3}{0}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={1}||exercise-3=={2}||exercise-4=={3}||exercise-5=={5}||exercise-6=={8}||exercise-7=={11}||exercise-8=={12}||exercise-9=={14}||exercise-10=={15}||exercise-11=={16}||exercise-12=={17}||exercise-13=={18}||exercise-14=={19}||exercise-15=={19}||exercise-16=={20}||exercise-17=={21}||exercise-18=={21}||exercise-19=={22}||exercise-20=={23}||exercise-21=={24}||exercise-22=={25}||exercise-23=={26}||exercise-24=={27}||exercise-25=={28}||exercise-26=={28}||exercise-27=={29}||exercise-28=={30}||exercise-29=={31}||exercise-30=={32}||exercise-31=={32}||exercise-32=={36}||exercise-33=={40}||exercise-34=={42}||exercise-35=={42}||exercise-36=={43}||exercise-37=={45}||exercise-38=={45}||exercise-39=={45}||exercise-40=={47}||exercise-41=={48}||exercise-42=={49}||exercise-43=={49}||exercise-44=={50}||exercise-45=={51}||exercise-46=={51}||exercise-47=={51}||exercise-48=={52}||exercise-49=={52}||exercise-50=={54}||exercise-51=={58}||exercise-52=={59}||exercise-53=={62}||exercise-54=={63}||exercise-55=={66}||exercise-56=={67}||exercise-57=={70}||exercise-58=={71}||exercise-59=={72}||exercise-60=={73}||exercise-61=={74}||exercise-62=={77}||exercise-63=={78}||exercise-64=={78}||exercise-65=={80}||exercise-66=={83}||exercise-67=={83}||exercise-68=={85}||exercise-69=={85}||exercise-70=={86}||exercise-71=={87}||exercise-72=={88}||exercise-73=={91}||exercise-74=={92}||exercise-75=={93}||exercise-76=={94}||exercise-77=={96}||exercise-78=={97}}
\XSIM{page}{exercise-1=={1}||exercise-2=={1}||exercise-3=={2}||exercise-4=={3}||exercise-5=={5}||exercise-6=={8}||exercise-7=={11}||exercise-8=={12}||exercise-9=={14}||exercise-10=={15}||exercise-11=={16}||exercise-12=={17}||exercise-13=={18}||exercise-14=={19}||exercise-15=={19}||exercise-16=={20}||exercise-17=={21}||exercise-18=={21}||exercise-19=={22}||exercise-20=={23}||exercise-21=={24}||exercise-22=={25}||exercise-23=={26}||exercise-24=={27}||exercise-25=={28}||exercise-26=={28}||exercise-27=={29}||exercise-28=={30}||exercise-29=={31}||exercise-30=={32}||exercise-31=={32}||exercise-32=={36}||exercise-33=={40}||exercise-34=={42}||exercise-35=={42}||exercise-36=={43}||exercise-37=={45}||exercise-38=={45}||exercise-39=={45}||exercise-40=={47}||exercise-41=={48}||exercise-42=={49}||exercise-43=={49}||exercise-44=={50}||exercise-45=={51}||exercise-46=={51}||exercise-47=={51}||exercise-48=={52}||exercise-49=={52}||exercise-50=={54}||exercise-51=={58}||exercise-52=={59}||exercise-53=={62}||exercise-54=={63}||exercise-55=={66}||exercise-56=={67}||exercise-57=={70}||exercise-58=={71}||exercise-59=={72}||exercise-60=={73}||exercise-61=={74}||exercise-62=={77}||exercise-63=={78}||exercise-64=={78}||exercise-65=={80}||exercise-66=={83}||exercise-67=={83}||exercise-68=={85}||exercise-69=={85}||exercise-70=={86}||exercise-71=={87}||exercise-72=={88}||exercise-73=={91}||exercise-74=={92}||exercise-75=={93}||exercise-76=={94}||exercise-77=={96}||exercise-78=={97}}
\XSIM{tags}{}
\XSIM{topics}{}
